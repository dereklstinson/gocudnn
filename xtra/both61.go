package xtra

const both61 = `//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26907403
// Cuda compilation tools, release 10.1, V10.1.243
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_61
.address_size 64

	// .globl	Transpose
// _ZZ8L1L2FP16E6l1l2h2 has been demoted
// _ZZ20MSELossbyBatchesFP16E5loss2 has been demoted
// _ZZ11MSELossFP16E5loss2 has been demoted

.visible .entry Transpose(
	.param .u32 Transpose_param_0,
	.param .u64 Transpose_param_1,
	.param .u64 Transpose_param_2,
	.param .u32 Transpose_param_3,
	.param .u64 Transpose_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<105>;
	.reg .b64 	%rd<45>;


	ld.param.u32 	%r32, [Transpose_param_0];
	ld.param.u64 	%rd11, [Transpose_param_1];
	ld.param.u64 	%rd13, [Transpose_param_2];
	ld.param.u32 	%r33, [Transpose_param_3];
	ld.param.u64 	%rd12, [Transpose_param_4];
	cvta.to.global.u64 	%rd1, %rd13;
	shl.b32 	%r1, %r33, 1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r91, %r2, %r34, %r35;
	setp.ge.s32	%p1, %r91, %r32;
	@%p1 bra 	BB0_15;

	cvta.to.global.u64 	%rd2, %rd12;
	cvta.to.global.u64 	%rd3, %rd11;
	mov.u32 	%r36, %nctaid.x;
	mul.lo.s32 	%r4, %r36, %r2;
	and.b32  	%r5, %r33, 3;
	mul.wide.s32 	%rd7, %r33, 4;
	add.s64 	%rd4, %rd1, %rd7;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd5, %rd1, %rd14;

BB0_2:
	mov.u32 	%r104, 0;
	setp.lt.s32	%p2, %r33, 1;
	@%p2 bra 	BB0_14;

	mov.u32 	%r98, 0;
	setp.eq.s32	%p3, %r5, 0;
	@%p3 bra 	BB0_4;

	setp.eq.s32	%p4, %r5, 1;
	@%p4 bra 	BB0_6;
	bra.uni 	BB0_7;

BB0_6:
	mov.u32 	%r96, %r91;
	mov.u32 	%r97, %r98;
	bra.uni 	BB0_10;

BB0_4:
	mov.u32 	%r99, %r91;
	mov.u32 	%r104, %r98;
	bra.uni 	BB0_11;

BB0_7:
	setp.eq.s32	%p5, %r5, 2;
	mov.u32 	%r93, %r91;
	mov.u32 	%r94, %r98;
	@%p5 bra 	BB0_9;

	ld.global.u32 	%r46, [%rd4];
	div.s32 	%r47, %r91, %r46;
	mul.lo.s32 	%r48, %r47, %r46;
	sub.s32 	%r93, %r91, %r48;
	ld.global.u32 	%r49, [%rd5];
	mul.wide.s32 	%rd15, %r49, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r50, [%rd16];
	mul.lo.s32 	%r94, %r50, %r47;
	mov.u32 	%r98, 1;

BB0_9:
	add.s32 	%r51, %r98, %r33;
	mul.wide.s32 	%rd17, %r51, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u32 	%r52, [%rd18];
	div.s32 	%r53, %r93, %r52;
	mul.lo.s32 	%r54, %r53, %r52;
	sub.s32 	%r96, %r93, %r54;
	add.s32 	%r55, %r98, %r1;
	mul.wide.s32 	%rd19, %r55, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r56, [%rd20];
	mul.wide.s32 	%rd21, %r56, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u32 	%r57, [%rd22];
	mad.lo.s32 	%r97, %r57, %r53, %r94;
	add.s32 	%r98, %r98, 1;

BB0_10:
	add.s32 	%r58, %r98, %r33;
	mul.wide.s32 	%rd23, %r58, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r59, [%rd24];
	div.s32 	%r60, %r96, %r59;
	mul.lo.s32 	%r61, %r60, %r59;
	sub.s32 	%r99, %r96, %r61;
	add.s32 	%r62, %r98, %r1;
	mul.wide.s32 	%rd25, %r62, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u32 	%r63, [%rd26];
	mul.wide.s32 	%rd27, %r63, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u32 	%r64, [%rd28];
	mad.lo.s32 	%r104, %r64, %r60, %r97;
	add.s32 	%r98, %r98, 1;

BB0_11:
	setp.lt.u32	%p6, %r33, 4;
	@%p6 bra 	BB0_14;

	mul.wide.s32 	%rd29, %r98, 4;
	add.s64 	%rd44, %rd1, %rd29;

BB0_13:
	add.s64 	%rd30, %rd44, %rd7;
	ld.global.u32 	%r65, [%rd30];
	div.s32 	%r66, %r99, %r65;
	mul.lo.s32 	%r67, %r66, %r65;
	sub.s32 	%r68, %r99, %r67;
	add.s64 	%rd31, %rd44, %rd14;
	ld.global.u32 	%r69, [%rd31];
	mul.wide.s32 	%rd32, %r69, 4;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.u32 	%r70, [%rd33];
	mad.lo.s32 	%r71, %r70, %r66, %r104;
	ld.global.u32 	%r72, [%rd30+4];
	div.s32 	%r73, %r68, %r72;
	mul.lo.s32 	%r74, %r73, %r72;
	sub.s32 	%r75, %r68, %r74;
	ld.global.u32 	%r76, [%rd31+4];
	mul.wide.s32 	%rd34, %r76, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.u32 	%r77, [%rd35];
	mad.lo.s32 	%r78, %r77, %r73, %r71;
	ld.global.u32 	%r79, [%rd30+8];
	div.s32 	%r80, %r75, %r79;
	mul.lo.s32 	%r81, %r80, %r79;
	sub.s32 	%r82, %r75, %r81;
	ld.global.u32 	%r83, [%rd31+8];
	mul.wide.s32 	%rd36, %r83, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.u32 	%r84, [%rd37];
	mad.lo.s32 	%r85, %r84, %r80, %r78;
	ld.global.u32 	%r86, [%rd30+12];
	div.s32 	%r87, %r82, %r86;
	mul.lo.s32 	%r88, %r87, %r86;
	sub.s32 	%r99, %r82, %r88;
	ld.global.u32 	%r89, [%rd31+12];
	mul.wide.s32 	%rd38, %r89, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.u32 	%r90, [%rd39];
	mad.lo.s32 	%r104, %r90, %r87, %r85;
	add.s64 	%rd44, %rd44, 16;
	add.s32 	%r98, %r98, 4;
	setp.lt.s32	%p7, %r98, %r33;
	@%p7 bra 	BB0_13;

BB0_14:
	mul.wide.s32 	%rd40, %r104, 4;
	add.s64 	%rd41, %rd3, %rd40;
	ld.global.f32 	%f1, [%rd41];
	mul.wide.s32 	%rd42, %r91, 4;
	add.s64 	%rd43, %rd2, %rd42;
	st.global.f32 	[%rd43], %f1;
	add.s32 	%r91, %r4, %r91;
	setp.lt.s32	%p8, %r91, %r32;
	@%p8 bra 	BB0_2;

BB0_15:
	ret;
}

	// .globl	SwapEveryOther
.visible .entry SwapEveryOther(
	.param .u32 SwapEveryOther_param_0,
	.param .u32 SwapEveryOther_param_1,
	.param .u64 SwapEveryOther_param_2,
	.param .u64 SwapEveryOther_param_3,
	.param .u32 SwapEveryOther_param_4,
	.param .u32 SwapEveryOther_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r8, [SwapEveryOther_param_0];
	ld.param.u32 	%r9, [SwapEveryOther_param_1];
	ld.param.u64 	%rd3, [SwapEveryOther_param_2];
	ld.param.u64 	%rd4, [SwapEveryOther_param_3];
	ld.param.u32 	%r17, [SwapEveryOther_param_4];
	ld.param.u32 	%r11, [SwapEveryOther_param_5];
	setp.ge.s32	%p1, %r17, %r9;
	@%p1 bra 	BB1_6;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r1, %r13, %r12, %r14;
	mov.u32 	%r15, %nctaid.x;
	mul.lo.s32 	%r2, %r15, %r13;

BB1_2:
	setp.ge.s32	%p2, %r1, %r8;
	@%p2 bra 	BB1_5;

	mul.lo.s32 	%r4, %r17, %r8;
	mov.u32 	%r18, %r1;

BB1_4:
	add.s32 	%r16, %r18, %r4;
	mul.wide.s32 	%rd5, %r16, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.f32 	%f1, [%rd6];
	add.s64 	%rd7, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd7];
	st.global.f32 	[%rd6], %f2;
	st.global.f32 	[%rd7], %f1;
	add.s32 	%r18, %r2, %r18;
	setp.lt.s32	%p3, %r18, %r8;
	@%p3 bra 	BB1_4;

BB1_5:
	bar.sync 	0;
	add.s32 	%r17, %r17, %r11;
	setp.lt.s32	%p4, %r17, %r9;
	@%p4 bra 	BB1_2;

BB1_6:
	ret;
}

	// .globl	SwapUpperLower
.visible .entry SwapUpperLower(
	.param .u32 SwapUpperLower_param_0,
	.param .u32 SwapUpperLower_param_1,
	.param .u64 SwapUpperLower_param_2,
	.param .u64 SwapUpperLower_param_3,
	.param .u32 SwapUpperLower_param_4,
	.param .u32 SwapUpperLower_param_5,
	.param .u32 SwapUpperLower_param_6
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r26, [SwapUpperLower_param_1];
	ld.param.u64 	%rd3, [SwapUpperLower_param_2];
	ld.param.u64 	%rd4, [SwapUpperLower_param_3];
	ld.param.u32 	%r28, [SwapUpperLower_param_4];
	ld.param.u32 	%r27, [SwapUpperLower_param_5];
	ld.param.u32 	%r25, [SwapUpperLower_param_0];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %tid.x;
	mad.lo.s32 	%r48, %r1, %r29, %r30;
	shr.u32 	%r31, %r25, 31;
	add.s32 	%r32, %r25, %r31;
	shr.s32 	%r3, %r32, 1;
	setp.gt.s32	%p1, %r28, 0;
	@%p1 bra 	BB2_7;
	bra.uni 	BB2_1;

BB2_7:
	setp.ge.s32	%p9, %r48, %r3;
	@%p9 bra 	BB2_13;

	setp.gt.s32	%p10, %r27, 0;
	selp.b32	%r14, 0, %r3, %p10;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r43, %tid.y;
	mad.lo.s32 	%r15, %r41, %r42, %r43;
	mov.u32 	%r44, %nctaid.y;
	mul.lo.s32 	%r16, %r44, %r41;
	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r17, %r45, %r1;

BB2_9:
	add.s32 	%r19, %r14, %r48;
	setp.ge.s32	%p11, %r19, %r25;
	setp.ge.s32	%p12, %r48, %r25;
	or.pred  	%p13, %p11, %p12;
	setp.ge.s32	%p14, %r15, %r26;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB2_12;

	mul.lo.s32 	%r20, %r48, %r26;
	mul.lo.s32 	%r21, %r19, %r26;
	mov.u32 	%r51, %r15;

BB2_11:
	add.s32 	%r46, %r51, %r20;
	mul.wide.s32 	%rd10, %r46, 4;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.f32 	%f3, [%rd11];
	add.s32 	%r47, %r51, %r21;
	mul.wide.s32 	%rd12, %r47, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.f32 	%f4, [%rd13];
	st.global.f32 	[%rd11], %f4;
	add.s64 	%rd14, %rd1, %rd10;
	st.global.f32 	[%rd14], %f3;
	add.s32 	%r51, %r16, %r51;
	setp.lt.s32	%p16, %r51, %r26;
	@%p16 bra 	BB2_11;

BB2_12:
	add.s32 	%r48, %r17, %r48;
	setp.lt.s32	%p17, %r48, %r3;
	@%p17 bra 	BB2_9;
	bra.uni 	BB2_13;

BB2_1:
	setp.ge.s32	%p2, %r48, %r3;
	@%p2 bra 	BB2_13;

	mov.u32 	%r33, %ctaid.y;
	mov.u32 	%r34, %ntid.y;
	mov.u32 	%r35, %tid.y;
	mad.lo.s32 	%r4, %r34, %r33, %r35;
	mov.u32 	%r36, %nctaid.y;
	mul.lo.s32 	%r5, %r36, %r34;
	mov.u32 	%r37, %nctaid.x;
	mul.lo.s32 	%r6, %r37, %r1;

BB2_3:
	add.s32 	%r8, %r3, %r48;
	setp.ge.s32	%p3, %r8, %r25;
	setp.ge.s32	%p4, %r4, %r26;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB2_6;

	mul.lo.s32 	%r9, %r8, %r26;
	setp.gt.s32	%p6, %r27, 0;
	selp.b32	%r38, %r48, %r8, %p6;
	mul.lo.s32 	%r10, %r38, %r26;
	mov.u32 	%r49, %r4;

BB2_5:
	add.s32 	%r39, %r49, %r9;
	mul.wide.s32 	%rd5, %r39, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.f32 	%f1, [%rd6];
	add.s32 	%r40, %r49, %r10;
	mul.wide.s32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f2, [%rd8];
	st.global.f32 	[%rd6], %f2;
	add.s64 	%rd9, %rd1, %rd5;
	st.global.f32 	[%rd9], %f1;
	add.s32 	%r49, %r5, %r49;
	setp.lt.s32	%p7, %r49, %r26;
	@%p7 bra 	BB2_5;

BB2_6:
	add.s32 	%r48, %r6, %r48;
	setp.lt.s32	%p8, %r48, %r3;
	@%p8 bra 	BB2_3;

BB2_13:
	ret;
}

	// .globl	ShapetoBatch4DNHWC
.visible .entry ShapetoBatch4DNHWC(
	.param .u32 ShapetoBatch4DNHWC_param_0,
	.param .u32 ShapetoBatch4DNHWC_param_1,
	.param .u32 ShapetoBatch4DNHWC_param_2,
	.param .u32 ShapetoBatch4DNHWC_param_3,
	.param .u32 ShapetoBatch4DNHWC_param_4,
	.param .u32 ShapetoBatch4DNHWC_param_5,
	.param .u32 ShapetoBatch4DNHWC_param_6,
	.param .u32 ShapetoBatch4DNHWC_param_7,
	.param .u32 ShapetoBatch4DNHWC_param_8,
	.param .u32 ShapetoBatch4DNHWC_param_9,
	.param .u32 ShapetoBatch4DNHWC_param_10,
	.param .u32 ShapetoBatch4DNHWC_param_11,
	.param .u64 ShapetoBatch4DNHWC_param_12,
	.param .u64 ShapetoBatch4DNHWC_param_13,
	.param .u32 ShapetoBatch4DNHWC_param_14,
	.param .u32 ShapetoBatch4DNHWC_param_15,
	.param .u8 ShapetoBatch4DNHWC_param_16
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r51, [ShapetoBatch4DNHWC_param_0];
	ld.param.u32 	%r52, [ShapetoBatch4DNHWC_param_1];
	ld.param.u32 	%r53, [ShapetoBatch4DNHWC_param_2];
	ld.param.u32 	%r54, [ShapetoBatch4DNHWC_param_3];
	ld.param.u32 	%r55, [ShapetoBatch4DNHWC_param_4];
	ld.param.u32 	%r56, [ShapetoBatch4DNHWC_param_5];
	ld.param.u32 	%r57, [ShapetoBatch4DNHWC_param_6];
	ld.param.u32 	%r58, [ShapetoBatch4DNHWC_param_7];
	ld.param.u32 	%r59, [ShapetoBatch4DNHWC_param_8];
	ld.param.u32 	%r60, [ShapetoBatch4DNHWC_param_9];
	ld.param.u32 	%r61, [ShapetoBatch4DNHWC_param_10];
	ld.param.u32 	%r62, [ShapetoBatch4DNHWC_param_11];
	ld.param.u64 	%rd4, [ShapetoBatch4DNHWC_param_12];
	ld.param.u64 	%rd5, [ShapetoBatch4DNHWC_param_13];
	ld.param.u32 	%r63, [ShapetoBatch4DNHWC_param_14];
	ld.param.u32 	%r64, [ShapetoBatch4DNHWC_param_15];
	ld.param.s8 	%rs1, [ShapetoBatch4DNHWC_param_16];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	setp.lt.s32	%p4, %r56, 1;
	@%p4 bra 	BB3_27;

	mov.u32 	%r66, %ctaid.x;
	mov.u32 	%r67, %ntid.x;
	mov.u32 	%r68, %tid.x;
	mad.lo.s32 	%r1, %r67, %r66, %r68;
	mov.u32 	%r69, %ntid.y;
	mov.u32 	%r70, %ctaid.y;
	mov.u32 	%r71, %tid.y;
	mad.lo.s32 	%r2, %r69, %r70, %r71;
	mov.u32 	%r72, %nctaid.x;
	mul.lo.s32 	%r3, %r72, %r67;
	mov.u32 	%r73, %ntid.z;
	mov.u32 	%r74, %ctaid.z;
	mov.u32 	%r75, %tid.z;
	mad.lo.s32 	%r4, %r73, %r74, %r75;
	mov.u32 	%r76, %nctaid.y;
	mul.lo.s32 	%r5, %r76, %r69;
	mov.u32 	%r77, %nctaid.z;
	mul.lo.s32 	%r10, %r77, %r73;
	mad.lo.s32 	%r9, %r74, %r73, %r75;
	mov.u32 	%r90, 0;
	and.b16  	%rs2, %rs1, 255;

BB3_2:
	setp.lt.s32	%p5, %r59, 1;
	@%p5 bra 	BB3_26;

	mul.lo.s32 	%r79, %r57, %r90;
	add.s32 	%r14, %r4, %r79;
	mul.lo.s32 	%r80, %r58, %r90;
	add.s32 	%r15, %r4, %r80;
	mov.u32 	%r91, 0;

BB3_4:
	setp.lt.s32	%p6, %r60, 1;
	@%p6 bra 	BB3_25;

	mul.lo.s32 	%r19, %r91, %r61;
	mul.lo.s32 	%r20, %r60, %r91;
	mov.u32 	%r92, 0;

BB3_6:
	setp.ge.s32	%p7, %r1, %r51;
	@%p7 bra 	BB3_24;

	mul.lo.s32 	%r22, %r92, %r62;
	add.s32 	%r82, %r20, %r92;
	mul.lo.s32 	%r23, %r51, %r82;
	mov.u32 	%r93, %r1;

BB3_8:
	setp.ge.s32	%p8, %r2, %r52;
	@%p8 bra 	BB3_23;

	setp.gt.s32	%p9, %r64, 0;
	add.s32 	%r25, %r93, %r19;
	setp.lt.s32	%p10, %r25, %r54;
	and.pred  	%p1, %p9, %p10;
	add.s32 	%r83, %r23, %r93;
	mul.lo.s32 	%r26, %r52, %r83;
	mul.lo.s32 	%r27, %r54, %r25;
	mov.u32 	%r94, %r2;

BB3_10:
	setp.ge.s32	%p11, %r4, %r53;
	@%p11 bra 	BB3_22;

	add.s32 	%r29, %r94, %r22;
	setp.eq.s16	%p12, %rs2, 0;
	@%p12 bra 	BB3_20;
	bra.uni 	BB3_12;

BB3_20:
	add.s32 	%r88, %r29, %r27;
	mad.lo.s32 	%r99, %r53, %r88, %r15;
	add.s32 	%r89, %r26, %r94;
	mad.lo.s32 	%r98, %r53, %r89, %r14;
	mov.u32 	%r100, %r9;

BB3_21:
	mul.wide.s32 	%rd9, %r98, 4;
	add.s64 	%rd10, %rd2, %rd9;
	mul.wide.s32 	%rd11, %r99, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f2, [%rd12];
	ld.global.f32 	%f3, [%rd10];
	add.f32 	%f4, %f3, %f2;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r99, %r99, %r10;
	add.s32 	%r98, %r98, %r10;
	add.s32 	%r100, %r100, %r10;
	setp.lt.s32	%p17, %r100, %r53;
	@%p17 bra 	BB3_21;
	bra.uni 	BB3_22;

BB3_12:
	setp.lt.s32	%p13, %r29, %r55;
	add.s32 	%r84, %r26, %r94;
	mad.lo.s32 	%r97, %r53, %r84, %r14;
	add.s32 	%r85, %r29, %r27;
	mad.lo.s32 	%r96, %r53, %r85, %r15;
	setp.gt.s32	%p14, %r63, 0;
	and.pred  	%p2, %p14, %p13;
	and.pred  	%p3, %p10, %p13;
	mov.u32 	%r95, %r9;

BB3_13:
	mul.wide.s32 	%rd6, %r97, 4;
	add.s64 	%rd3, %rd2, %rd6;
	@%p3 bra 	BB3_18;
	bra.uni 	BB3_14;

BB3_18:
	mul.wide.s32 	%rd7, %r96, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f1, [%rd8];
	st.global.f32 	[%rd3], %f1;
	bra.uni 	BB3_19;

BB3_14:
	@!%p2 bra 	BB3_16;
	bra.uni 	BB3_15;

BB3_15:
	mov.u32 	%r86, 0;
	st.global.u32 	[%rd3], %r86;

BB3_16:
	@!%p1 bra 	BB3_19;
	bra.uni 	BB3_17;

BB3_17:
	mov.u32 	%r87, 0;
	st.global.u32 	[%rd3], %r87;

BB3_19:
	add.s32 	%r97, %r97, %r10;
	add.s32 	%r96, %r96, %r10;
	add.s32 	%r95, %r95, %r10;
	setp.lt.s32	%p16, %r95, %r53;
	@%p16 bra 	BB3_13;

BB3_22:
	add.s32 	%r94, %r5, %r94;
	setp.lt.s32	%p18, %r94, %r52;
	@%p18 bra 	BB3_10;

BB3_23:
	add.s32 	%r93, %r3, %r93;
	setp.lt.s32	%p19, %r93, %r51;
	@%p19 bra 	BB3_8;

BB3_24:
	add.s32 	%r92, %r92, 1;
	setp.lt.s32	%p20, %r92, %r60;
	@%p20 bra 	BB3_6;

BB3_25:
	add.s32 	%r91, %r91, 1;
	setp.lt.s32	%p21, %r91, %r59;
	@%p21 bra 	BB3_4;

BB3_26:
	add.s32 	%r90, %r90, 1;
	setp.lt.s32	%p22, %r90, %r56;
	@%p22 bra 	BB3_2;

BB3_27:
	ret;
}

	// .globl	ShapetoBatch4DNCHW
.visible .entry ShapetoBatch4DNCHW(
	.param .u32 ShapetoBatch4DNCHW_param_0,
	.param .u32 ShapetoBatch4DNCHW_param_1,
	.param .u32 ShapetoBatch4DNCHW_param_2,
	.param .u32 ShapetoBatch4DNCHW_param_3,
	.param .u32 ShapetoBatch4DNCHW_param_4,
	.param .u32 ShapetoBatch4DNCHW_param_5,
	.param .u32 ShapetoBatch4DNCHW_param_6,
	.param .u32 ShapetoBatch4DNCHW_param_7,
	.param .u32 ShapetoBatch4DNCHW_param_8,
	.param .u32 ShapetoBatch4DNCHW_param_9,
	.param .u32 ShapetoBatch4DNCHW_param_10,
	.param .u32 ShapetoBatch4DNCHW_param_11,
	.param .u64 ShapetoBatch4DNCHW_param_12,
	.param .u64 ShapetoBatch4DNCHW_param_13,
	.param .u32 ShapetoBatch4DNCHW_param_14,
	.param .u32 ShapetoBatch4DNCHW_param_15,
	.param .u8 ShapetoBatch4DNCHW_param_16
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<103>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r52, [ShapetoBatch4DNCHW_param_0];
	ld.param.u32 	%r53, [ShapetoBatch4DNCHW_param_1];
	ld.param.u32 	%r54, [ShapetoBatch4DNCHW_param_2];
	ld.param.u32 	%r55, [ShapetoBatch4DNCHW_param_3];
	ld.param.u32 	%r56, [ShapetoBatch4DNCHW_param_4];
	ld.param.u32 	%r57, [ShapetoBatch4DNCHW_param_5];
	ld.param.u32 	%r58, [ShapetoBatch4DNCHW_param_6];
	ld.param.u32 	%r59, [ShapetoBatch4DNCHW_param_7];
	ld.param.u32 	%r60, [ShapetoBatch4DNCHW_param_8];
	ld.param.u32 	%r61, [ShapetoBatch4DNCHW_param_9];
	ld.param.u32 	%r62, [ShapetoBatch4DNCHW_param_10];
	ld.param.u32 	%r63, [ShapetoBatch4DNCHW_param_11];
	ld.param.u64 	%rd4, [ShapetoBatch4DNCHW_param_12];
	ld.param.u64 	%rd5, [ShapetoBatch4DNCHW_param_13];
	ld.param.u32 	%r64, [ShapetoBatch4DNCHW_param_14];
	ld.param.u32 	%r65, [ShapetoBatch4DNCHW_param_15];
	ld.param.s8 	%rs1, [ShapetoBatch4DNCHW_param_16];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	setp.lt.s32	%p2, %r57, 1;
	@%p2 bra 	BB4_27;

	mov.u32 	%r67, %ctaid.x;
	mov.u32 	%r68, %ntid.x;
	mov.u32 	%r69, %tid.x;
	mad.lo.s32 	%r1, %r68, %r67, %r69;
	mov.u32 	%r70, %ntid.y;
	mov.u32 	%r71, %ctaid.y;
	mov.u32 	%r72, %tid.y;
	mad.lo.s32 	%r2, %r70, %r71, %r72;
	mov.u32 	%r73, %nctaid.x;
	mul.lo.s32 	%r3, %r73, %r68;
	mov.u32 	%r74, %ntid.z;
	mov.u32 	%r75, %ctaid.z;
	mov.u32 	%r76, %tid.z;
	mad.lo.s32 	%r4, %r74, %r75, %r76;
	mov.u32 	%r77, %nctaid.y;
	mul.lo.s32 	%r5, %r77, %r70;
	mov.u32 	%r78, %nctaid.z;
	mul.lo.s32 	%r11, %r78, %r74;
	mad.lo.s32 	%r10, %r75, %r74, %r76;
	mov.u32 	%r91, 0;
	and.b16  	%rs2, %rs1, 255;

BB4_2:
	setp.lt.s32	%p3, %r60, 1;
	@%p3 bra 	BB4_26;

	mul.lo.s32 	%r80, %r58, %r91;
	add.s32 	%r14, %r4, %r80;
	mad.lo.s32 	%r15, %r59, %r91, %r4;
	mov.u32 	%r92, 0;

BB4_4:
	setp.lt.s32	%p4, %r61, 1;
	@%p4 bra 	BB4_25;

	mul.lo.s32 	%r18, %r92, %r62;
	mul.lo.s32 	%r19, %r61, %r92;
	mov.u32 	%r93, 0;

BB4_6:
	setp.ge.s32	%p5, %r1, %r52;
	@%p5 bra 	BB4_24;

	add.s32 	%r82, %r19, %r93;
	mul.lo.s32 	%r21, %r54, %r82;
	mul.lo.s32 	%r83, %r63, %r93;
	add.s32 	%r22, %r15, %r83;
	add.s32 	%r23, %r4, %r83;
	mov.u32 	%r94, %r1;

BB4_8:
	setp.ge.s32	%p6, %r2, %r53;
	@%p6 bra 	BB4_23;

	add.s32 	%r84, %r21, %r94;
	mul.lo.s32 	%r25, %r52, %r84;
	mul.lo.s32 	%r26, %r55, %r94;
	mov.u32 	%r95, %r2;

BB4_10:
	setp.ge.s32	%p7, %r4, %r54;
	@%p7 bra 	BB4_22;

	add.s32 	%r28, %r95, %r18;
	setp.eq.s16	%p8, %rs2, 0;
	@%p8 bra 	BB4_20;
	bra.uni 	BB4_12;

BB4_20:
	add.s32 	%r89, %r25, %r95;
	mad.lo.s32 	%r101, %r53, %r89, %r14;
	add.s32 	%r90, %r28, %r26;
	mad.lo.s32 	%r100, %r56, %r90, %r22;
	mov.u32 	%r102, %r10;

BB4_21:
	mul.wide.s32 	%rd9, %r101, 4;
	add.s64 	%rd10, %rd2, %rd9;
	mul.wide.s32 	%rd11, %r100, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f2, [%rd12];
	ld.global.f32 	%f3, [%rd10];
	add.f32 	%f4, %f3, %f2;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r101, %r101, %r11;
	add.s32 	%r100, %r100, %r11;
	add.s32 	%r102, %r102, %r11;
	setp.lt.s32	%p18, %r102, %r54;
	@%p18 bra 	BB4_21;
	bra.uni 	BB4_22;

BB4_12:
	add.s32 	%r85, %r25, %r95;
	mad.lo.s32 	%r99, %r53, %r85, %r14;
	add.s32 	%r86, %r28, %r26;
	mad.lo.s32 	%r98, %r56, %r86, %r22;
	setp.lt.s32	%p9, %r28, %r55;
	setp.gt.s32	%p10, %r65, 0;
	and.pred  	%p1, %p10, %p9;
	mov.u32 	%r96, %r10;
	mov.u32 	%r97, %r23;

BB4_13:
	setp.lt.s32	%p11, %r97, %r56;
	and.pred  	%p13, %p9, %p11;
	mul.wide.s32 	%rd6, %r99, 4;
	add.s64 	%rd3, %rd2, %rd6;
	@%p13 bra 	BB4_18;
	bra.uni 	BB4_14;

BB4_18:
	mul.wide.s32 	%rd7, %r98, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f1, [%rd8];
	st.global.f32 	[%rd3], %f1;
	bra.uni 	BB4_19;

BB4_14:
	setp.ge.s32	%p14, %r97, %r56;
	setp.lt.s32	%p15, %r64, 1;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	BB4_16;

	mov.u32 	%r87, 0;
	st.global.u32 	[%rd3], %r87;

BB4_16:
	@!%p1 bra 	BB4_19;
	bra.uni 	BB4_17;

BB4_17:
	mov.u32 	%r88, 0;
	st.global.u32 	[%rd3], %r88;

BB4_19:
	add.s32 	%r99, %r99, %r11;
	add.s32 	%r98, %r98, %r11;
	add.s32 	%r97, %r97, %r11;
	add.s32 	%r96, %r96, %r11;
	setp.lt.s32	%p17, %r96, %r54;
	@%p17 bra 	BB4_13;

BB4_22:
	add.s32 	%r95, %r5, %r95;
	setp.lt.s32	%p19, %r95, %r53;
	@%p19 bra 	BB4_10;

BB4_23:
	add.s32 	%r94, %r3, %r94;
	setp.lt.s32	%p20, %r94, %r52;
	@%p20 bra 	BB4_8;

BB4_24:
	add.s32 	%r93, %r93, 1;
	setp.lt.s32	%p21, %r93, %r61;
	@%p21 bra 	BB4_6;

BB4_25:
	add.s32 	%r92, %r92, 1;
	setp.lt.s32	%p22, %r92, %r60;
	@%p22 bra 	BB4_4;

BB4_26:
	add.s32 	%r91, %r91, 1;
	setp.lt.s32	%p23, %r91, %r57;
	@%p23 bra 	BB4_2;

BB4_27:
	ret;
}

	// .globl	NearestNeighborNHWC
.visible .entry NearestNeighborNHWC(
	.param .u32 NearestNeighborNHWC_param_0,
	.param .u32 NearestNeighborNHWC_param_1,
	.param .u64 NearestNeighborNHWC_param_2,
	.param .u32 NearestNeighborNHWC_param_3,
	.param .u32 NearestNeighborNHWC_param_4,
	.param .u32 NearestNeighborNHWC_param_5,
	.param .u32 NearestNeighborNHWC_param_6,
	.param .u32 NearestNeighborNHWC_param_7,
	.param .f32 NearestNeighborNHWC_param_8,
	.param .f32 NearestNeighborNHWC_param_9,
	.param .u64 NearestNeighborNHWC_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNHWC_param_0];
	ld.param.u32 	%r9, [NearestNeighborNHWC_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNHWC_param_2];
	ld.param.u32 	%r10, [NearestNeighborNHWC_param_3];
	ld.param.u32 	%r11, [NearestNeighborNHWC_param_4];
	ld.param.u32 	%r12, [NearestNeighborNHWC_param_5];
	ld.param.u32 	%r13, [NearestNeighborNHWC_param_6];
	ld.param.u32 	%r14, [NearestNeighborNHWC_param_7];
	ld.param.f32 	%f13, [NearestNeighborNHWC_param_8];
	ld.param.f32 	%f14, [NearestNeighborNHWC_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNHWC_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r37, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r37, %r9;
	@%p1 bra 	BB5_11;

	add.s32 	%r18, %r10, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r11, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;

BB5_2:
	rem.s32 	%r3, %r37, %r12;
	div.s32 	%r20, %r37, %r12;
	rem.s32 	%r4, %r20, %r14;
	div.s32 	%r21, %r20, %r14;
	rem.s32 	%r22, %r21, %r13;
	div.s32 	%r5, %r21, %r13;
	cvt.rn.f32.s32	%f15, %r22;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB5_5;
	bra.uni 	BB5_3;

BB5_5:
	cvt.rmi.f32.f32	%f28, %f3;
	bra.uni 	BB5_6;

BB5_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r23, %f3;
	and.b32  	%r24, %r23, -2147483648;
	or.b32  	%r25, %r24, 1056964608;
	mov.b32 	 %f17, %r25;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f28, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB5_6;

	cvt.rzi.f32.f32	%f28, %f3;

BB5_6:
	min.f32 	%f20, %f28, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r4;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB5_9;
	bra.uni 	BB5_7;

BB5_9:
	cvt.rmi.f32.f32	%f29, %f8;
	bra.uni 	BB5_10;

BB5_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r26, %f8;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	 %f23, %r28;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f29, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB5_10;

	cvt.rzi.f32.f32	%f29, %f8;

BB5_10:
	mul.lo.s32 	%r29, %r11, %r10;
	mul.lo.s32 	%r30, %r29, %r12;
	min.f32 	%f26, %f29, %f2;
	cvt.rzi.s32.f32	%r31, %f26;
	mad.lo.s32 	%r32, %r6, %r11, %r31;
	mad.lo.s32 	%r33, %r32, %r12, %r3;
	mad.lo.s32 	%r34, %r30, %r5, %r33;
	mul.wide.s32 	%rd4, %r34, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	mul.wide.s32 	%rd7, %r37, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f27;
	mov.u32 	%r36, %nctaid.x;
	mad.lo.s32 	%r37, %r36, %r16, %r37;
	setp.lt.s32	%p8, %r37, %r9;
	@%p8 bra 	BB5_2;

BB5_11:
	ret;
}

	// .globl	NearestNeighborNCHW
.visible .entry NearestNeighborNCHW(
	.param .u32 NearestNeighborNCHW_param_0,
	.param .u32 NearestNeighborNCHW_param_1,
	.param .u64 NearestNeighborNCHW_param_2,
	.param .u32 NearestNeighborNCHW_param_3,
	.param .u32 NearestNeighborNCHW_param_4,
	.param .u32 NearestNeighborNCHW_param_5,
	.param .u32 NearestNeighborNCHW_param_6,
	.param .u32 NearestNeighborNCHW_param_7,
	.param .f32 NearestNeighborNCHW_param_8,
	.param .f32 NearestNeighborNCHW_param_9,
	.param .u64 NearestNeighborNCHW_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNCHW_param_0];
	ld.param.u32 	%r9, [NearestNeighborNCHW_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNCHW_param_2];
	ld.param.u32 	%r10, [NearestNeighborNCHW_param_3];
	ld.param.u32 	%r11, [NearestNeighborNCHW_param_4];
	ld.param.u32 	%r12, [NearestNeighborNCHW_param_5];
	ld.param.u32 	%r13, [NearestNeighborNCHW_param_6];
	ld.param.u32 	%r14, [NearestNeighborNCHW_param_7];
	ld.param.f32 	%f13, [NearestNeighborNCHW_param_8];
	ld.param.f32 	%f14, [NearestNeighborNCHW_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNCHW_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r36, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r36, %r9;
	@%p1 bra 	BB6_11;

	add.s32 	%r18, %r10, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r11, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;

BB6_2:
	rem.s32 	%r3, %r36, %r14;
	div.s32 	%r20, %r36, %r14;
	rem.s32 	%r21, %r20, %r13;
	div.s32 	%r4, %r20, %r13;
	rem.s32 	%r5, %r4, %r12;
	cvt.rn.f32.s32	%f15, %r21;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB6_5;
	bra.uni 	BB6_3;

BB6_5:
	cvt.rmi.f32.f32	%f28, %f3;
	bra.uni 	BB6_6;

BB6_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r22, %f3;
	and.b32  	%r23, %r22, -2147483648;
	or.b32  	%r24, %r23, 1056964608;
	mov.b32 	 %f17, %r24;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f28, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB6_6;

	cvt.rzi.f32.f32	%f28, %f3;

BB6_6:
	min.f32 	%f20, %f28, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r3;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB6_9;
	bra.uni 	BB6_7;

BB6_9:
	cvt.rmi.f32.f32	%f29, %f8;
	bra.uni 	BB6_10;

BB6_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r25, %f8;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, 1056964608;
	mov.b32 	 %f23, %r27;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f29, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB6_10;

	cvt.rzi.f32.f32	%f29, %f8;

BB6_10:
	sub.s32 	%r28, %r4, %r5;
	mul.lo.s32 	%r29, %r11, %r10;
	min.f32 	%f26, %f29, %f2;
	cvt.rzi.s32.f32	%r30, %f26;
	mad.lo.s32 	%r31, %r5, %r10, %r6;
	mad.lo.s32 	%r32, %r31, %r11, %r30;
	mad.lo.s32 	%r33, %r29, %r28, %r32;
	mul.wide.s32 	%rd4, %r33, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	mul.wide.s32 	%rd7, %r36, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f27;
	mov.u32 	%r35, %nctaid.x;
	mad.lo.s32 	%r36, %r35, %r16, %r36;
	setp.lt.s32	%p8, %r36, %r9;
	@%p8 bra 	BB6_2;

BB6_11:
	ret;
}

	// .globl	NearestNeighborNCHWBack
.visible .entry NearestNeighborNCHWBack(
	.param .u32 NearestNeighborNCHWBack_param_0,
	.param .u32 NearestNeighborNCHWBack_param_1,
	.param .u64 NearestNeighborNCHWBack_param_2,
	.param .u32 NearestNeighborNCHWBack_param_3,
	.param .u32 NearestNeighborNCHWBack_param_4,
	.param .u32 NearestNeighborNCHWBack_param_5,
	.param .u32 NearestNeighborNCHWBack_param_6,
	.param .u32 NearestNeighborNCHWBack_param_7,
	.param .f32 NearestNeighborNCHWBack_param_8,
	.param .f32 NearestNeighborNCHWBack_param_9,
	.param .u64 NearestNeighborNCHWBack_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNCHWBack_param_0];
	ld.param.u32 	%r9, [NearestNeighborNCHWBack_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNCHWBack_param_2];
	ld.param.u32 	%r10, [NearestNeighborNCHWBack_param_3];
	ld.param.u32 	%r11, [NearestNeighborNCHWBack_param_4];
	ld.param.u32 	%r12, [NearestNeighborNCHWBack_param_5];
	ld.param.u32 	%r13, [NearestNeighborNCHWBack_param_6];
	ld.param.u32 	%r14, [NearestNeighborNCHWBack_param_7];
	ld.param.f32 	%f13, [NearestNeighborNCHWBack_param_8];
	ld.param.f32 	%f14, [NearestNeighborNCHWBack_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNCHWBack_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r36, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r36, %r9;
	@%p1 bra 	BB7_11;

	add.s32 	%r18, %r13, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r14, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;

BB7_2:
	rem.s32 	%r3, %r36, %r11;
	div.s32 	%r20, %r36, %r11;
	rem.s32 	%r21, %r20, %r10;
	div.s32 	%r4, %r20, %r10;
	rem.s32 	%r5, %r4, %r12;
	cvt.rn.f32.s32	%f15, %r21;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB7_5;
	bra.uni 	BB7_3;

BB7_5:
	cvt.rmi.f32.f32	%f29, %f3;
	bra.uni 	BB7_6;

BB7_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r22, %f3;
	and.b32  	%r23, %r22, -2147483648;
	or.b32  	%r24, %r23, 1056964608;
	mov.b32 	 %f17, %r24;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f29, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB7_6;

	cvt.rzi.f32.f32	%f29, %f3;

BB7_6:
	min.f32 	%f20, %f29, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r3;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB7_9;
	bra.uni 	BB7_7;

BB7_9:
	cvt.rmi.f32.f32	%f30, %f8;
	bra.uni 	BB7_10;

BB7_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r25, %f8;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, 1056964608;
	mov.b32 	 %f23, %r27;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f30, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB7_10;

	cvt.rzi.f32.f32	%f30, %f8;

BB7_10:
	sub.s32 	%r28, %r4, %r5;
	mul.lo.s32 	%r29, %r11, %r10;
	min.f32 	%f26, %f30, %f2;
	cvt.rzi.s32.f32	%r30, %f26;
	mad.lo.s32 	%r31, %r5, %r13, %r6;
	mad.lo.s32 	%r32, %r31, %r14, %r30;
	mad.lo.s32 	%r33, %r29, %r28, %r32;
	mul.wide.s32 	%rd4, %r33, 4;
	add.s64 	%rd5, %rd3, %rd4;
	mul.wide.s32 	%rd7, %r36, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f27, [%rd8];
	atom.global.add.f32 	%f28, [%rd5], %f27;
	mov.u32 	%r35, %nctaid.x;
	mad.lo.s32 	%r36, %r35, %r16, %r36;
	setp.lt.s32	%p8, %r36, %r9;
	@%p8 bra 	BB7_2;

BB7_11:
	ret;
}

	// .globl	NearestNeighborNHWCBack
.visible .entry NearestNeighborNHWCBack(
	.param .u32 NearestNeighborNHWCBack_param_0,
	.param .u32 NearestNeighborNHWCBack_param_1,
	.param .u64 NearestNeighborNHWCBack_param_2,
	.param .u32 NearestNeighborNHWCBack_param_3,
	.param .u32 NearestNeighborNHWCBack_param_4,
	.param .u32 NearestNeighborNHWCBack_param_5,
	.param .u32 NearestNeighborNHWCBack_param_6,
	.param .u32 NearestNeighborNHWCBack_param_7,
	.param .f32 NearestNeighborNHWCBack_param_8,
	.param .f32 NearestNeighborNHWCBack_param_9,
	.param .u64 NearestNeighborNHWCBack_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNHWCBack_param_0];
	ld.param.u32 	%r9, [NearestNeighborNHWCBack_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNHWCBack_param_2];
	ld.param.u32 	%r10, [NearestNeighborNHWCBack_param_3];
	ld.param.u32 	%r11, [NearestNeighborNHWCBack_param_4];
	ld.param.u32 	%r12, [NearestNeighborNHWCBack_param_5];
	ld.param.u32 	%r13, [NearestNeighborNHWCBack_param_6];
	ld.param.u32 	%r14, [NearestNeighborNHWCBack_param_7];
	ld.param.f32 	%f13, [NearestNeighborNHWCBack_param_8];
	ld.param.f32 	%f14, [NearestNeighborNHWCBack_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNHWCBack_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r37, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r37, %r9;
	@%p1 bra 	BB8_11;

	add.s32 	%r18, %r13, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r14, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;

BB8_2:
	rem.s32 	%r3, %r37, %r12;
	div.s32 	%r20, %r37, %r12;
	rem.s32 	%r4, %r20, %r11;
	div.s32 	%r21, %r20, %r11;
	rem.s32 	%r22, %r21, %r10;
	div.s32 	%r5, %r21, %r10;
	cvt.rn.f32.s32	%f15, %r22;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB8_5;
	bra.uni 	BB8_3;

BB8_5:
	cvt.rmi.f32.f32	%f29, %f3;
	bra.uni 	BB8_6;

BB8_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r23, %f3;
	and.b32  	%r24, %r23, -2147483648;
	or.b32  	%r25, %r24, 1056964608;
	mov.b32 	 %f17, %r25;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f29, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB8_6;

	cvt.rzi.f32.f32	%f29, %f3;

BB8_6:
	min.f32 	%f20, %f29, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r4;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB8_9;
	bra.uni 	BB8_7;

BB8_9:
	cvt.rmi.f32.f32	%f30, %f8;
	bra.uni 	BB8_10;

BB8_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r26, %f8;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	 %f23, %r28;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f30, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB8_10;

	cvt.rzi.f32.f32	%f30, %f8;

BB8_10:
	mul.lo.s32 	%r29, %r11, %r10;
	mul.lo.s32 	%r30, %r29, %r12;
	min.f32 	%f26, %f30, %f2;
	cvt.rzi.s32.f32	%r31, %f26;
	mad.lo.s32 	%r32, %r6, %r14, %r31;
	mad.lo.s32 	%r33, %r32, %r12, %r3;
	mad.lo.s32 	%r34, %r30, %r5, %r33;
	mul.wide.s32 	%rd4, %r34, 4;
	add.s64 	%rd5, %rd3, %rd4;
	mul.wide.s32 	%rd7, %r37, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f27, [%rd8];
	atom.global.add.f32 	%f28, [%rd5], %f27;
	mov.u32 	%r36, %nctaid.x;
	mad.lo.s32 	%r37, %r36, %r16, %r37;
	setp.lt.s32	%p8, %r37, %r9;
	@%p8 bra 	BB8_2;

BB8_11:
	ret;
}

	// .globl	AdaGrad
.visible .entry AdaGrad(
	.param .u32 AdaGrad_param_0,
	.param .u64 AdaGrad_param_1,
	.param .u64 AdaGrad_param_2,
	.param .u64 AdaGrad_param_3,
	.param .f32 AdaGrad_param_4,
	.param .f32 AdaGrad_param_5,
	.param .f32 AdaGrad_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [AdaGrad_param_0];
	ld.param.u64 	%rd4, [AdaGrad_param_1];
	ld.param.u64 	%rd5, [AdaGrad_param_2];
	ld.param.u64 	%rd6, [AdaGrad_param_3];
	ld.param.f32 	%f1, [AdaGrad_param_4];
	ld.param.f32 	%f2, [AdaGrad_param_5];
	ld.param.f32 	%f3, [AdaGrad_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB9_3;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB9_2:
	mul.wide.s32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.f32 	%f4, [%rd9];
	ld.global.f32 	%f5, [%rd8];
	fma.rn.f32 	%f6, %f4, %f4, %f5;
	st.global.f32 	[%rd8], %f6;
	ld.global.f32 	%f7, [%rd9];
	mul.f32 	%f8, %f7, %f1;
	sqrt.rn.f32 	%f9, %f6;
	add.f32 	%f10, %f9, %f2;
	div.rn.f32 	%f11, %f8, %f10;
	add.s64 	%rd10, %rd1, %rd7;
	ld.global.f32 	%f12, [%rd10];
	sub.f32 	%f13, %f12, %f11;
	st.global.f32 	[%rd10], %f13;
	ld.global.f32 	%f14, [%rd9];
	mul.f32 	%f15, %f14, %f3;
	st.global.f32 	[%rd9], %f15;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p2, %r10, %r6;
	@%p2 bra 	BB9_2;

BB9_3:
	ret;
}

	// .globl	Adam
.visible .entry Adam(
	.param .u32 Adam_param_0,
	.param .u64 Adam_param_1,
	.param .u64 Adam_param_2,
	.param .u64 Adam_param_3,
	.param .u64 Adam_param_4,
	.param .f32 Adam_param_5,
	.param .f32 Adam_param_6,
	.param .f32 Adam_param_7,
	.param .f32 Adam_param_8,
	.param .f32 Adam_param_9,
	.param .f32 Adam_param_10,
	.param .f32 Adam_param_11
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<11>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r6, [Adam_param_0];
	ld.param.u64 	%rd4, [Adam_param_1];
	ld.param.u64 	%rd5, [Adam_param_2];
	ld.param.u64 	%rd6, [Adam_param_3];
	ld.param.u64 	%rd7, [Adam_param_4];
	ld.param.f32 	%f1, [Adam_param_5];
	ld.param.f32 	%f2, [Adam_param_6];
	ld.param.f32 	%f3, [Adam_param_7];
	ld.param.f32 	%f4, [Adam_param_8];
	ld.param.f32 	%f5, [Adam_param_9];
	ld.param.f32 	%f6, [Adam_param_10];
	ld.param.f32 	%f7, [Adam_param_11];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB10_3;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
	cvt.f64.f32	%fd3, %f2;
	mov.f64 	%fd4, 0d3FF0000000000000;
	sub.f64 	%fd1, %fd4, %fd3;
	cvt.f64.f32	%fd5, %f3;
	sub.f64 	%fd2, %fd4, %fd5;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd10, %rd7;

BB10_2:
	mul.wide.s32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.f32 	%f8, [%rd9];
	mul.f32 	%f9, %f8, %f2;
	cvt.f64.f32	%fd6, %f9;
	add.s64 	%rd11, %rd10, %rd8;
	ld.global.f32 	%f10, [%rd11];
	cvt.f64.f32	%fd7, %f10;
	fma.rn.f64 	%fd8, %fd1, %fd7, %fd6;
	cvt.rn.f32.f64	%f11, %fd8;
	st.global.f32 	[%rd9], %f11;
	div.rn.f32 	%f12, %f11, %f5;
	add.s64 	%rd12, %rd2, %rd8;
	ld.global.f32 	%f13, [%rd12];
	mul.f32 	%f14, %f13, %f3;
	cvt.f64.f32	%fd9, %f14;
	ld.global.f32 	%f15, [%rd11];
	mul.f32 	%f16, %f15, %f15;
	cvt.f64.f32	%fd10, %f16;
	fma.rn.f64 	%fd11, %fd2, %fd10, %fd9;
	cvt.rn.f32.f64	%f17, %fd11;
	st.global.f32 	[%rd12], %f17;
	div.rn.f32 	%f18, %f17, %f6;
	mul.f32 	%f19, %f12, %f1;
	sqrt.rn.f32 	%f20, %f18;
	add.f32 	%f21, %f20, %f4;
	div.rn.f32 	%f22, %f19, %f21;
	add.s64 	%rd13, %rd1, %rd8;
	ld.global.f32 	%f23, [%rd13];
	sub.f32 	%f24, %f23, %f22;
	st.global.f32 	[%rd13], %f24;
	ld.global.f32 	%f25, [%rd11];
	mul.f32 	%f26, %f25, %f7;
	st.global.f32 	[%rd11], %f26;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p2, %r10, %r6;
	@%p2 bra 	BB10_2;

BB10_3:
	ret;
}

	// .globl	AdaDelta
.visible .entry AdaDelta(
	.param .u32 AdaDelta_param_0,
	.param .u64 AdaDelta_param_1,
	.param .u64 AdaDelta_param_2,
	.param .u64 AdaDelta_param_3,
	.param .u64 AdaDelta_param_4,
	.param .f32 AdaDelta_param_5,
	.param .f32 AdaDelta_param_6,
	.param .f32 AdaDelta_param_7,
	.param .f32 AdaDelta_param_8
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r5, [AdaDelta_param_0];
	ld.param.u64 	%rd2, [AdaDelta_param_1];
	ld.param.u64 	%rd3, [AdaDelta_param_2];
	ld.param.u64 	%rd4, [AdaDelta_param_3];
	ld.param.u64 	%rd5, [AdaDelta_param_4];
	ld.param.f32 	%f1, [AdaDelta_param_6];
	ld.param.f32 	%f2, [AdaDelta_param_7];
	ld.param.f32 	%f3, [AdaDelta_param_8];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r14, %r7, %r6, %r8;
	setp.ge.s32	%p1, %r14, %r5;
	@%p1 bra 	BB11_3;

	cvt.f64.f32	%fd2, %f2;
	mov.f64 	%fd3, 0d3FF0000000000000;
	sub.f64 	%fd1, %fd3, %fd2;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r1, %r11, %r7;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd8, %rd5;
	cvta.to.global.u64 	%rd10, %rd4;
	cvta.to.global.u64 	%rd12, %rd2;

BB11_2:
	mul.wide.s32 	%rd6, %r14, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f4, [%rd7];
	mul.f32 	%f5, %f4, %f2;
	cvt.f64.f32	%fd4, %f5;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f6, [%rd9];
	cvt.f64.f32	%fd5, %f6;
	mul.f64 	%fd6, %fd1, %fd5;
	fma.rn.f64 	%fd7, %fd5, %fd6, %fd4;
	cvt.rn.f32.f64	%f7, %fd7;
	st.global.f32 	[%rd7], %f7;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f8, [%rd11];
	add.f32 	%f9, %f8, %f1;
	add.f32 	%f10, %f7, %f1;
	div.rn.f32 	%f11, %f9, %f10;
	sqrt.rn.f32 	%f12, %f11;
	ld.global.f32 	%f13, [%rd9];
	mul.f32 	%f14, %f12, %f13;
	mov.f32 	%f15, 0f3F800000;
	sub.f32 	%f16, %f15, %f2;
	mul.f32 	%f17, %f16, %f14;
	mul.f32 	%f18, %f14, %f17;
	fma.rn.f32 	%f19, %f8, %f2, %f18;
	st.global.f32 	[%rd11], %f19;
	add.s64 	%rd13, %rd12, %rd6;
	ld.global.f32 	%f20, [%rd13];
	sub.f32 	%f21, %f20, %f14;
	st.global.f32 	[%rd13], %f21;
	ld.global.f32 	%f22, [%rd9];
	mul.f32 	%f23, %f22, %f3;
	st.global.f32 	[%rd9], %f23;
	add.s32 	%r14, %r1, %r14;
	setp.lt.s32	%p2, %r14, %r5;
	@%p2 bra 	BB11_2;

BB11_3:
	ret;
}

	// .globl	L1L2
.visible .entry L1L2(
	.param .u32 L1L2_param_0,
	.param .u64 L1L2_param_1,
	.param .u64 L1L2_param_2,
	.param .u64 L1L2_param_3,
	.param .u64 L1L2_param_4,
	.param .f32 L1L2_param_5,
	.param .f32 L1L2_param_6,
	.param .f32 L1L2_param_7
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r6, [L1L2_param_0];
	ld.param.u64 	%rd5, [L1L2_param_1];
	ld.param.u64 	%rd6, [L1L2_param_2];
	ld.param.u64 	%rd7, [L1L2_param_3];
	ld.param.u64 	%rd8, [L1L2_param_4];
	ld.param.f32 	%f1, [L1L2_param_5];
	ld.param.f32 	%f2, [L1L2_param_6];
	ld.param.f32 	%f3, [L1L2_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB12_3;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd8;

BB12_2:
	mul.wide.s32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f32 	%f4, [%rd10];
	abs.f32 	%f5, %f4;
	mul.f32 	%f6, %f5, %f2;
	atom.global.add.f32 	%f7, [%rd3], %f6;
	ld.global.f32 	%f8, [%rd10];
	mul.f32 	%f9, %f8, %f8;
	mul.f32 	%f10, %f9, %f3;
	mul.f32 	%f11, %f10, 0f3F000000;
	atom.global.add.f32 	%f12, [%rd4], %f11;
	ld.global.f32 	%f13, [%rd10];
	setp.gt.f32	%p2, %f13, 0f00000000;
	selp.f32	%f14, 0f3F800000, 0fBF800000, %p2;
	add.s64 	%rd11, %rd1, %rd9;
	ld.global.f32 	%f15, [%rd11];
	fma.rn.f32 	%f16, %f13, %f3, %f15;
	fma.rn.f32 	%f17, %f14, %f2, %f16;
	div.rn.f32 	%f18, %f17, %f1;
	st.global.f32 	[%rd11], %f18;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB12_2;

BB12_3:
	ret;
}

	// .globl	ThreshForward
.visible .entry ThreshForward(
	.param .u32 ThreshForward_param_0,
	.param .u32 ThreshForward_param_1,
	.param .u64 ThreshForward_param_2,
	.param .u64 ThreshForward_param_3,
	.param .u64 ThreshForward_param_4,
	.param .u64 ThreshForward_param_5,
	.param .u64 ThreshForward_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<21>;


	ld.param.u32 	%r8, [ThreshForward_param_0];
	ld.param.u32 	%r9, [ThreshForward_param_1];
	ld.param.u64 	%rd5, [ThreshForward_param_2];
	ld.param.u64 	%rd6, [ThreshForward_param_3];
	ld.param.u64 	%rd7, [ThreshForward_param_4];
	ld.param.u64 	%rd8, [ThreshForward_param_5];
	ld.param.u64 	%rd9, [ThreshForward_param_6];
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB13_8;

	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd5;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r18, 0;
	cvta.to.global.u64 	%rd12, %rd8;
	cvta.to.global.u64 	%rd15, %rd6;
	cvta.to.global.u64 	%rd18, %rd9;

BB13_2:
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r19, %r11, %r13, %r15;
	mul.lo.s32 	%r4, %r18, %r8;
	setp.ge.s32	%p2, %r19, %r8;
	@%p2 bra 	BB13_7;

BB13_3:
	add.s32 	%r16, %r19, %r4;
	mul.wide.s32 	%rd10, %r16, 4;
	add.s64 	%rd11, %rd2, %rd10;
	cvt.s64.s32	%rd3, %r19;
	mul.wide.s32 	%rd13, %r19, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f5, [%rd14];
	ld.global.f32 	%f1, [%rd11];
	setp.gt.f32	%p3, %f1, %f5;
	add.s64 	%rd4, %rd15, %rd10;
	@%p3 bra 	BB13_5;
	bra.uni 	BB13_4;

BB13_5:
	add.s64 	%rd20, %rd18, %rd13;
	ld.global.f32 	%f7, [%rd20];
	mul.f32 	%f8, %f1, %f7;
	bra.uni 	BB13_6;

BB13_4:
	add.s64 	%rd17, %rd1, %rd13;
	ld.global.f32 	%f6, [%rd17];
	mul.f32 	%f8, %f6, %f1;

BB13_6:
	st.global.f32 	[%rd4], %f8;
	cvt.u32.u64	%r17, %rd3;
	add.s32 	%r19, %r1, %r17;
	setp.lt.s32	%p4, %r19, %r8;
	@%p4 bra 	BB13_3;

BB13_7:
	add.s32 	%r18, %r18, 1;
	setp.lt.s32	%p5, %r18, %r9;
	@%p5 bra 	BB13_2;

BB13_8:
	ret;
}

	// .globl	ThreshBackward
.visible .entry ThreshBackward(
	.param .u32 ThreshBackward_param_0,
	.param .u32 ThreshBackward_param_1,
	.param .u64 ThreshBackward_param_2,
	.param .u64 ThreshBackward_param_3,
	.param .u64 ThreshBackward_param_4,
	.param .u64 ThreshBackward_param_5,
	.param .u64 ThreshBackward_param_6,
	.param .u64 ThreshBackward_param_7,
	.param .u64 ThreshBackward_param_8,
	.param .u64 ThreshBackward_param_9,
	.param .u64 ThreshBackward_param_10
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<43>;


	ld.param.u32 	%r6, [ThreshBackward_param_0];
	ld.param.u32 	%r7, [ThreshBackward_param_1];
	ld.param.u64 	%rd2, [ThreshBackward_param_2];
	ld.param.u64 	%rd3, [ThreshBackward_param_3];
	ld.param.u64 	%rd4, [ThreshBackward_param_4];
	ld.param.u64 	%rd5, [ThreshBackward_param_5];
	ld.param.u64 	%rd6, [ThreshBackward_param_6];
	ld.param.u64 	%rd7, [ThreshBackward_param_7];
	ld.param.u64 	%rd8, [ThreshBackward_param_8];
	ld.param.u64 	%rd9, [ThreshBackward_param_9];
	ld.param.u64 	%rd10, [ThreshBackward_param_10];
	setp.lt.s32	%p1, %r7, 1;
	@%p1 bra 	BB14_8;

	mov.u32 	%r17, 0;
	cvta.to.global.u64 	%rd11, %rd2;
	cvta.to.global.u64 	%rd13, %rd7;
	cvta.to.global.u64 	%rd27, %rd9;
	cvta.to.global.u64 	%rd36, %rd10;
	cvta.to.global.u64 	%rd41, %rd8;
	cvta.to.global.u64 	%rd16, %rd5;
	cvta.to.global.u64 	%rd25, %rd6;

BB14_2:
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r18, %r10, %r9, %r11;
	setp.ge.s32	%p2, %r18, %r6;
	@%p2 bra 	BB14_7;

BB14_3:
	mad.lo.s32 	%r12, %r17, %r6, %r18;
	mul.wide.s32 	%rd12, %r12, 4;
	add.s64 	%rd1, %rd11, %rd12;
	mul.wide.s32 	%rd14, %r18, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f1, [%rd15];
	ld.global.f32 	%f2, [%rd1];
	setp.gt.f32	%p3, %f2, %f1;
	@%p3 bra 	BB14_5;
	bra.uni 	BB14_4;

BB14_5:
	add.s64 	%rd29, %rd27, %rd14;
	cvta.to.global.u64 	%rd31, %rd4;
	add.s64 	%rd32, %rd31, %rd12;
	ld.global.f32 	%f10, [%rd32];
	ld.global.f32 	%f11, [%rd29];
	mul.f32 	%f12, %f11, %f10;
	cvta.to.global.u64 	%rd33, %rd3;
	add.s64 	%rd34, %rd33, %rd12;
	st.global.f32 	[%rd34], %f12;
	add.s64 	%rd35, %rd31, %rd14;
	ld.global.f32 	%f13, [%rd1];
	ld.global.f32 	%f14, [%rd35];
	add.s64 	%rd37, %rd36, %rd14;
	ld.global.f32 	%f15, [%rd37];
	fma.rn.f32 	%f16, %f14, %f13, %f15;
	st.global.f32 	[%rd37], %f16;
	bra.uni 	BB14_6;

BB14_4:
	add.s64 	%rd18, %rd16, %rd14;
	cvta.to.global.u64 	%rd20, %rd4;
	add.s64 	%rd21, %rd20, %rd12;
	ld.global.f32 	%f3, [%rd21];
	ld.global.f32 	%f4, [%rd18];
	mul.f32 	%f5, %f4, %f3;
	cvta.to.global.u64 	%rd22, %rd3;
	add.s64 	%rd23, %rd22, %rd12;
	st.global.f32 	[%rd23], %f5;
	add.s64 	%rd24, %rd20, %rd14;
	ld.global.f32 	%f6, [%rd1];
	ld.global.f32 	%f7, [%rd24];
	add.s64 	%rd26, %rd25, %rd14;
	ld.global.f32 	%f8, [%rd26];
	fma.rn.f32 	%f9, %f7, %f6, %f8;
	st.global.f32 	[%rd26], %f9;

BB14_6:
	cvta.to.global.u64 	%rd38, %rd4;
	add.s64 	%rd40, %rd38, %rd14;
	add.s64 	%rd42, %rd41, %rd14;
	ld.global.f32 	%f17, [%rd42];
	ld.global.f32 	%f18, [%rd40];
	add.f32 	%f19, %f18, %f17;
	st.global.f32 	[%rd42], %f19;
	mov.u32 	%r16, %nctaid.x;
	mad.lo.s32 	%r18, %r16, %r10, %r18;
	setp.lt.s32	%p4, %r18, %r6;
	@%p4 bra 	BB14_3;

BB14_7:
	add.s32 	%r17, %r17, 1;
	setp.lt.s32	%p5, %r17, %r7;
	@%p5 bra 	BB14_2;

BB14_8:
	ret;
}

	// .globl	PreluForward
.visible .entry PreluForward(
	.param .u32 PreluForward_param_0,
	.param .u32 PreluForward_param_1,
	.param .u64 PreluForward_param_2,
	.param .u64 PreluForward_param_3,
	.param .u64 PreluForward_param_4
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r8, [PreluForward_param_0];
	ld.param.u32 	%r9, [PreluForward_param_1];
	ld.param.u64 	%rd4, [PreluForward_param_2];
	ld.param.u64 	%rd5, [PreluForward_param_3];
	ld.param.u64 	%rd6, [PreluForward_param_4];
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB15_7;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r12, %r11, %r13;
	mov.u32 	%r14, %nctaid.x;
	mul.lo.s32 	%r2, %r14, %r12;
	mov.u32 	%r16, 0;
	cvta.to.global.u64 	%rd9, %rd5;

BB15_2:
	mul.lo.s32 	%r4, %r16, %r8;
	setp.ge.s32	%p2, %r1, %r8;
	mov.u32 	%r17, %r1;
	@%p2 bra 	BB15_6;

BB15_3:
	add.s32 	%r15, %r17, %r4;
	mul.wide.s32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f5, [%rd8];
	setp.gt.f32	%p3, %f5, 0f00000000;
	add.s64 	%rd3, %rd9, %rd7;
	@%p3 bra 	BB15_5;

	mul.wide.s32 	%rd10, %r17, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f4, [%rd11];
	mul.f32 	%f5, %f4, %f5;

BB15_5:
	st.global.f32 	[%rd3], %f5;
	add.s32 	%r17, %r2, %r17;
	setp.lt.s32	%p4, %r17, %r8;
	@%p4 bra 	BB15_3;

BB15_6:
	add.s32 	%r16, %r16, 1;
	setp.lt.s32	%p5, %r16, %r9;
	@%p5 bra 	BB15_2;

BB15_7:
	ret;
}

	// .globl	PreluBackward
.visible .entry PreluBackward(
	.param .u32 PreluBackward_param_0,
	.param .u32 PreluBackward_param_1,
	.param .u64 PreluBackward_param_2,
	.param .u64 PreluBackward_param_3,
	.param .u64 PreluBackward_param_4,
	.param .u64 PreluBackward_param_5,
	.param .u64 PreluBackward_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<24>;


	ld.param.u32 	%r8, [PreluBackward_param_0];
	ld.param.u32 	%r9, [PreluBackward_param_1];
	ld.param.u64 	%rd3, [PreluBackward_param_2];
	ld.param.u64 	%rd4, [PreluBackward_param_3];
	ld.param.u64 	%rd5, [PreluBackward_param_4];
	ld.param.u64 	%rd6, [PreluBackward_param_5];
	ld.param.u64 	%rd7, [PreluBackward_param_6];
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB16_8;

	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r19, 0;
	cvta.to.global.u64 	%rd8, %rd4;
	cvta.to.global.u64 	%rd11, %rd6;
	cvta.to.global.u64 	%rd19, %rd7;

BB16_2:
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r20, %r11, %r13, %r15;
	mul.lo.s32 	%r4, %r19, %r8;
	setp.ge.s32	%p2, %r20, %r8;
	@%p2 bra 	BB16_7;

BB16_3:
	add.s32 	%r16, %r20, %r4;
	mul.wide.s32 	%rd9, %r16, 4;
	add.s64 	%rd1, %rd8, %rd9;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.f32	%p3, %f1, 0f00000000;
	cvta.to.global.u64 	%rd10, %rd5;
	add.s64 	%rd2, %rd10, %rd9;
	@%p3 bra 	BB16_5;
	bra.uni 	BB16_4;

BB16_5:
	ld.global.f32 	%f9, [%rd2];
	mad.lo.s32 	%r18, %r19, %r8, %r20;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r18, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.f32 	[%rd23], %f9;
	bra.uni 	BB16_6;

BB16_4:
	mul.wide.s32 	%rd12, %r20, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f2, [%rd2];
	ld.global.f32 	%f3, [%rd13];
	mul.f32 	%f4, %f3, %f2;
	mad.lo.s32 	%r17, %r19, %r8, %r20;
	cvta.to.global.u64 	%rd14, %rd3;
	mul.wide.s32 	%rd15, %r17, 4;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.f32 	[%rd16], %f4;
	add.s64 	%rd18, %rd10, %rd12;
	ld.global.f32 	%f5, [%rd1];
	ld.global.f32 	%f6, [%rd18];
	add.s64 	%rd20, %rd19, %rd12;
	ld.global.f32 	%f7, [%rd20];
	fma.rn.f32 	%f8, %f6, %f5, %f7;
	st.global.f32 	[%rd20], %f8;

BB16_6:
	add.s32 	%r20, %r1, %r20;
	setp.lt.s32	%p4, %r20, %r8;
	@%p4 bra 	BB16_3;

BB16_7:
	add.s32 	%r19, %r19, 1;
	setp.lt.s32	%p5, %r19, %r9;
	@%p5 bra 	BB16_2;

BB16_8:
	ret;
}

	// .globl	LeakyForwardAlphaBeta
.visible .entry LeakyForwardAlphaBeta(
	.param .u32 LeakyForwardAlphaBeta_param_0,
	.param .u64 LeakyForwardAlphaBeta_param_1,
	.param .u64 LeakyForwardAlphaBeta_param_2,
	.param .f32 LeakyForwardAlphaBeta_param_3,
	.param .f32 LeakyForwardAlphaBeta_param_4,
	.param .f32 LeakyForwardAlphaBeta_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r6, [LeakyForwardAlphaBeta_param_0];
	ld.param.u64 	%rd3, [LeakyForwardAlphaBeta_param_1];
	ld.param.u64 	%rd4, [LeakyForwardAlphaBeta_param_2];
	ld.param.f32 	%f1, [LeakyForwardAlphaBeta_param_3];
	ld.param.f32 	%f2, [LeakyForwardAlphaBeta_param_4];
	ld.param.f32 	%f3, [LeakyForwardAlphaBeta_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB17_3;

	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB17_2:
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd2, %rd5;
	add.s64 	%rd7, %rd1, %rd5;
	ld.global.f32 	%f4, [%rd7];
	setp.gt.f32	%p2, %f4, 0f00000000;
	ld.global.f32 	%f5, [%rd6];
	mul.f32 	%f6, %f4, %f1;
	selp.f32	%f7, %f4, %f6, %p2;
	mul.f32 	%f8, %f7, %f2;
	fma.rn.f32 	%f9, %f5, %f3, %f8;
	st.global.f32 	[%rd6], %f9;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB17_2;

BB17_3:
	ret;
}

	// .globl	LeakyBackwardAlphaBeta
.visible .entry LeakyBackwardAlphaBeta(
	.param .u32 LeakyBackwardAlphaBeta_param_0,
	.param .u64 LeakyBackwardAlphaBeta_param_1,
	.param .u64 LeakyBackwardAlphaBeta_param_2,
	.param .u64 LeakyBackwardAlphaBeta_param_3,
	.param .f32 LeakyBackwardAlphaBeta_param_4,
	.param .f32 LeakyBackwardAlphaBeta_param_5,
	.param .f32 LeakyBackwardAlphaBeta_param_6
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [LeakyBackwardAlphaBeta_param_0];
	ld.param.u64 	%rd4, [LeakyBackwardAlphaBeta_param_1];
	ld.param.u64 	%rd5, [LeakyBackwardAlphaBeta_param_2];
	ld.param.u64 	%rd6, [LeakyBackwardAlphaBeta_param_3];
	ld.param.f32 	%f1, [LeakyBackwardAlphaBeta_param_4];
	ld.param.f32 	%f2, [LeakyBackwardAlphaBeta_param_5];
	ld.param.f32 	%f3, [LeakyBackwardAlphaBeta_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB18_3;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB18_2:
	mul.wide.s32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.f32 	%f4, [%rd9];
	setp.gt.f32	%p2, %f4, 0f00000000;
	add.s64 	%rd10, %rd1, %rd7;
	ld.global.f32 	%f5, [%rd8];
	ld.global.f32 	%f6, [%rd10];
	mul.f32 	%f7, %f6, %f1;
	selp.f32	%f8, %f6, %f7, %p2;
	mul.f32 	%f9, %f8, %f2;
	fma.rn.f32 	%f10, %f5, %f3, %f9;
	st.global.f32 	[%rd8], %f10;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB18_2;

BB18_3:
	ret;
}

	// .globl	LeakyForwardAlpha
.visible .entry LeakyForwardAlpha(
	.param .u32 LeakyForwardAlpha_param_0,
	.param .u64 LeakyForwardAlpha_param_1,
	.param .u64 LeakyForwardAlpha_param_2,
	.param .f32 LeakyForwardAlpha_param_3,
	.param .f32 LeakyForwardAlpha_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r6, [LeakyForwardAlpha_param_0];
	ld.param.u64 	%rd3, [LeakyForwardAlpha_param_1];
	ld.param.u64 	%rd4, [LeakyForwardAlpha_param_2];
	ld.param.f32 	%f1, [LeakyForwardAlpha_param_3];
	ld.param.f32 	%f2, [LeakyForwardAlpha_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB19_3;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB19_2:
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.f32 	%f3, [%rd6];
	setp.gt.f32	%p2, %f3, 0f00000000;
	add.s64 	%rd7, %rd1, %rd5;
	mul.f32 	%f4, %f3, %f1;
	selp.f32	%f5, %f3, %f4, %p2;
	mul.f32 	%f6, %f5, %f2;
	st.global.f32 	[%rd7], %f6;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB19_2;

BB19_3:
	ret;
}

	// .globl	LeakyBackwardAlpha
.visible .entry LeakyBackwardAlpha(
	.param .u32 LeakyBackwardAlpha_param_0,
	.param .u64 LeakyBackwardAlpha_param_1,
	.param .u64 LeakyBackwardAlpha_param_2,
	.param .u64 LeakyBackwardAlpha_param_3,
	.param .f32 LeakyBackwardAlpha_param_4,
	.param .f32 LeakyBackwardAlpha_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [LeakyBackwardAlpha_param_0];
	ld.param.u64 	%rd4, [LeakyBackwardAlpha_param_1];
	ld.param.u64 	%rd5, [LeakyBackwardAlpha_param_2];
	ld.param.u64 	%rd6, [LeakyBackwardAlpha_param_3];
	ld.param.f32 	%f1, [LeakyBackwardAlpha_param_4];
	ld.param.f32 	%f2, [LeakyBackwardAlpha_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB20_3;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB20_2:
	mul.wide.s32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f3, [%rd8];
	setp.gt.f32	%p2, %f3, 0f00000000;
	add.s64 	%rd9, %rd2, %rd7;
	add.s64 	%rd10, %rd1, %rd7;
	ld.global.f32 	%f4, [%rd9];
	mul.f32 	%f5, %f4, %f1;
	selp.f32	%f6, %f4, %f5, %p2;
	mul.f32 	%f7, %f6, %f2;
	st.global.f32 	[%rd10], %f7;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB20_2;

BB20_3:
	ret;
}

	// .globl	LeakyForward
.visible .entry LeakyForward(
	.param .u32 LeakyForward_param_0,
	.param .u64 LeakyForward_param_1,
	.param .u64 LeakyForward_param_2,
	.param .f32 LeakyForward_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r6, [LeakyForward_param_0];
	ld.param.u64 	%rd3, [LeakyForward_param_1];
	ld.param.u64 	%rd4, [LeakyForward_param_2];
	ld.param.f32 	%f1, [LeakyForward_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB21_3;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB21_2:
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.f32 	%f2, [%rd6];
	setp.gt.f32	%p2, %f2, 0f00000000;
	add.s64 	%rd7, %rd1, %rd5;
	mul.f32 	%f3, %f2, %f1;
	selp.f32	%f4, %f2, %f3, %p2;
	st.global.f32 	[%rd7], %f4;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB21_2;

BB21_3:
	ret;
}

	// .globl	LeakyBackward
.visible .entry LeakyBackward(
	.param .u32 LeakyBackward_param_0,
	.param .u64 LeakyBackward_param_1,
	.param .u64 LeakyBackward_param_2,
	.param .u64 LeakyBackward_param_3,
	.param .f32 LeakyBackward_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [LeakyBackward_param_0];
	ld.param.u64 	%rd4, [LeakyBackward_param_1];
	ld.param.u64 	%rd5, [LeakyBackward_param_2];
	ld.param.u64 	%rd6, [LeakyBackward_param_3];
	ld.param.f32 	%f1, [LeakyBackward_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB22_3;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB22_2:
	mul.wide.s32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f2, [%rd8];
	setp.gt.f32	%p2, %f2, 0f00000000;
	add.s64 	%rd9, %rd2, %rd7;
	add.s64 	%rd10, %rd1, %rd7;
	ld.global.f32 	%f3, [%rd9];
	mul.f32 	%f4, %f3, %f1;
	selp.f32	%f5, %f3, %f4, %p2;
	st.global.f32 	[%rd10], %f5;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB22_2;

BB22_3:
	ret;
}

	// .globl	MSELoss
.visible .entry MSELoss(
	.param .u32 MSELoss_param_0,
	.param .u64 MSELoss_param_1,
	.param .u64 MSELoss_param_2,
	.param .u64 MSELoss_param_3,
	.param .u64 MSELoss_param_4,
	.param .f32 MSELoss_param_5,
	.param .f32 MSELoss_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r6, [MSELoss_param_0];
	ld.param.u64 	%rd5, [MSELoss_param_1];
	ld.param.u64 	%rd6, [MSELoss_param_2];
	ld.param.u64 	%rd7, [MSELoss_param_3];
	ld.param.u64 	%rd8, [MSELoss_param_4];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r7, 0;
	st.global.u32 	[%rd1], %r7;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r11, %r1, %r8, %r9;
	setp.ge.s32	%p1, %r11, %r6;
	@%p1 bra 	BB23_3;

	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
	cvta.to.global.u64 	%rd4, %rd7;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r10, %r1;

BB23_2:
	mul.wide.s32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd4, %rd9;
	add.s64 	%rd11, %rd3, %rd9;
	ld.global.f32 	%f1, [%rd11];
	ld.global.f32 	%f2, [%rd10];
	sub.f32 	%f3, %f2, %f1;
	add.s64 	%rd12, %rd2, %rd9;
	st.global.f32 	[%rd12], %f3;
	mul.f32 	%f4, %f3, %f3;
	mul.f32 	%f5, %f4, 0f3F000000;
	atom.global.add.f32 	%f6, [%rd1], %f5;
	add.s32 	%r11, %r3, %r11;
	setp.lt.s32	%p2, %r11, %r6;
	@%p2 bra 	BB23_2;

BB23_3:
	ret;
}

	// .globl	MSELossbyBatches
.visible .entry MSELossbyBatches(
	.param .u32 MSELossbyBatches_param_0,
	.param .u32 MSELossbyBatches_param_1,
	.param .u64 MSELossbyBatches_param_2,
	.param .u64 MSELossbyBatches_param_3,
	.param .u64 MSELossbyBatches_param_4,
	.param .u64 MSELossbyBatches_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r11, [MSELossbyBatches_param_0];
	ld.param.u32 	%r12, [MSELossbyBatches_param_1];
	ld.param.u64 	%rd5, [MSELossbyBatches_param_2];
	ld.param.u64 	%rd6, [MSELossbyBatches_param_3];
	ld.param.u64 	%rd7, [MSELossbyBatches_param_4];
	ld.param.u64 	%rd8, [MSELossbyBatches_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r21, %r1, %r13, %r14;
	setp.ge.s32	%p1, %r21, %r11;
	@%p1 bra 	BB24_6;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd7;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r16, %r15, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r4, %r18, %r1;
	mov.u32 	%r19, %nctaid.y;
	mul.lo.s32 	%r5, %r19, %r16;
	cvta.to.global.u64 	%rd9, %rd8;

BB24_2:
	setp.ge.s32	%p2, %r3, %r12;
	@%p2 bra 	BB24_5;

	mul.wide.s32 	%rd10, %r21, 4;
	add.s64 	%rd4, %rd9, %rd10;
	mul.lo.s32 	%r7, %r21, %r12;
	mov.u32 	%r22, %r3;

BB24_4:
	add.s32 	%r20, %r22, %r7;
	mul.wide.s32 	%rd11, %r20, 4;
	add.s64 	%rd12, %rd3, %rd11;
	add.s64 	%rd13, %rd2, %rd11;
	ld.global.f32 	%f1, [%rd13];
	ld.global.f32 	%f2, [%rd12];
	sub.f32 	%f3, %f2, %f1;
	add.s64 	%rd14, %rd1, %rd11;
	st.global.f32 	[%rd14], %f3;
	mul.f32 	%f4, %f3, %f3;
	mul.f32 	%f5, %f4, 0f3F000000;
	atom.global.add.f32 	%f6, [%rd4], %f5;
	add.s32 	%r22, %r5, %r22;
	setp.lt.s32	%p3, %r22, %r12;
	@%p3 bra 	BB24_4;

BB24_5:
	add.s32 	%r21, %r4, %r21;
	setp.lt.s32	%p4, %r21, %r11;
	@%p4 bra 	BB24_2;

BB24_6:
	ret;
}

	// .globl	ConcatForwardNCHW
.visible .entry ConcatForwardNCHW(
	.param .u32 ConcatForwardNCHW_param_0,
	.param .u32 ConcatForwardNCHW_param_1,
	.param .u32 ConcatForwardNCHW_param_2,
	.param .u32 ConcatForwardNCHW_param_3,
	.param .u64 ConcatForwardNCHW_param_4,
	.param .u32 ConcatForwardNCHW_param_5,
	.param .u32 ConcatForwardNCHW_param_6,
	.param .u64 ConcatForwardNCHW_param_7,
	.param .u64 ConcatForwardNCHW_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<73>;
	.reg .b64 	%rd<16>;


	ld.param.u32 	%r31, [ConcatForwardNCHW_param_0];
	ld.param.u32 	%r32, [ConcatForwardNCHW_param_1];
	ld.param.u32 	%r33, [ConcatForwardNCHW_param_2];
	ld.param.u32 	%r34, [ConcatForwardNCHW_param_3];
	ld.param.u64 	%rd3, [ConcatForwardNCHW_param_4];
	ld.param.u32 	%r35, [ConcatForwardNCHW_param_5];
	ld.param.u32 	%r36, [ConcatForwardNCHW_param_6];
	ld.param.u64 	%rd4, [ConcatForwardNCHW_param_7];
	ld.param.u64 	%rd5, [ConcatForwardNCHW_param_8];
	setp.lt.s32	%p1, %r32, 1;
	@%p1 bra 	BB25_15;

	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r38, %ctaid.x;
	mov.u32 	%r39, %ntid.x;
	mov.u32 	%r40, %tid.x;
	mad.lo.s32 	%r1, %r39, %r38, %r40;
	add.s32 	%r41, %r36, %r34;
	mad.lo.s32 	%r2, %r32, %r41, %r1;
	mov.u32 	%r42, %nctaid.x;
	mul.lo.s32 	%r3, %r42, %r39;
	add.s32 	%r4, %r2, %r34;
	mov.u32 	%r64, 0;

BB25_2:
	setp.lt.s32	%p2, %r33, 1;
	@%p2 bra 	BB25_8;

	mad.lo.s32 	%r6, %r34, %r64, %r1;
	mov.u32 	%r65, 0;

BB25_4:
	setp.ge.s32	%p3, %r1, %r31;
	@%p3 bra 	BB25_7;

	mul.lo.s32 	%r48, %r31, %r65;
	add.s32 	%r68, %r6, %r48;
	add.s32 	%r67, %r2, %r48;
	mad.lo.s32 	%r66, %r38, %r39, %r40;

BB25_6:
	mul.wide.s32 	%rd6, %r68, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f1, [%rd7];
	cvta.to.global.u64 	%rd8, %rd5;
	mul.wide.s32 	%rd9, %r67, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f1;
	add.s32 	%r68, %r68, %r3;
	add.s32 	%r67, %r67, %r3;
	add.s32 	%r66, %r66, %r3;
	setp.lt.s32	%p4, %r66, %r31;
	@%p4 bra 	BB25_6;

BB25_7:
	add.s32 	%r65, %r65, 1;
	setp.lt.s32	%p5, %r65, %r33;
	@%p5 bra 	BB25_4;

BB25_8:
	setp.lt.s32	%p6, %r35, 1;
	@%p6 bra 	BB25_14;

	mad.lo.s32 	%r18, %r36, %r64, %r1;
	mov.u32 	%r69, 0;

BB25_10:
	setp.ge.s32	%p7, %r1, %r31;
	@%p7 bra 	BB25_13;

	mul.lo.s32 	%r60, %r31, %r69;
	add.s32 	%r72, %r18, %r60;
	add.s32 	%r71, %r4, %r60;
	mad.lo.s32 	%r70, %r38, %r39, %r40;

BB25_12:
	mul.wide.s32 	%rd11, %r72, 4;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.f32 	%f2, [%rd12];
	cvta.to.global.u64 	%rd13, %rd5;
	mul.wide.s32 	%rd14, %r71, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f32 	[%rd15], %f2;
	add.s32 	%r72, %r72, %r3;
	add.s32 	%r71, %r71, %r3;
	add.s32 	%r70, %r70, %r3;
	setp.lt.s32	%p8, %r70, %r31;
	@%p8 bra 	BB25_12;

BB25_13:
	add.s32 	%r69, %r69, 1;
	setp.lt.s32	%p9, %r69, %r35;
	@%p9 bra 	BB25_10;

BB25_14:
	add.s32 	%r64, %r64, 1;
	setp.lt.s32	%p10, %r64, %r32;
	@%p10 bra 	BB25_2;

BB25_15:
	ret;
}

	// .globl	ConcatBackwardNCHW
.visible .entry ConcatBackwardNCHW(
	.param .u32 ConcatBackwardNCHW_param_0,
	.param .u32 ConcatBackwardNCHW_param_1,
	.param .u32 ConcatBackwardNCHW_param_2,
	.param .u32 ConcatBackwardNCHW_param_3,
	.param .u64 ConcatBackwardNCHW_param_4,
	.param .u32 ConcatBackwardNCHW_param_5,
	.param .u32 ConcatBackwardNCHW_param_6,
	.param .u64 ConcatBackwardNCHW_param_7,
	.param .u64 ConcatBackwardNCHW_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r32, [ConcatBackwardNCHW_param_0];
	ld.param.u32 	%r33, [ConcatBackwardNCHW_param_1];
	ld.param.u32 	%r34, [ConcatBackwardNCHW_param_2];
	ld.param.u32 	%r35, [ConcatBackwardNCHW_param_3];
	ld.param.u64 	%rd2, [ConcatBackwardNCHW_param_4];
	ld.param.u32 	%r36, [ConcatBackwardNCHW_param_5];
	ld.param.u32 	%r37, [ConcatBackwardNCHW_param_6];
	ld.param.u64 	%rd3, [ConcatBackwardNCHW_param_7];
	ld.param.u64 	%rd4, [ConcatBackwardNCHW_param_8];
	cvta.to.global.u64 	%rd1, %rd4;
	setp.lt.s32	%p1, %r33, 1;
	@%p1 bra 	BB26_15;

	mov.u32 	%r39, %ctaid.x;
	mov.u32 	%r40, %ntid.x;
	mov.u32 	%r41, %tid.x;
	mad.lo.s32 	%r1, %r40, %r39, %r41;
	mov.u32 	%r42, %nctaid.x;
	mul.lo.s32 	%r5, %r42, %r40;
	add.s32 	%r43, %r37, %r35;
	mad.lo.s32 	%r3, %r33, %r43, %r1;
	add.s32 	%r4, %r3, %r35;
	mov.u32 	%r62, 0;
	cvta.to.global.u64 	%rd7, %rd2;
	cvta.to.global.u64 	%rd12, %rd3;

BB26_2:
	setp.lt.s32	%p2, %r34, 1;
	@%p2 bra 	BB26_8;

	mad.lo.s32 	%r7, %r35, %r62, %r1;
	mov.u32 	%r63, 0;

BB26_4:
	setp.ge.s32	%p3, %r1, %r32;
	@%p3 bra 	BB26_7;

	mul.lo.s32 	%r49, %r32, %r63;
	add.s32 	%r66, %r3, %r49;
	add.s32 	%r65, %r7, %r49;
	mad.lo.s32 	%r64, %r39, %r40, %r41;

BB26_6:
	mul.wide.s32 	%rd5, %r66, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f1, [%rd6];
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f1;
	add.s32 	%r66, %r66, %r5;
	add.s32 	%r65, %r65, %r5;
	add.s32 	%r64, %r64, %r5;
	setp.lt.s32	%p4, %r64, %r32;
	@%p4 bra 	BB26_6;

BB26_7:
	add.s32 	%r63, %r63, 1;
	setp.lt.s32	%p5, %r63, %r34;
	@%p5 bra 	BB26_4;

BB26_8:
	setp.lt.s32	%p6, %r36, 1;
	@%p6 bra 	BB26_14;

	mad.lo.s32 	%r19, %r37, %r62, %r1;
	mov.u32 	%r67, 0;

BB26_10:
	setp.ge.s32	%p7, %r1, %r32;
	@%p7 bra 	BB26_13;

	mul.lo.s32 	%r58, %r32, %r67;
	add.s32 	%r70, %r4, %r58;
	add.s32 	%r69, %r19, %r58;
	mad.lo.s32 	%r68, %r39, %r40, %r41;

BB26_12:
	mul.wide.s32 	%rd10, %r70, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.f32 	%f2, [%rd11];
	mul.wide.s32 	%rd13, %r69, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f2;
	add.s32 	%r70, %r70, %r5;
	add.s32 	%r69, %r69, %r5;
	add.s32 	%r68, %r68, %r5;
	setp.lt.s32	%p8, %r68, %r32;
	@%p8 bra 	BB26_12;

BB26_13:
	add.s32 	%r67, %r67, 1;
	setp.lt.s32	%p9, %r67, %r36;
	@%p9 bra 	BB26_10;

BB26_14:
	add.s32 	%r62, %r62, 1;
	setp.lt.s32	%p10, %r62, %r33;
	@%p10 bra 	BB26_2;

BB26_15:
	ret;
}

	// .globl	ConcatForwardNCHWhalf
.visible .entry ConcatForwardNCHWhalf(
	.param .u32 ConcatForwardNCHWhalf_param_0,
	.param .u32 ConcatForwardNCHWhalf_param_1,
	.param .u32 ConcatForwardNCHWhalf_param_2,
	.param .u32 ConcatForwardNCHWhalf_param_3,
	.param .u64 ConcatForwardNCHWhalf_param_4,
	.param .u32 ConcatForwardNCHWhalf_param_5,
	.param .u32 ConcatForwardNCHWhalf_param_6,
	.param .u64 ConcatForwardNCHWhalf_param_7,
	.param .u64 ConcatForwardNCHWhalf_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<74>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r30, [ConcatForwardNCHWhalf_param_0];
	ld.param.u32 	%r31, [ConcatForwardNCHWhalf_param_1];
	ld.param.u32 	%r32, [ConcatForwardNCHWhalf_param_2];
	ld.param.u32 	%r33, [ConcatForwardNCHWhalf_param_3];
	ld.param.u64 	%rd4, [ConcatForwardNCHWhalf_param_4];
	ld.param.u32 	%r34, [ConcatForwardNCHWhalf_param_5];
	ld.param.u32 	%r35, [ConcatForwardNCHWhalf_param_6];
	ld.param.u64 	%rd5, [ConcatForwardNCHWhalf_param_7];
	ld.param.u64 	%rd6, [ConcatForwardNCHWhalf_param_8];
	cvta.to.global.u64 	%rd1, %rd6;
	setp.lt.s32	%p1, %r31, 1;
	@%p1 bra 	BB27_15;

	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r38, %r37, %r39;
	add.s32 	%r40, %r35, %r33;
	mad.lo.s32 	%r2, %r31, %r40, %r1;
	add.s32 	%r3, %r2, %r33;
	mov.u32 	%r65, 0;

BB27_2:
	setp.lt.s32	%p2, %r32, 1;
	@%p2 bra 	BB27_8;

	mad.lo.s32 	%r5, %r33, %r65, %r1;
	mov.u32 	%r66, 0;

BB27_4:
	setp.ge.s32	%p3, %r1, %r30;
	@%p3 bra 	BB27_7;

	mul.lo.s32 	%r46, %r30, %r66;
	add.s32 	%r69, %r2, %r46;
	add.s32 	%r68, %r5, %r46;
	mad.lo.s32 	%r67, %r37, %r38, %r39;

BB27_6:
	mul.wide.s32 	%rd7, %r69, 2;
	add.s64 	%rd8, %rd1, %rd7;
	mul.wide.s32 	%rd9, %r68, 2;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.u16 	%rs1, [%rd10];
	st.global.u16 	[%rd8], %rs1;
	mov.u32 	%r51, %nctaid.x;
	mul.lo.s32 	%r52, %r51, %r38;
	add.s32 	%r69, %r69, %r52;
	add.s32 	%r68, %r68, %r52;
	add.s32 	%r67, %r67, %r52;
	setp.lt.s32	%p4, %r67, %r30;
	@%p4 bra 	BB27_6;

BB27_7:
	add.s32 	%r66, %r66, 1;
	setp.lt.s32	%p5, %r66, %r32;
	@%p5 bra 	BB27_4;

BB27_8:
	setp.lt.s32	%p6, %r34, 1;
	@%p6 bra 	BB27_14;

	mad.lo.s32 	%r17, %r35, %r65, %r1;
	mov.u32 	%r70, 0;

BB27_10:
	setp.ge.s32	%p7, %r1, %r30;
	@%p7 bra 	BB27_13;

	mul.lo.s32 	%r58, %r30, %r70;
	add.s32 	%r73, %r3, %r58;
	add.s32 	%r72, %r17, %r58;
	mad.lo.s32 	%r71, %r37, %r38, %r39;

BB27_12:
	mul.wide.s32 	%rd11, %r73, 2;
	add.s64 	%rd12, %rd1, %rd11;
	mul.wide.s32 	%rd13, %r72, 2;
	add.s64 	%rd14, %rd3, %rd13;
	ld.global.u16 	%rs2, [%rd14];
	st.global.u16 	[%rd12], %rs2;
	mov.u32 	%r63, %nctaid.x;
	mul.lo.s32 	%r64, %r63, %r38;
	add.s32 	%r73, %r73, %r64;
	add.s32 	%r72, %r72, %r64;
	add.s32 	%r71, %r71, %r64;
	setp.lt.s32	%p8, %r71, %r30;
	@%p8 bra 	BB27_12;

BB27_13:
	add.s32 	%r70, %r70, 1;
	setp.lt.s32	%p9, %r70, %r34;
	@%p9 bra 	BB27_10;

BB27_14:
	add.s32 	%r65, %r65, 1;
	setp.lt.s32	%p10, %r65, %r31;
	@%p10 bra 	BB27_2;

BB27_15:
	ret;
}

	// .globl	ConcatBackwardNCHWhalf
.visible .entry ConcatBackwardNCHWhalf(
	.param .u32 ConcatBackwardNCHWhalf_param_0,
	.param .u32 ConcatBackwardNCHWhalf_param_1,
	.param .u32 ConcatBackwardNCHWhalf_param_2,
	.param .u32 ConcatBackwardNCHWhalf_param_3,
	.param .u64 ConcatBackwardNCHWhalf_param_4,
	.param .u32 ConcatBackwardNCHWhalf_param_5,
	.param .u32 ConcatBackwardNCHWhalf_param_6,
	.param .u64 ConcatBackwardNCHWhalf_param_7,
	.param .u64 ConcatBackwardNCHWhalf_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<74>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r30, [ConcatBackwardNCHWhalf_param_0];
	ld.param.u32 	%r31, [ConcatBackwardNCHWhalf_param_1];
	ld.param.u32 	%r32, [ConcatBackwardNCHWhalf_param_2];
	ld.param.u32 	%r33, [ConcatBackwardNCHWhalf_param_3];
	ld.param.u64 	%rd4, [ConcatBackwardNCHWhalf_param_4];
	ld.param.u32 	%r34, [ConcatBackwardNCHWhalf_param_5];
	ld.param.u32 	%r35, [ConcatBackwardNCHWhalf_param_6];
	ld.param.u64 	%rd5, [ConcatBackwardNCHWhalf_param_7];
	ld.param.u64 	%rd6, [ConcatBackwardNCHWhalf_param_8];
	cvta.to.global.u64 	%rd1, %rd6;
	setp.lt.s32	%p1, %r31, 1;
	@%p1 bra 	BB28_15;

	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r38, %r37, %r39;
	add.s32 	%r40, %r35, %r33;
	mad.lo.s32 	%r2, %r31, %r40, %r1;
	add.s32 	%r3, %r2, %r33;
	mov.u32 	%r65, 0;

BB28_2:
	setp.lt.s32	%p2, %r32, 1;
	@%p2 bra 	BB28_8;

	mad.lo.s32 	%r5, %r33, %r65, %r1;
	mov.u32 	%r66, 0;

BB28_4:
	setp.ge.s32	%p3, %r1, %r30;
	@%p3 bra 	BB28_7;

	mul.lo.s32 	%r46, %r30, %r66;
	add.s32 	%r69, %r5, %r46;
	add.s32 	%r68, %r2, %r46;
	mad.lo.s32 	%r67, %r37, %r38, %r39;

BB28_6:
	mul.wide.s32 	%rd7, %r69, 2;
	add.s64 	%rd8, %rd2, %rd7;
	mul.wide.s32 	%rd9, %r68, 2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u16 	%rs1, [%rd10];
	st.global.u16 	[%rd8], %rs1;
	mov.u32 	%r51, %nctaid.x;
	mul.lo.s32 	%r52, %r51, %r38;
	add.s32 	%r69, %r69, %r52;
	add.s32 	%r68, %r68, %r52;
	add.s32 	%r67, %r67, %r52;
	setp.lt.s32	%p4, %r67, %r30;
	@%p4 bra 	BB28_6;

BB28_7:
	add.s32 	%r66, %r66, 1;
	setp.lt.s32	%p5, %r66, %r32;
	@%p5 bra 	BB28_4;

BB28_8:
	setp.lt.s32	%p6, %r34, 1;
	@%p6 bra 	BB28_14;

	mad.lo.s32 	%r17, %r35, %r65, %r1;
	mov.u32 	%r70, 0;

BB28_10:
	setp.ge.s32	%p7, %r1, %r30;
	@%p7 bra 	BB28_13;

	mul.lo.s32 	%r58, %r30, %r70;
	add.s32 	%r73, %r17, %r58;
	add.s32 	%r72, %r3, %r58;
	mad.lo.s32 	%r71, %r37, %r38, %r39;

BB28_12:
	mul.wide.s32 	%rd11, %r73, 2;
	add.s64 	%rd12, %rd3, %rd11;
	mul.wide.s32 	%rd13, %r72, 2;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.u16 	%rs2, [%rd14];
	st.global.u16 	[%rd12], %rs2;
	mov.u32 	%r63, %nctaid.x;
	mul.lo.s32 	%r64, %r63, %r38;
	add.s32 	%r73, %r73, %r64;
	add.s32 	%r72, %r72, %r64;
	add.s32 	%r71, %r71, %r64;
	setp.lt.s32	%p8, %r71, %r30;
	@%p8 bra 	BB28_12;

BB28_13:
	add.s32 	%r70, %r70, 1;
	setp.lt.s32	%p9, %r70, %r34;
	@%p9 bra 	BB28_10;

BB28_14:
	add.s32 	%r65, %r65, 1;
	setp.lt.s32	%p10, %r65, %r31;
	@%p10 bra 	BB28_2;

BB28_15:
	ret;
}

	// .globl	MakePlanarImageBatchesUint8
.visible .entry MakePlanarImageBatchesUint8(
	.param .u32 MakePlanarImageBatchesUint8_param_0,
	.param .u32 MakePlanarImageBatchesUint8_param_1,
	.param .u32 MakePlanarImageBatchesUint8_param_2,
	.param .u64 MakePlanarImageBatchesUint8_param_3,
	.param .u64 MakePlanarImageBatchesUint8_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<34>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r19, [MakePlanarImageBatchesUint8_param_0];
	ld.param.u32 	%r20, [MakePlanarImageBatchesUint8_param_1];
	ld.param.u32 	%r21, [MakePlanarImageBatchesUint8_param_2];
	ld.param.u64 	%rd3, [MakePlanarImageBatchesUint8_param_3];
	ld.param.u64 	%rd4, [MakePlanarImageBatchesUint8_param_4];
	setp.lt.s32	%p1, %r20, 1;
	@%p1 bra 	BB29_9;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r1, %r24, %r23, %r25;
	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r2, %r26, %r24;
	mad.lo.s32 	%r5, %r23, %r24, %r25;
	mov.u32 	%r29, 0;

BB29_2:
	setp.lt.s32	%p2, %r21, 1;
	@%p2 bra 	BB29_8;

	mul.lo.s32 	%r7, %r21, %r29;
	mov.u32 	%r30, 0;

BB29_4:
	setp.ge.s32	%p3, %r1, %r19;
	@%p3 bra 	BB29_7;

	add.s32 	%r28, %r7, %r30;
	mad.lo.s32 	%r33, %r19, %r28, %r1;
	mad.lo.s32 	%r32, %r19, %r30, %r1;
	mov.u32 	%r31, %r5;

BB29_6:
	mul.wide.s32 	%rd5, %r32, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.f32 	%f1, [%rd6];
	mul.wide.s32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd1, %rd7;
	st.global.f32 	[%rd8], %f1;
	add.s32 	%r33, %r33, %r2;
	add.s32 	%r32, %r32, %r2;
	add.s32 	%r31, %r31, %r2;
	setp.lt.s32	%p4, %r31, %r19;
	@%p4 bra 	BB29_6;

BB29_7:
	add.s32 	%r30, %r30, 1;
	setp.lt.s32	%p5, %r30, %r21;
	@%p5 bra 	BB29_4;

BB29_8:
	add.s32 	%r29, %r29, 1;
	setp.lt.s32	%p6, %r29, %r20;
	@%p6 bra 	BB29_2;

BB29_9:
	ret;
}

	// .globl	TransposeFP16
.visible .entry TransposeFP16(
	.param .u32 TransposeFP16_param_0,
	.param .u64 TransposeFP16_param_1,
	.param .u64 TransposeFP16_param_2,
	.param .u32 TransposeFP16_param_3,
	.param .u64 TransposeFP16_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<105>;
	.reg .b64 	%rd<45>;


	ld.param.u32 	%r32, [TransposeFP16_param_0];
	ld.param.u64 	%rd11, [TransposeFP16_param_1];
	ld.param.u64 	%rd13, [TransposeFP16_param_2];
	ld.param.u32 	%r33, [TransposeFP16_param_3];
	ld.param.u64 	%rd12, [TransposeFP16_param_4];
	cvta.to.global.u64 	%rd1, %rd13;
	shl.b32 	%r1, %r33, 1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r35, %tid.x;
	mad.lo.s32 	%r91, %r2, %r34, %r35;
	setp.ge.s32	%p1, %r91, %r32;
	@%p1 bra 	BB30_15;

	cvta.to.global.u64 	%rd2, %rd12;
	cvta.to.global.u64 	%rd3, %rd11;
	mov.u32 	%r36, %nctaid.x;
	mul.lo.s32 	%r4, %r36, %r2;
	and.b32  	%r5, %r33, 3;
	mul.wide.s32 	%rd7, %r33, 4;
	add.s64 	%rd4, %rd1, %rd7;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd5, %rd1, %rd14;

BB30_2:
	mov.u32 	%r104, 0;
	setp.lt.s32	%p2, %r33, 1;
	@%p2 bra 	BB30_14;

	mov.u32 	%r98, 0;
	setp.eq.s32	%p3, %r5, 0;
	@%p3 bra 	BB30_4;

	setp.eq.s32	%p4, %r5, 1;
	@%p4 bra 	BB30_6;
	bra.uni 	BB30_7;

BB30_6:
	mov.u32 	%r96, %r91;
	mov.u32 	%r97, %r98;
	bra.uni 	BB30_10;

BB30_4:
	mov.u32 	%r99, %r91;
	mov.u32 	%r104, %r98;
	bra.uni 	BB30_11;

BB30_7:
	setp.eq.s32	%p5, %r5, 2;
	mov.u32 	%r93, %r91;
	mov.u32 	%r94, %r98;
	@%p5 bra 	BB30_9;

	ld.global.u32 	%r46, [%rd4];
	div.s32 	%r47, %r91, %r46;
	mul.lo.s32 	%r48, %r47, %r46;
	sub.s32 	%r93, %r91, %r48;
	ld.global.u32 	%r49, [%rd5];
	mul.wide.s32 	%rd15, %r49, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r50, [%rd16];
	mul.lo.s32 	%r94, %r50, %r47;
	mov.u32 	%r98, 1;

BB30_9:
	add.s32 	%r51, %r98, %r33;
	mul.wide.s32 	%rd17, %r51, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u32 	%r52, [%rd18];
	div.s32 	%r53, %r93, %r52;
	mul.lo.s32 	%r54, %r53, %r52;
	sub.s32 	%r96, %r93, %r54;
	add.s32 	%r55, %r98, %r1;
	mul.wide.s32 	%rd19, %r55, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r56, [%rd20];
	mul.wide.s32 	%rd21, %r56, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u32 	%r57, [%rd22];
	mad.lo.s32 	%r97, %r57, %r53, %r94;
	add.s32 	%r98, %r98, 1;

BB30_10:
	add.s32 	%r58, %r98, %r33;
	mul.wide.s32 	%rd23, %r58, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r59, [%rd24];
	div.s32 	%r60, %r96, %r59;
	mul.lo.s32 	%r61, %r60, %r59;
	sub.s32 	%r99, %r96, %r61;
	add.s32 	%r62, %r98, %r1;
	mul.wide.s32 	%rd25, %r62, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u32 	%r63, [%rd26];
	mul.wide.s32 	%rd27, %r63, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u32 	%r64, [%rd28];
	mad.lo.s32 	%r104, %r64, %r60, %r97;
	add.s32 	%r98, %r98, 1;

BB30_11:
	setp.lt.u32	%p6, %r33, 4;
	@%p6 bra 	BB30_14;

	mul.wide.s32 	%rd29, %r98, 4;
	add.s64 	%rd44, %rd1, %rd29;

BB30_13:
	add.s64 	%rd30, %rd44, %rd7;
	ld.global.u32 	%r65, [%rd30];
	div.s32 	%r66, %r99, %r65;
	mul.lo.s32 	%r67, %r66, %r65;
	sub.s32 	%r68, %r99, %r67;
	add.s64 	%rd31, %rd44, %rd14;
	ld.global.u32 	%r69, [%rd31];
	mul.wide.s32 	%rd32, %r69, 4;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.u32 	%r70, [%rd33];
	mad.lo.s32 	%r71, %r70, %r66, %r104;
	ld.global.u32 	%r72, [%rd30+4];
	div.s32 	%r73, %r68, %r72;
	mul.lo.s32 	%r74, %r73, %r72;
	sub.s32 	%r75, %r68, %r74;
	ld.global.u32 	%r76, [%rd31+4];
	mul.wide.s32 	%rd34, %r76, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.u32 	%r77, [%rd35];
	mad.lo.s32 	%r78, %r77, %r73, %r71;
	ld.global.u32 	%r79, [%rd30+8];
	div.s32 	%r80, %r75, %r79;
	mul.lo.s32 	%r81, %r80, %r79;
	sub.s32 	%r82, %r75, %r81;
	ld.global.u32 	%r83, [%rd31+8];
	mul.wide.s32 	%rd36, %r83, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.u32 	%r84, [%rd37];
	mad.lo.s32 	%r85, %r84, %r80, %r78;
	ld.global.u32 	%r86, [%rd30+12];
	div.s32 	%r87, %r82, %r86;
	mul.lo.s32 	%r88, %r87, %r86;
	sub.s32 	%r99, %r82, %r88;
	ld.global.u32 	%r89, [%rd31+12];
	mul.wide.s32 	%rd38, %r89, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.u32 	%r90, [%rd39];
	mad.lo.s32 	%r104, %r90, %r87, %r85;
	add.s64 	%rd44, %rd44, 16;
	add.s32 	%r98, %r98, 4;
	setp.lt.s32	%p7, %r98, %r33;
	@%p7 bra 	BB30_13;

BB30_14:
	mul.wide.s32 	%rd40, %r91, 2;
	add.s64 	%rd41, %rd2, %rd40;
	mul.wide.s32 	%rd42, %r104, 2;
	add.s64 	%rd43, %rd3, %rd42;
	ld.global.u16 	%rs1, [%rd43];
	st.global.u16 	[%rd41], %rs1;
	add.s32 	%r91, %r4, %r91;
	setp.lt.s32	%p8, %r91, %r32;
	@%p8 bra 	BB30_2;

BB30_15:
	ret;
}

	// .globl	SwapEveryOtherFP16
.visible .entry SwapEveryOtherFP16(
	.param .u32 SwapEveryOtherFP16_param_0,
	.param .u32 SwapEveryOtherFP16_param_1,
	.param .u64 SwapEveryOtherFP16_param_2,
	.param .u64 SwapEveryOtherFP16_param_3,
	.param .u32 SwapEveryOtherFP16_param_4,
	.param .u32 SwapEveryOtherFP16_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r10, [SwapEveryOtherFP16_param_1];
	ld.param.u64 	%rd3, [SwapEveryOtherFP16_param_2];
	ld.param.u64 	%rd4, [SwapEveryOtherFP16_param_3];
	ld.param.u32 	%r27, [SwapEveryOtherFP16_param_4];
	ld.param.u32 	%r12, [SwapEveryOtherFP16_param_5];
	ld.param.u32 	%r9, [SwapEveryOtherFP16_param_0];
	shr.u32 	%r13, %r9, 31;
	add.s32 	%r14, %r9, %r13;
	shr.s32 	%r1, %r14, 1;
	setp.ge.s32	%p2, %r27, %r10;
	@%p2 bra 	BB31_8;

	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mul.lo.s32 	%r17, %r15, %r16;
	mov.u32 	%r18, %tid.x;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	add.s32 	%r2, %r17, %r18;
	neg.s32 	%r19, %r18;
	setp.eq.s32	%p3, %r17, %r19;
	and.b32  	%r20, %r9, 1;
	setp.eq.b32	%p4, %r20, 1;
	and.pred  	%p1, %p3, %p4;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r3, %r21, %r15;

BB31_2:
	setp.ge.s32	%p5, %r2, %r1;
	@%p5 bra 	BB31_5;

	mul.lo.s32 	%r5, %r27, %r1;
	mov.u32 	%r28, %r2;

BB31_4:
	add.s32 	%r22, %r28, %r5;
	mul.wide.s32 	%rd5, %r22, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.u32 	%r23, [%rd6];
	add.s64 	%rd7, %rd2, %rd5;
	ld.global.u32 	%r24, [%rd7];
	st.global.u32 	[%rd6], %r24;
	st.global.u32 	[%rd7], %r23;
	add.s32 	%r28, %r3, %r28;
	setp.lt.s32	%p6, %r28, %r1;
	@%p6 bra 	BB31_4;

BB31_5:
	@!%p1 bra 	BB31_7;
	bra.uni 	BB31_6;

BB31_6:
	mad.lo.s32 	%r25, %r27, %r9, %r9;
	add.s32 	%r26, %r25, -1;
	mul.wide.s32 	%rd9, %r26, 2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u16 	%rs1, [%rd10];
	add.s64 	%rd12, %rd2, %rd9;
	st.global.u16 	[%rd12], %rs1;

BB31_7:
	bar.sync 	0;
	add.s32 	%r27, %r27, %r12;
	setp.lt.s32	%p7, %r27, %r10;
	@%p7 bra 	BB31_2;

BB31_8:
	ret;
}

	// .globl	SwapUpperLowerFP16
.visible .entry SwapUpperLowerFP16(
	.param .u32 SwapUpperLowerFP16_param_0,
	.param .u32 SwapUpperLowerFP16_param_1,
	.param .u64 SwapUpperLowerFP16_param_2,
	.param .u64 SwapUpperLowerFP16_param_3,
	.param .u32 SwapUpperLowerFP16_param_4,
	.param .u32 SwapUpperLowerFP16_param_5,
	.param .u32 SwapUpperLowerFP16_param_6
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<15>;


	ld.param.u32 	%r26, [SwapUpperLowerFP16_param_1];
	ld.param.u64 	%rd3, [SwapUpperLowerFP16_param_2];
	ld.param.u64 	%rd4, [SwapUpperLowerFP16_param_3];
	ld.param.u32 	%r28, [SwapUpperLowerFP16_param_4];
	ld.param.u32 	%r27, [SwapUpperLowerFP16_param_5];
	ld.param.u32 	%r25, [SwapUpperLowerFP16_param_0];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %tid.x;
	mad.lo.s32 	%r48, %r1, %r29, %r30;
	shr.u32 	%r31, %r25, 31;
	add.s32 	%r32, %r25, %r31;
	shr.s32 	%r3, %r32, 1;
	setp.gt.s32	%p1, %r28, 0;
	@%p1 bra 	BB32_7;
	bra.uni 	BB32_1;

BB32_7:
	setp.ge.s32	%p9, %r48, %r3;
	@%p9 bra 	BB32_13;

	setp.gt.s32	%p10, %r27, 0;
	selp.b32	%r14, 0, %r3, %p10;
	mov.u32 	%r41, %ntid.y;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r43, %tid.y;
	mad.lo.s32 	%r15, %r41, %r42, %r43;
	mov.u32 	%r44, %nctaid.y;
	mul.lo.s32 	%r16, %r44, %r41;
	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r17, %r45, %r1;

BB32_9:
	add.s32 	%r19, %r14, %r48;
	setp.ge.s32	%p11, %r19, %r25;
	setp.ge.s32	%p12, %r48, %r25;
	or.pred  	%p13, %p11, %p12;
	setp.ge.s32	%p14, %r15, %r26;
	or.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB32_12;

	mul.lo.s32 	%r20, %r48, %r26;
	mul.lo.s32 	%r21, %r19, %r26;
	mov.u32 	%r51, %r15;

BB32_11:
	add.s32 	%r46, %r51, %r20;
	mul.wide.s32 	%rd10, %r46, 2;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.u16 	%rs3, [%rd11];
	add.s32 	%r47, %r51, %r21;
	mul.wide.s32 	%rd12, %r47, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u16 	%rs4, [%rd13];
	st.global.u16 	[%rd11], %rs4;
	add.s64 	%rd14, %rd1, %rd10;
	st.global.u16 	[%rd14], %rs3;
	add.s32 	%r51, %r16, %r51;
	setp.lt.s32	%p16, %r51, %r26;
	@%p16 bra 	BB32_11;

BB32_12:
	add.s32 	%r48, %r17, %r48;
	setp.lt.s32	%p17, %r48, %r3;
	@%p17 bra 	BB32_9;
	bra.uni 	BB32_13;

BB32_1:
	setp.ge.s32	%p2, %r48, %r3;
	@%p2 bra 	BB32_13;

	mov.u32 	%r33, %ctaid.y;
	mov.u32 	%r34, %ntid.y;
	mov.u32 	%r35, %tid.y;
	mad.lo.s32 	%r4, %r34, %r33, %r35;
	mov.u32 	%r36, %nctaid.y;
	mul.lo.s32 	%r5, %r36, %r34;
	mov.u32 	%r37, %nctaid.x;
	mul.lo.s32 	%r6, %r37, %r1;

BB32_3:
	add.s32 	%r8, %r3, %r48;
	setp.ge.s32	%p3, %r8, %r25;
	setp.ge.s32	%p4, %r4, %r26;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB32_6;

	mul.lo.s32 	%r9, %r8, %r26;
	setp.gt.s32	%p6, %r27, 0;
	selp.b32	%r38, %r48, %r8, %p6;
	mul.lo.s32 	%r10, %r38, %r26;
	mov.u32 	%r49, %r4;

BB32_5:
	add.s32 	%r39, %r49, %r9;
	mul.wide.s32 	%rd5, %r39, 2;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	add.s32 	%r40, %r49, %r10;
	mul.wide.s32 	%rd7, %r40, 2;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.u16 	%rs2, [%rd8];
	st.global.u16 	[%rd6], %rs2;
	add.s64 	%rd9, %rd1, %rd5;
	st.global.u16 	[%rd9], %rs1;
	add.s32 	%r49, %r5, %r49;
	setp.lt.s32	%p7, %r49, %r26;
	@%p7 bra 	BB32_5;

BB32_6:
	add.s32 	%r48, %r6, %r48;
	setp.lt.s32	%p8, %r48, %r3;
	@%p8 bra 	BB32_3;

BB32_13:
	ret;
}

	// .globl	ShapetoBatch4DNHWCFP16
.visible .entry ShapetoBatch4DNHWCFP16(
	.param .u32 ShapetoBatch4DNHWCFP16_param_0,
	.param .u32 ShapetoBatch4DNHWCFP16_param_1,
	.param .u32 ShapetoBatch4DNHWCFP16_param_2,
	.param .u32 ShapetoBatch4DNHWCFP16_param_3,
	.param .u32 ShapetoBatch4DNHWCFP16_param_4,
	.param .u32 ShapetoBatch4DNHWCFP16_param_5,
	.param .u32 ShapetoBatch4DNHWCFP16_param_6,
	.param .u32 ShapetoBatch4DNHWCFP16_param_7,
	.param .u32 ShapetoBatch4DNHWCFP16_param_8,
	.param .u32 ShapetoBatch4DNHWCFP16_param_9,
	.param .u32 ShapetoBatch4DNHWCFP16_param_10,
	.param .u32 ShapetoBatch4DNHWCFP16_param_11,
	.param .u64 ShapetoBatch4DNHWCFP16_param_12,
	.param .u64 ShapetoBatch4DNHWCFP16_param_13,
	.param .u32 ShapetoBatch4DNHWCFP16_param_14,
	.param .u32 ShapetoBatch4DNHWCFP16_param_15,
	.param .u8 ShapetoBatch4DNHWCFP16_param_16
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r38, [ShapetoBatch4DNHWCFP16_param_0];
	ld.param.u32 	%r39, [ShapetoBatch4DNHWCFP16_param_1];
	ld.param.u32 	%r40, [ShapetoBatch4DNHWCFP16_param_2];
	ld.param.u32 	%r41, [ShapetoBatch4DNHWCFP16_param_3];
	ld.param.u32 	%r42, [ShapetoBatch4DNHWCFP16_param_4];
	ld.param.u32 	%r43, [ShapetoBatch4DNHWCFP16_param_5];
	ld.param.u32 	%r44, [ShapetoBatch4DNHWCFP16_param_6];
	ld.param.u32 	%r45, [ShapetoBatch4DNHWCFP16_param_7];
	ld.param.u32 	%r46, [ShapetoBatch4DNHWCFP16_param_8];
	ld.param.u32 	%r47, [ShapetoBatch4DNHWCFP16_param_9];
	ld.param.u32 	%r48, [ShapetoBatch4DNHWCFP16_param_10];
	ld.param.u32 	%r49, [ShapetoBatch4DNHWCFP16_param_11];
	ld.param.u64 	%rd5, [ShapetoBatch4DNHWCFP16_param_12];
	ld.param.u64 	%rd6, [ShapetoBatch4DNHWCFP16_param_13];
	ld.param.u32 	%r50, [ShapetoBatch4DNHWCFP16_param_14];
	ld.param.u32 	%r51, [ShapetoBatch4DNHWCFP16_param_15];
	ld.param.s8 	%rs1, [ShapetoBatch4DNHWCFP16_param_16];
	setp.lt.s32	%p4, %r43, 1;
	@%p4 bra 	BB33_26;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r53, %ctaid.x;
	mov.u32 	%r54, %ntid.x;
	mov.u32 	%r55, %tid.x;
	mad.lo.s32 	%r1, %r54, %r53, %r55;
	mov.u32 	%r56, %ntid.y;
	mov.u32 	%r57, %ctaid.y;
	mov.u32 	%r58, %tid.y;
	mad.lo.s32 	%r2, %r56, %r57, %r58;
	mov.u32 	%r59, %nctaid.x;
	mul.lo.s32 	%r3, %r59, %r54;
	mov.u32 	%r60, %ntid.z;
	mov.u32 	%r61, %ctaid.z;
	mov.u32 	%r62, %tid.z;
	mad.lo.s32 	%r4, %r60, %r61, %r62;
	mov.u32 	%r63, %nctaid.y;
	mul.lo.s32 	%r5, %r63, %r56;
	mov.u32 	%r64, %nctaid.z;
	mul.lo.s32 	%r6, %r64, %r60;
	mad.lo.s32 	%r9, %r61, %r60, %r62;
	mov.u32 	%r75, 0;
	and.b16  	%rs2, %rs1, 255;

BB33_2:
	setp.lt.s32	%p5, %r46, 1;
	@%p5 bra 	BB33_25;

	mad.lo.s32 	%r11, %r44, %r75, %r4;
	mad.lo.s32 	%r12, %r45, %r75, %r4;
	mov.u32 	%r76, 0;

BB33_4:
	setp.lt.s32	%p6, %r47, 1;
	@%p6 bra 	BB33_24;

	mul.lo.s32 	%r16, %r76, %r48;
	mul.lo.s32 	%r15, %r47, %r76;
	mov.u32 	%r77, 0;

BB33_6:
	setp.ge.s32	%p7, %r1, %r38;
	@%p7 bra 	BB33_23;

	mul.lo.s32 	%r18, %r77, %r49;
	add.s32 	%r67, %r15, %r77;
	mul.lo.s32 	%r19, %r38, %r67;
	mov.u32 	%r78, %r1;

BB33_8:
	setp.ge.s32	%p8, %r2, %r39;
	@%p8 bra 	BB33_22;

	setp.gt.s32	%p9, %r51, 0;
	add.s32 	%r21, %r78, %r16;
	setp.lt.s32	%p10, %r21, %r41;
	and.pred  	%p1, %p9, %p10;
	add.s32 	%r68, %r19, %r78;
	mul.lo.s32 	%r22, %r39, %r68;
	add.s32 	%r69, %r16, %r78;
	mul.lo.s32 	%r23, %r41, %r69;
	mov.u32 	%r79, %r2;

BB33_10:
	setp.ge.s32	%p11, %r4, %r40;
	@%p11 bra 	BB33_21;

	add.s32 	%r70, %r79, %r18;
	setp.lt.s32	%p13, %r70, %r42;
	and.pred  	%p2, %p10, %p13;
	setp.gt.s32	%p14, %r50, 0;
	and.pred  	%p3, %p14, %p13;
	add.s32 	%r71, %r22, %r79;
	mad.lo.s32 	%r82, %r40, %r71, %r11;
	add.s32 	%r72, %r70, %r23;
	mad.lo.s32 	%r81, %r40, %r72, %r12;
	mov.u32 	%r80, %r9;

BB33_12:
	mul.wide.s32 	%rd7, %r82, 2;
	add.s64 	%rd3, %rd2, %rd7;
	mul.wide.s32 	%rd8, %r81, 2;
	add.s64 	%rd4, %rd1, %rd8;
	setp.eq.s16	%p15, %rs2, 0;
	@%p15 bra 	BB33_19;
	bra.uni 	BB33_13;

BB33_19:
	ld.global.u16 	%rs7, [%rd4];
	ld.global.u16 	%rs8, [%rd3];
	// inline asm
	{add.f16 %rs6,%rs7,%rs8;
}
	// inline asm
	st.global.u16 	[%rd4], %rs6;
	bra.uni 	BB33_20;

BB33_13:
	@%p2 bra 	BB33_18;
	bra.uni 	BB33_14;

BB33_18:
	ld.global.u16 	%rs5, [%rd4];
	st.global.u16 	[%rd3], %rs5;
	bra.uni 	BB33_20;

BB33_14:
	@!%p3 bra 	BB33_16;
	bra.uni 	BB33_15;

BB33_15:
	mov.u32 	%r73, 0;
	// inline asm
	cvt.rn.f16.s32 %rs3, %r73;
	// inline asm
	st.global.u16 	[%rd3], %rs3;

BB33_16:
	@!%p1 bra 	BB33_20;
	bra.uni 	BB33_17;

BB33_17:
	mov.u32 	%r74, 0;
	// inline asm
	cvt.rn.f16.s32 %rs4, %r74;
	// inline asm
	st.global.u16 	[%rd3], %rs4;

BB33_20:
	add.s32 	%r82, %r82, %r6;
	add.s32 	%r81, %r81, %r6;
	add.s32 	%r80, %r80, %r6;
	setp.lt.s32	%p16, %r80, %r40;
	@%p16 bra 	BB33_12;

BB33_21:
	add.s32 	%r79, %r5, %r79;
	setp.lt.s32	%p17, %r79, %r39;
	@%p17 bra 	BB33_10;

BB33_22:
	add.s32 	%r78, %r3, %r78;
	setp.lt.s32	%p18, %r78, %r38;
	@%p18 bra 	BB33_8;

BB33_23:
	add.s32 	%r77, %r77, 1;
	setp.lt.s32	%p19, %r77, %r47;
	@%p19 bra 	BB33_6;

BB33_24:
	add.s32 	%r76, %r76, 1;
	setp.lt.s32	%p20, %r76, %r46;
	@%p20 bra 	BB33_4;

BB33_25:
	add.s32 	%r75, %r75, 1;
	setp.lt.s32	%p21, %r75, %r43;
	@%p21 bra 	BB33_2;

BB33_26:
	ret;
}

	// .globl	ShapetoBatch4DNCHWFP16
.visible .entry ShapetoBatch4DNCHWFP16(
	.param .u32 ShapetoBatch4DNCHWFP16_param_0,
	.param .u32 ShapetoBatch4DNCHWFP16_param_1,
	.param .u32 ShapetoBatch4DNCHWFP16_param_2,
	.param .u32 ShapetoBatch4DNCHWFP16_param_3,
	.param .u32 ShapetoBatch4DNCHWFP16_param_4,
	.param .u32 ShapetoBatch4DNCHWFP16_param_5,
	.param .u32 ShapetoBatch4DNCHWFP16_param_6,
	.param .u32 ShapetoBatch4DNCHWFP16_param_7,
	.param .u32 ShapetoBatch4DNCHWFP16_param_8,
	.param .u32 ShapetoBatch4DNCHWFP16_param_9,
	.param .u32 ShapetoBatch4DNCHWFP16_param_10,
	.param .u32 ShapetoBatch4DNCHWFP16_param_11,
	.param .u64 ShapetoBatch4DNCHWFP16_param_12,
	.param .u64 ShapetoBatch4DNCHWFP16_param_13,
	.param .u32 ShapetoBatch4DNCHWFP16_param_14,
	.param .u32 ShapetoBatch4DNCHWFP16_param_15,
	.param .u8 ShapetoBatch4DNCHWFP16_param_16
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<86>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r41, [ShapetoBatch4DNCHWFP16_param_0];
	ld.param.u32 	%r42, [ShapetoBatch4DNCHWFP16_param_1];
	ld.param.u32 	%r43, [ShapetoBatch4DNCHWFP16_param_2];
	ld.param.u32 	%r44, [ShapetoBatch4DNCHWFP16_param_3];
	ld.param.u32 	%r45, [ShapetoBatch4DNCHWFP16_param_4];
	ld.param.u32 	%r46, [ShapetoBatch4DNCHWFP16_param_5];
	ld.param.u32 	%r47, [ShapetoBatch4DNCHWFP16_param_6];
	ld.param.u32 	%r48, [ShapetoBatch4DNCHWFP16_param_7];
	ld.param.u32 	%r49, [ShapetoBatch4DNCHWFP16_param_8];
	ld.param.u32 	%r50, [ShapetoBatch4DNCHWFP16_param_9];
	ld.param.u32 	%r51, [ShapetoBatch4DNCHWFP16_param_10];
	ld.param.u32 	%r52, [ShapetoBatch4DNCHWFP16_param_11];
	ld.param.u64 	%rd5, [ShapetoBatch4DNCHWFP16_param_12];
	ld.param.u64 	%rd6, [ShapetoBatch4DNCHWFP16_param_13];
	ld.param.u32 	%r53, [ShapetoBatch4DNCHWFP16_param_14];
	ld.param.u32 	%r54, [ShapetoBatch4DNCHWFP16_param_15];
	ld.param.s8 	%rs1, [ShapetoBatch4DNCHWFP16_param_16];
	setp.lt.s32	%p2, %r46, 1;
	@%p2 bra 	BB34_26;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r56, %ctaid.x;
	mov.u32 	%r57, %ntid.x;
	mov.u32 	%r58, %tid.x;
	mad.lo.s32 	%r1, %r57, %r56, %r58;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r61, %tid.y;
	mad.lo.s32 	%r2, %r59, %r60, %r61;
	mov.u32 	%r62, %nctaid.x;
	mul.lo.s32 	%r3, %r62, %r57;
	mov.u32 	%r63, %ntid.z;
	mov.u32 	%r64, %ctaid.z;
	mov.u32 	%r65, %tid.z;
	mad.lo.s32 	%r4, %r63, %r64, %r65;
	mov.u32 	%r66, %nctaid.y;
	mul.lo.s32 	%r5, %r66, %r59;
	mov.u32 	%r67, %nctaid.z;
	mul.lo.s32 	%r6, %r67, %r63;
	mad.lo.s32 	%r10, %r64, %r63, %r65;
	mov.u32 	%r77, 0;
	and.b16  	%rs2, %rs1, 255;

BB34_2:
	setp.lt.s32	%p3, %r49, 1;
	@%p3 bra 	BB34_25;

	mad.lo.s32 	%r12, %r47, %r77, %r4;
	mad.lo.s32 	%r13, %r48, %r77, %r4;
	mov.u32 	%r78, 0;

BB34_4:
	setp.lt.s32	%p4, %r50, 1;
	@%p4 bra 	BB34_24;

	mul.lo.s32 	%r15, %r78, %r51;
	mul.lo.s32 	%r16, %r50, %r78;
	mov.u32 	%r79, 0;

BB34_6:
	setp.ge.s32	%p5, %r1, %r41;
	@%p5 bra 	BB34_23;

	add.s32 	%r70, %r16, %r79;
	mul.lo.s32 	%r18, %r43, %r70;
	mul.lo.s32 	%r71, %r52, %r79;
	add.s32 	%r19, %r13, %r71;
	add.s32 	%r20, %r4, %r71;
	mov.u32 	%r80, %r1;

BB34_8:
	setp.ge.s32	%p6, %r2, %r42;
	@%p6 bra 	BB34_22;

	add.s32 	%r72, %r18, %r80;
	mul.lo.s32 	%r22, %r41, %r72;
	mul.lo.s32 	%r23, %r44, %r80;
	mov.u32 	%r81, %r2;

BB34_10:
	setp.ge.s32	%p7, %r4, %r43;
	@%p7 bra 	BB34_21;

	setp.gt.s32	%p8, %r54, 0;
	add.s32 	%r25, %r81, %r15;
	setp.lt.s32	%p9, %r25, %r44;
	and.pred  	%p1, %p8, %p9;
	add.s32 	%r73, %r22, %r81;
	mad.lo.s32 	%r85, %r42, %r73, %r12;
	add.s32 	%r74, %r25, %r23;
	mad.lo.s32 	%r84, %r45, %r74, %r19;
	mov.u32 	%r82, %r10;
	mov.u32 	%r83, %r20;

BB34_12:
	mul.wide.s32 	%rd7, %r85, 2;
	add.s64 	%rd3, %rd2, %rd7;
	mul.wide.s32 	%rd8, %r84, 2;
	add.s64 	%rd4, %rd1, %rd8;
	setp.eq.s16	%p10, %rs2, 0;
	@%p10 bra 	BB34_19;
	bra.uni 	BB34_13;

BB34_19:
	ld.global.u16 	%rs7, [%rd4];
	ld.global.u16 	%rs8, [%rd3];
	// inline asm
	{add.f16 %rs6,%rs7,%rs8;
}
	// inline asm
	st.global.u16 	[%rd4], %rs6;
	bra.uni 	BB34_20;

BB34_13:
	setp.lt.s32	%p12, %r83, %r45;
	and.pred  	%p13, %p9, %p12;
	@%p13 bra 	BB34_18;
	bra.uni 	BB34_14;

BB34_18:
	ld.global.u16 	%rs5, [%rd4];
	st.global.u16 	[%rd3], %rs5;
	bra.uni 	BB34_20;

BB34_14:
	setp.ge.s32	%p14, %r83, %r45;
	setp.lt.s32	%p15, %r53, 1;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	BB34_16;

	mov.u32 	%r75, 0;
	// inline asm
	cvt.rn.f16.s32 %rs3, %r75;
	// inline asm
	st.global.u16 	[%rd3], %rs3;

BB34_16:
	@!%p1 bra 	BB34_20;
	bra.uni 	BB34_17;

BB34_17:
	mov.u32 	%r76, 0;
	// inline asm
	cvt.rn.f16.s32 %rs4, %r76;
	// inline asm
	st.global.u16 	[%rd3], %rs4;

BB34_20:
	add.s32 	%r85, %r85, %r6;
	add.s32 	%r84, %r84, %r6;
	add.s32 	%r83, %r83, %r6;
	add.s32 	%r82, %r82, %r6;
	setp.lt.s32	%p17, %r82, %r43;
	@%p17 bra 	BB34_12;

BB34_21:
	add.s32 	%r81, %r5, %r81;
	setp.lt.s32	%p18, %r81, %r42;
	@%p18 bra 	BB34_10;

BB34_22:
	add.s32 	%r80, %r3, %r80;
	setp.lt.s32	%p19, %r80, %r41;
	@%p19 bra 	BB34_8;

BB34_23:
	add.s32 	%r79, %r79, 1;
	setp.lt.s32	%p20, %r79, %r50;
	@%p20 bra 	BB34_6;

BB34_24:
	add.s32 	%r78, %r78, 1;
	setp.lt.s32	%p21, %r78, %r49;
	@%p21 bra 	BB34_4;

BB34_25:
	add.s32 	%r77, %r77, 1;
	setp.lt.s32	%p22, %r77, %r46;
	@%p22 bra 	BB34_2;

BB34_26:
	ret;
}

	// .globl	NearestNeighborNCHWFP16
.visible .entry NearestNeighborNCHWFP16(
	.param .u32 NearestNeighborNCHWFP16_param_0,
	.param .u32 NearestNeighborNCHWFP16_param_1,
	.param .u64 NearestNeighborNCHWFP16_param_2,
	.param .u32 NearestNeighborNCHWFP16_param_3,
	.param .u32 NearestNeighborNCHWFP16_param_4,
	.param .u32 NearestNeighborNCHWFP16_param_5,
	.param .u32 NearestNeighborNCHWFP16_param_6,
	.param .u32 NearestNeighborNCHWFP16_param_7,
	.param .f32 NearestNeighborNCHWFP16_param_8,
	.param .f32 NearestNeighborNCHWFP16_param_9,
	.param .u64 NearestNeighborNCHWFP16_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNCHWFP16_param_0];
	ld.param.u32 	%r9, [NearestNeighborNCHWFP16_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNCHWFP16_param_2];
	ld.param.u32 	%r10, [NearestNeighborNCHWFP16_param_3];
	ld.param.u32 	%r11, [NearestNeighborNCHWFP16_param_4];
	ld.param.u32 	%r12, [NearestNeighborNCHWFP16_param_5];
	ld.param.u32 	%r13, [NearestNeighborNCHWFP16_param_6];
	ld.param.u32 	%r14, [NearestNeighborNCHWFP16_param_7];
	ld.param.f32 	%f13, [NearestNeighborNCHWFP16_param_8];
	ld.param.f32 	%f14, [NearestNeighborNCHWFP16_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNCHWFP16_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r36, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r36, %r9;
	@%p1 bra 	BB35_11;

	add.s32 	%r18, %r10, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r11, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;

BB35_2:
	rem.s32 	%r3, %r36, %r14;
	div.s32 	%r20, %r36, %r14;
	rem.s32 	%r21, %r20, %r13;
	div.s32 	%r4, %r20, %r13;
	rem.s32 	%r5, %r4, %r12;
	cvt.rn.f32.s32	%f15, %r21;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB35_5;
	bra.uni 	BB35_3;

BB35_5:
	cvt.rmi.f32.f32	%f27, %f3;
	bra.uni 	BB35_6;

BB35_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r22, %f3;
	and.b32  	%r23, %r22, -2147483648;
	or.b32  	%r24, %r23, 1056964608;
	mov.b32 	 %f17, %r24;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f27, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB35_6;

	cvt.rzi.f32.f32	%f27, %f3;

BB35_6:
	min.f32 	%f20, %f27, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r3;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB35_9;
	bra.uni 	BB35_7;

BB35_9:
	cvt.rmi.f32.f32	%f28, %f8;
	bra.uni 	BB35_10;

BB35_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r25, %f8;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, 1056964608;
	mov.b32 	 %f23, %r27;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f28, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB35_10;

	cvt.rzi.f32.f32	%f28, %f8;

BB35_10:
	sub.s32 	%r28, %r4, %r5;
	mul.lo.s32 	%r29, %r11, %r10;
	min.f32 	%f26, %f28, %f2;
	cvt.rzi.s32.f32	%r30, %f26;
	mad.lo.s32 	%r31, %r5, %r10, %r6;
	mad.lo.s32 	%r32, %r31, %r11, %r30;
	mul.wide.s32 	%rd4, %r36, 2;
	add.s64 	%rd5, %rd3, %rd4;
	mad.lo.s32 	%r33, %r29, %r28, %r32;
	mul.wide.s32 	%rd7, %r33, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u16 	%rs1, [%rd8];
	st.global.u16 	[%rd5], %rs1;
	mov.u32 	%r35, %nctaid.x;
	mad.lo.s32 	%r36, %r35, %r16, %r36;
	setp.lt.s32	%p8, %r36, %r9;
	@%p8 bra 	BB35_2;

BB35_11:
	ret;
}

	// .globl	NearestNeighborNHWCBackFP16
.visible .entry NearestNeighborNHWCBackFP16(
	.param .u32 NearestNeighborNHWCBackFP16_param_0,
	.param .u32 NearestNeighborNHWCBackFP16_param_1,
	.param .u64 NearestNeighborNHWCBackFP16_param_2,
	.param .u32 NearestNeighborNHWCBackFP16_param_3,
	.param .u32 NearestNeighborNHWCBackFP16_param_4,
	.param .u32 NearestNeighborNHWCBackFP16_param_5,
	.param .u32 NearestNeighborNHWCBackFP16_param_6,
	.param .u32 NearestNeighborNHWCBackFP16_param_7,
	.param .f32 NearestNeighborNHWCBackFP16_param_8,
	.param .f32 NearestNeighborNHWCBackFP16_param_9,
	.param .u64 NearestNeighborNHWCBackFP16_param_10
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r12, [NearestNeighborNHWCBackFP16_param_0];
	ld.param.u32 	%r13, [NearestNeighborNHWCBackFP16_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNHWCBackFP16_param_2];
	ld.param.u32 	%r14, [NearestNeighborNHWCBackFP16_param_3];
	ld.param.u32 	%r15, [NearestNeighborNHWCBackFP16_param_4];
	ld.param.u32 	%r16, [NearestNeighborNHWCBackFP16_param_5];
	ld.param.u32 	%r17, [NearestNeighborNHWCBackFP16_param_6];
	ld.param.u32 	%r18, [NearestNeighborNHWCBackFP16_param_7];
	ld.param.f32 	%f23, [NearestNeighborNHWCBackFP16_param_8];
	ld.param.f32 	%f24, [NearestNeighborNHWCBackFP16_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNHWCBackFP16_param_10];
	mov.f32 	%f25, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f25;}

	// inline asm
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r65, %r19, %r20, %r21;
	add.s32 	%r22, %r13, -1;
	setp.ge.s32	%p1, %r65, %r22;
	@%p1 bra 	BB36_11;

	add.s32 	%r23, %r17, -1;
	cvt.rn.f32.s32	%f1, %r23;
	add.s32 	%r24, %r18, -1;
	cvt.rn.f32.s32	%f2, %r24;

BB36_2:
	rem.s32 	%r3, %r65, %r16;
	div.s32 	%r25, %r65, %r16;
	rem.s32 	%r4, %r25, %r15;
	div.s32 	%r26, %r25, %r15;
	rem.s32 	%r27, %r26, %r14;
	div.s32 	%r5, %r26, %r14;
	cvt.rn.f32.s32	%f26, %r27;
	mul.f32 	%f3, %f26, %f23;
	setp.eq.s32	%p2, %r12, 0;
	@%p2 bra 	BB36_5;
	bra.uni 	BB36_3;

BB36_5:
	cvt.rmi.f32.f32	%f52, %f3;
	bra.uni 	BB36_6;

BB36_3:
	abs.f32 	%f27, %f3;
	mov.b32 	 %r28, %f3;
	and.b32  	%r29, %r28, -2147483648;
	or.b32  	%r30, %r29, 1056964608;
	mov.b32 	 %f28, %r30;
	add.f32 	%f29, %f3, %f28;
	cvt.rzi.f32.f32	%f30, %f29;
	setp.gt.f32	%p3, %f27, 0f4B000000;
	selp.f32	%f52, %f3, %f30, %p3;
	setp.geu.f32	%p4, %f27, 0f3F000000;
	@%p4 bra 	BB36_6;

	cvt.rzi.f32.f32	%f52, %f3;

BB36_6:
	min.f32 	%f31, %f52, %f1;
	cvt.rzi.s32.f32	%r6, %f31;
	cvt.rn.f32.s32	%f32, %r4;
	mul.f32 	%f8, %f32, %f24;
	@%p2 bra 	BB36_9;
	bra.uni 	BB36_7;

BB36_9:
	cvt.rmi.f32.f32	%f53, %f8;
	bra.uni 	BB36_10;

BB36_7:
	abs.f32 	%f33, %f8;
	mov.b32 	 %r31, %f8;
	and.b32  	%r32, %r31, -2147483648;
	or.b32  	%r33, %r32, 1056964608;
	mov.b32 	 %f34, %r33;
	add.f32 	%f35, %f8, %f34;
	cvt.rzi.f32.f32	%f36, %f35;
	setp.gt.f32	%p6, %f33, 0f4B000000;
	selp.f32	%f53, %f8, %f36, %p6;
	setp.geu.f32	%p7, %f33, 0f3F000000;
	@%p7 bra 	BB36_10;

	cvt.rzi.f32.f32	%f53, %f8;

BB36_10:
	mul.lo.s32 	%r37, %r15, %r14;
	mul.lo.s32 	%r38, %r37, %r16;
	min.f32 	%f37, %f53, %f2;
	cvt.rzi.s32.f32	%r39, %f37;
	mad.lo.s32 	%r40, %r6, %r18, %r39;
	mad.lo.s32 	%r41, %r40, %r16, %r3;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r65, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs3, [%rd6];
	// inline asm
	{  mov.b32 %r34, {%rs3,%rs2};}

	// inline asm
	mad.lo.s32 	%r42, %r38, %r5, %r41;
	mul.wide.s32 	%rd7, %r42, 2;
	add.s64 	%rd3, %rd1, %rd7;
	// inline asm
	{ atom.add.noftz.f16x2 %r35,[%rd3],%r34; }

	// inline asm
	mov.u32 	%r44, %nctaid.x;
	mad.lo.s32 	%r65, %r44, %r19, %r65;
	setp.lt.s32	%p8, %r65, %r22;
	@%p8 bra 	BB36_2;

BB36_11:
	setp.eq.s32	%p9, %r12, 0;
	rem.s32 	%r8, %r22, %r16;
	div.s32 	%r47, %r22, %r16;
	rem.s32 	%r9, %r47, %r15;
	div.s32 	%r48, %r47, %r15;
	rem.s32 	%r49, %r48, %r14;
	div.s32 	%r10, %r48, %r14;
	cvt.rn.f32.s32	%f38, %r49;
	mul.f32 	%f13, %f38, %f23;
	@%p9 bra 	BB36_14;

	abs.f32 	%f39, %f13;
	mov.b32 	 %r50, %f13;
	and.b32  	%r51, %r50, -2147483648;
	or.b32  	%r52, %r51, 1056964608;
	mov.b32 	 %f40, %r52;
	add.f32 	%f41, %f13, %f40;
	cvt.rzi.f32.f32	%f42, %f41;
	setp.gt.f32	%p10, %f39, 0f4B000000;
	selp.f32	%f54, %f13, %f42, %p10;
	setp.geu.f32	%p11, %f39, 0f3F000000;
	@%p11 bra 	BB36_15;

	cvt.rzi.f32.f32	%f54, %f13;
	bra.uni 	BB36_15;

BB36_14:
	cvt.rmi.f32.f32	%f54, %f13;

BB36_15:
	add.s32 	%r53, %r17, -1;
	cvt.rn.f32.s32	%f43, %r53;
	min.f32 	%f44, %f54, %f43;
	cvt.rzi.s32.f32	%r11, %f44;
	cvt.rn.f32.s32	%f45, %r9;
	mul.f32 	%f18, %f45, %f24;
	@%p9 bra 	BB36_18;

	abs.f32 	%f46, %f18;
	mov.b32 	 %r54, %f18;
	and.b32  	%r55, %r54, -2147483648;
	or.b32  	%r56, %r55, 1056964608;
	mov.b32 	 %f47, %r56;
	add.f32 	%f48, %f18, %f47;
	cvt.rzi.f32.f32	%f49, %f48;
	setp.gt.f32	%p13, %f46, 0f4B000000;
	selp.f32	%f55, %f18, %f49, %p13;
	setp.geu.f32	%p14, %f46, 0f3F000000;
	@%p14 bra 	BB36_19;

	cvt.rzi.f32.f32	%f55, %f18;
	bra.uni 	BB36_19;

BB36_18:
	cvt.rmi.f32.f32	%f55, %f18;

BB36_19:
	mul.lo.s32 	%r57, %r15, %r14;
	mul.lo.s32 	%r58, %r57, %r16;
	add.s32 	%r59, %r18, -1;
	cvt.rn.f32.s32	%f50, %r59;
	min.f32 	%f51, %f55, %f50;
	cvt.rzi.s32.f32	%r60, %f51;
	mad.lo.s32 	%r61, %r11, %r18, %r60;
	mad.lo.s32 	%r62, %r61, %r16, %r8;
	mad.lo.s32 	%r63, %r58, %r10, %r62;
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r63, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u16 	%rs6, [%rd10];
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r22, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u16 	%rs7, [%rd13];
	// inline asm
	{add.f16 %rs5,%rs6,%rs7;
}
	// inline asm
	st.global.u16 	[%rd10], %rs5;
	ret;
}

	// .globl	NearestNeighborNHWCFP16
.visible .entry NearestNeighborNHWCFP16(
	.param .u32 NearestNeighborNHWCFP16_param_0,
	.param .u32 NearestNeighborNHWCFP16_param_1,
	.param .u64 NearestNeighborNHWCFP16_param_2,
	.param .u32 NearestNeighborNHWCFP16_param_3,
	.param .u32 NearestNeighborNHWCFP16_param_4,
	.param .u32 NearestNeighborNHWCFP16_param_5,
	.param .u32 NearestNeighborNHWCFP16_param_6,
	.param .u32 NearestNeighborNHWCFP16_param_7,
	.param .f32 NearestNeighborNHWCFP16_param_8,
	.param .f32 NearestNeighborNHWCFP16_param_9,
	.param .u64 NearestNeighborNHWCFP16_param_10
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r8, [NearestNeighborNHWCFP16_param_0];
	ld.param.u32 	%r9, [NearestNeighborNHWCFP16_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNHWCFP16_param_2];
	ld.param.u32 	%r10, [NearestNeighborNHWCFP16_param_3];
	ld.param.u32 	%r11, [NearestNeighborNHWCFP16_param_4];
	ld.param.u32 	%r12, [NearestNeighborNHWCFP16_param_5];
	ld.param.u32 	%r13, [NearestNeighborNHWCFP16_param_6];
	ld.param.u32 	%r14, [NearestNeighborNHWCFP16_param_7];
	ld.param.f32 	%f13, [NearestNeighborNHWCFP16_param_8];
	ld.param.f32 	%f14, [NearestNeighborNHWCFP16_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNHWCFP16_param_10];
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r37, %r16, %r15, %r17;
	setp.ge.s32	%p1, %r37, %r9;
	@%p1 bra 	BB37_11;

	add.s32 	%r18, %r10, -1;
	cvt.rn.f32.s32	%f1, %r18;
	add.s32 	%r19, %r11, -1;
	cvt.rn.f32.s32	%f2, %r19;
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;

BB37_2:
	rem.s32 	%r3, %r37, %r12;
	div.s32 	%r20, %r37, %r12;
	rem.s32 	%r4, %r20, %r14;
	div.s32 	%r21, %r20, %r14;
	rem.s32 	%r22, %r21, %r13;
	div.s32 	%r5, %r21, %r13;
	cvt.rn.f32.s32	%f15, %r22;
	mul.f32 	%f3, %f15, %f13;
	setp.eq.s32	%p2, %r8, 0;
	@%p2 bra 	BB37_5;
	bra.uni 	BB37_3;

BB37_5:
	cvt.rmi.f32.f32	%f27, %f3;
	bra.uni 	BB37_6;

BB37_3:
	abs.f32 	%f16, %f3;
	mov.b32 	 %r23, %f3;
	and.b32  	%r24, %r23, -2147483648;
	or.b32  	%r25, %r24, 1056964608;
	mov.b32 	 %f17, %r25;
	add.f32 	%f18, %f3, %f17;
	cvt.rzi.f32.f32	%f19, %f18;
	setp.gt.f32	%p3, %f16, 0f4B000000;
	selp.f32	%f27, %f3, %f19, %p3;
	setp.geu.f32	%p4, %f16, 0f3F000000;
	@%p4 bra 	BB37_6;

	cvt.rzi.f32.f32	%f27, %f3;

BB37_6:
	min.f32 	%f20, %f27, %f1;
	cvt.rzi.s32.f32	%r6, %f20;
	cvt.rn.f32.s32	%f21, %r4;
	mul.f32 	%f8, %f21, %f14;
	@%p2 bra 	BB37_9;
	bra.uni 	BB37_7;

BB37_9:
	cvt.rmi.f32.f32	%f28, %f8;
	bra.uni 	BB37_10;

BB37_7:
	abs.f32 	%f22, %f8;
	mov.b32 	 %r26, %f8;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	 %f23, %r28;
	add.f32 	%f24, %f8, %f23;
	cvt.rzi.f32.f32	%f25, %f24;
	setp.gt.f32	%p6, %f22, 0f4B000000;
	selp.f32	%f28, %f8, %f25, %p6;
	setp.geu.f32	%p7, %f22, 0f3F000000;
	@%p7 bra 	BB37_10;

	cvt.rzi.f32.f32	%f28, %f8;

BB37_10:
	mul.lo.s32 	%r29, %r11, %r10;
	mul.lo.s32 	%r30, %r29, %r12;
	min.f32 	%f26, %f28, %f2;
	cvt.rzi.s32.f32	%r31, %f26;
	mad.lo.s32 	%r32, %r6, %r11, %r31;
	mad.lo.s32 	%r33, %r32, %r12, %r3;
	mul.wide.s32 	%rd4, %r37, 2;
	add.s64 	%rd5, %rd3, %rd4;
	mad.lo.s32 	%r34, %r30, %r5, %r33;
	mul.wide.s32 	%rd7, %r34, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u16 	%rs1, [%rd8];
	st.global.u16 	[%rd5], %rs1;
	mov.u32 	%r36, %nctaid.x;
	mad.lo.s32 	%r37, %r36, %r16, %r37;
	setp.lt.s32	%p8, %r37, %r9;
	@%p8 bra 	BB37_2;

BB37_11:
	ret;
}

	// .globl	NearestNeighborNCHWBackFP16
.visible .entry NearestNeighborNCHWBackFP16(
	.param .u32 NearestNeighborNCHWBackFP16_param_0,
	.param .u32 NearestNeighborNCHWBackFP16_param_1,
	.param .u64 NearestNeighborNCHWBackFP16_param_2,
	.param .u32 NearestNeighborNCHWBackFP16_param_3,
	.param .u32 NearestNeighborNCHWBackFP16_param_4,
	.param .u32 NearestNeighborNCHWBackFP16_param_5,
	.param .u32 NearestNeighborNCHWBackFP16_param_6,
	.param .u32 NearestNeighborNCHWBackFP16_param_7,
	.param .f32 NearestNeighborNCHWBackFP16_param_8,
	.param .f32 NearestNeighborNCHWBackFP16_param_9,
	.param .u64 NearestNeighborNCHWBackFP16_param_10
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<14>;


	ld.param.u32 	%r12, [NearestNeighborNCHWBackFP16_param_0];
	ld.param.u32 	%r13, [NearestNeighborNCHWBackFP16_param_1];
	ld.param.u64 	%rd1, [NearestNeighborNCHWBackFP16_param_2];
	ld.param.u32 	%r14, [NearestNeighborNCHWBackFP16_param_3];
	ld.param.u32 	%r15, [NearestNeighborNCHWBackFP16_param_4];
	ld.param.u32 	%r16, [NearestNeighborNCHWBackFP16_param_5];
	ld.param.u32 	%r17, [NearestNeighborNCHWBackFP16_param_6];
	ld.param.u32 	%r18, [NearestNeighborNCHWBackFP16_param_7];
	ld.param.f32 	%f23, [NearestNeighborNCHWBackFP16_param_8];
	ld.param.f32 	%f24, [NearestNeighborNCHWBackFP16_param_9];
	ld.param.u64 	%rd2, [NearestNeighborNCHWBackFP16_param_10];
	mov.f32 	%f25, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f25;}

	// inline asm
	mov.u32 	%r19, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r63, %r19, %r20, %r21;
	add.s32 	%r22, %r13, -1;
	setp.ge.s32	%p1, %r63, %r22;
	@%p1 bra 	BB38_11;

	add.s32 	%r23, %r17, -1;
	cvt.rn.f32.s32	%f1, %r23;
	add.s32 	%r24, %r18, -1;
	cvt.rn.f32.s32	%f2, %r24;

BB38_2:
	rem.s32 	%r3, %r63, %r15;
	div.s32 	%r25, %r63, %r15;
	rem.s32 	%r26, %r25, %r14;
	div.s32 	%r4, %r25, %r14;
	rem.s32 	%r5, %r4, %r16;
	cvt.rn.f32.s32	%f26, %r26;
	mul.f32 	%f3, %f26, %f23;
	setp.eq.s32	%p2, %r12, 0;
	@%p2 bra 	BB38_5;
	bra.uni 	BB38_3;

BB38_5:
	cvt.rmi.f32.f32	%f52, %f3;
	bra.uni 	BB38_6;

BB38_3:
	abs.f32 	%f27, %f3;
	mov.b32 	 %r27, %f3;
	and.b32  	%r28, %r27, -2147483648;
	or.b32  	%r29, %r28, 1056964608;
	mov.b32 	 %f28, %r29;
	add.f32 	%f29, %f3, %f28;
	cvt.rzi.f32.f32	%f30, %f29;
	setp.gt.f32	%p3, %f27, 0f4B000000;
	selp.f32	%f52, %f3, %f30, %p3;
	setp.geu.f32	%p4, %f27, 0f3F000000;
	@%p4 bra 	BB38_6;

	cvt.rzi.f32.f32	%f52, %f3;

BB38_6:
	min.f32 	%f31, %f52, %f1;
	cvt.rzi.s32.f32	%r6, %f31;
	cvt.rn.f32.s32	%f32, %r3;
	mul.f32 	%f8, %f32, %f24;
	@%p2 bra 	BB38_9;
	bra.uni 	BB38_7;

BB38_9:
	cvt.rmi.f32.f32	%f53, %f8;
	bra.uni 	BB38_10;

BB38_7:
	abs.f32 	%f33, %f8;
	mov.b32 	 %r30, %f8;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	 %f34, %r32;
	add.f32 	%f35, %f8, %f34;
	cvt.rzi.f32.f32	%f36, %f35;
	setp.gt.f32	%p6, %f33, 0f4B000000;
	selp.f32	%f53, %f8, %f36, %p6;
	setp.geu.f32	%p7, %f33, 0f3F000000;
	@%p7 bra 	BB38_10;

	cvt.rzi.f32.f32	%f53, %f8;

BB38_10:
	sub.s32 	%r36, %r4, %r5;
	mul.lo.s32 	%r37, %r15, %r14;
	min.f32 	%f37, %f53, %f2;
	cvt.rzi.s32.f32	%r38, %f37;
	mad.lo.s32 	%r39, %r5, %r17, %r6;
	mad.lo.s32 	%r40, %r39, %r18, %r38;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r63, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs3, [%rd6];
	// inline asm
	{  mov.b32 %r33, {%rs3,%rs2};}

	// inline asm
	mad.lo.s32 	%r41, %r37, %r36, %r40;
	mul.wide.s32 	%rd7, %r41, 2;
	add.s64 	%rd3, %rd1, %rd7;
	// inline asm
	{ atom.add.noftz.f16x2 %r34,[%rd3],%r33; }

	// inline asm
	mov.u32 	%r43, %nctaid.x;
	mad.lo.s32 	%r63, %r43, %r19, %r63;
	setp.lt.s32	%p8, %r63, %r22;
	@%p8 bra 	BB38_2;

BB38_11:
	setp.eq.s32	%p9, %r12, 0;
	rem.s32 	%r8, %r22, %r15;
	div.s32 	%r46, %r22, %r15;
	rem.s32 	%r47, %r46, %r14;
	div.s32 	%r9, %r46, %r14;
	rem.s32 	%r10, %r9, %r16;
	cvt.rn.f32.s32	%f38, %r47;
	mul.f32 	%f13, %f38, %f23;
	@%p9 bra 	BB38_14;

	abs.f32 	%f39, %f13;
	mov.b32 	 %r48, %f13;
	and.b32  	%r49, %r48, -2147483648;
	or.b32  	%r50, %r49, 1056964608;
	mov.b32 	 %f40, %r50;
	add.f32 	%f41, %f13, %f40;
	cvt.rzi.f32.f32	%f42, %f41;
	setp.gt.f32	%p10, %f39, 0f4B000000;
	selp.f32	%f54, %f13, %f42, %p10;
	setp.geu.f32	%p11, %f39, 0f3F000000;
	@%p11 bra 	BB38_15;

	cvt.rzi.f32.f32	%f54, %f13;
	bra.uni 	BB38_15;

BB38_14:
	cvt.rmi.f32.f32	%f54, %f13;

BB38_15:
	add.s32 	%r51, %r17, -1;
	cvt.rn.f32.s32	%f43, %r51;
	min.f32 	%f44, %f54, %f43;
	cvt.rzi.s32.f32	%r11, %f44;
	cvt.rn.f32.s32	%f45, %r8;
	mul.f32 	%f18, %f45, %f24;
	@%p9 bra 	BB38_18;

	abs.f32 	%f46, %f18;
	mov.b32 	 %r52, %f18;
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r53, 1056964608;
	mov.b32 	 %f47, %r54;
	add.f32 	%f48, %f18, %f47;
	cvt.rzi.f32.f32	%f49, %f48;
	setp.gt.f32	%p13, %f46, 0f4B000000;
	selp.f32	%f55, %f18, %f49, %p13;
	setp.geu.f32	%p14, %f46, 0f3F000000;
	@%p14 bra 	BB38_19;

	cvt.rzi.f32.f32	%f55, %f18;
	bra.uni 	BB38_19;

BB38_18:
	cvt.rmi.f32.f32	%f55, %f18;

BB38_19:
	sub.s32 	%r55, %r9, %r10;
	mul.lo.s32 	%r56, %r15, %r14;
	add.s32 	%r57, %r18, -1;
	cvt.rn.f32.s32	%f50, %r57;
	min.f32 	%f51, %f55, %f50;
	cvt.rzi.s32.f32	%r58, %f51;
	mad.lo.s32 	%r59, %r10, %r17, %r11;
	mad.lo.s32 	%r60, %r59, %r18, %r58;
	mad.lo.s32 	%r61, %r56, %r55, %r60;
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r61, 2;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u16 	%rs6, [%rd10];
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r22, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u16 	%rs7, [%rd13];
	// inline asm
	{add.f16 %rs5,%rs6,%rs7;
}
	// inline asm
	st.global.u16 	[%rd10], %rs5;
	ret;
}

	// .globl	AdaGradFP16
.visible .entry AdaGradFP16(
	.param .u32 AdaGradFP16_param_0,
	.param .u64 AdaGradFP16_param_1,
	.param .u64 AdaGradFP16_param_2,
	.param .u64 AdaGradFP16_param_3,
	.param .align 2 .b8 AdaGradFP16_param_4[2],
	.param .align 2 .b8 AdaGradFP16_param_5[2],
	.param .align 2 .b8 AdaGradFP16_param_6[2]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<68>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd6, [AdaGradFP16_param_1];
	ld.param.u64 	%rd7, [AdaGradFP16_param_2];
	ld.param.u64 	%rd8, [AdaGradFP16_param_3];
	ld.param.u16 	%rs1, [AdaGradFP16_param_4];
	ld.param.u16 	%rs2, [AdaGradFP16_param_5];
	ld.param.u16 	%rs3, [AdaGradFP16_param_6];
	ld.param.u32 	%r15, [AdaGradFP16_param_0];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r19, %ctaid.x;
	mul.lo.s32 	%r2, %r1, %r19;
	// inline asm
	{  mov.b32 %r16, {%rs1,%rs1};}

	// inline asm
	// inline asm
	{  mov.b32 %r17, {%rs2,%rs2};}

	// inline asm
	// inline asm
	{  mov.b32 %r18, {%rs3,%rs3};}

	// inline asm
	mov.u32 	%r6, %tid.x;
	add.s32 	%r52, %r2, %r6;
	shr.u32 	%r20, %r15, 31;
	add.s32 	%r21, %r15, %r20;
	shr.s32 	%r8, %r21, 1;
	setp.ge.s32	%p1, %r52, %r8;
	@%p1 bra 	BB39_7;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd8;
	mov.u32 	%r22, %nctaid.x;
	mul.lo.s32 	%r9, %r22, %r1;

BB39_2:
	mul.wide.s32 	%rd9, %r52, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.u32 	%r26, [%rd10];
	add.s64 	%rd4, %rd2, %rd9;
	ld.global.u32 	%r24, [%rd4];
	// inline asm
	{fma.rn.f16x2 %r23,%r24,%r24,%r26;
}
	// inline asm
	st.global.u32 	[%rd10], %r23;
	ld.global.u32 	%r29, [%rd4];
	// inline asm
	{mul.f16x2 %r27,%r16,%r29;
}
	// inline asm
	// inline asm
	{.reg.b16         hl, hu;         
 .reg.b32         fl, fu;         
  mov.b32         {hl, hu}, %r23;   
  cvt.f32.f16     fl, hl;         
  cvt.f32.f16     fu, hu;         
  sqrt.approx.f32   fl, fl;     
  sqrt.approx.f32   fu, fu;     
  cvt.rn.f16.f32      hl, fl;     
  cvt.rn.f16.f32      hu, fu;     
  mov.b32         %r30, {hl, hu};   
}
	// inline asm
	// inline asm
	{add.f16x2 %r32,%r30,%r17;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r27;
 mov.b16 %rs19, low;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r32;
 mov.b16 %rs20, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f13, %rs19;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f14, %rs20;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f15, %f14;
}
	// inline asm
	mul.f32 	%f17, %f13, %f15;
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f17;}

	// inline asm
	and.b16  	%rs25, %rs65, 32767;
	mov.u16 	%rs26, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs25, %rs26;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs24, 0;
	setp.eq.s16	%p3, %rs25, 0;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB39_4;

	neg.f32 	%f19, %f14;
	fma.rn.f32 	%f20, %f19, %f17, %f13;
	fma.rn.f32 	%f18, %f15, %f20, %f17;
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f18;}

	// inline asm

BB39_4:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r27;
 mov.b16 %rs28, high;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r32;
 mov.b16 %rs29, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs28;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f22, %rs29;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f23, %f22;
}
	// inline asm
	mul.f32 	%f25, %f21, %f23;
	// inline asm
	{  cvt.rn.f16.f32 %rs66, %f25;}

	// inline asm
	and.b16  	%rs34, %rs66, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs34, %rs26;
  selp.u16 %rs33, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p5, %rs33, 0;
	setp.eq.s16	%p6, %rs34, 0;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	BB39_6;

	neg.f32 	%f27, %f22;
	fma.rn.f32 	%f28, %f27, %f25, %f21;
	fma.rn.f32 	%f26, %f23, %f28, %f25;
	// inline asm
	{  cvt.rn.f16.f32 %rs66, %f26;}

	// inline asm

BB39_6:
	// inline asm
	{  mov.b32 %r39, {%rs65,%rs66};}

	// inline asm
	mov.f32 	%f29, 0f00000000;
	// inline asm
	{.reg .f16 low;
  cvt.rn.f16.f32 low, %f29;
  mov.b32 %r40, {low,low};}

	// inline asm
	// inline asm
	{sub.f16x2 %r41,%r40,%r39;
}
	// inline asm
	add.s64 	%rd12, %rd1, %rd9;
	ld.global.u32 	%r46, [%rd12];
	// inline asm
	{add.f16x2 %r44,%r41,%r46;
}
	// inline asm
	st.global.u32 	[%rd12], %r44;
	ld.global.u32 	%r48, [%rd4];
	// inline asm
	{mul.f16x2 %r47,%r48,%r18;
}
	// inline asm
	st.global.u32 	[%rd4], %r47;
	add.s32 	%r52, %r9, %r52;
	setp.lt.s32	%p8, %r52, %r8;
	@%p8 bra 	BB39_2;

BB39_7:
	neg.s32 	%r50, %r6;
	setp.ne.s32	%p9, %r2, %r50;
	and.b32  	%r51, %r15, 1;
	setp.eq.b32	%p10, %r51, 1;
	not.pred 	%p11, %p10;
	or.pred  	%p12, %p9, %p11;
	@%p12 bra 	BB39_11;

	cvta.to.global.u64 	%rd13, %rd7;
	cvta.to.global.u64 	%rd14, %rd8;
	add.s32 	%r14, %r15, -1;
	mul.wide.s32 	%rd15, %r14, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u16 	%rs42, [%rd16];
	add.s64 	%rd5, %rd13, %rd15;
	ld.global.u16 	%rs40, [%rd5];
	// inline asm
	{fma.rn.f16 %rs39,%rs40,%rs40,%rs42;
}
	// inline asm
	st.global.u16 	[%rd16], %rs39;
	ld.global.u16 	%rs45, [%rd5];
	// inline asm
	{mul.f16 %rs43,%rs1,%rs45;
}
	// inline asm
	// inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs39;     
  cvt.f32.f16     f,r;      
  sqrt.approx.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs46,r;     
}
	// inline asm
	// inline asm
	{add.f16 %rs48,%rs46,%rs2;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f30, %rs43;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f31, %rs48;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f32, %f31;
}
	// inline asm
	mul.f32 	%f34, %f30, %f32;
	// inline asm
	{  cvt.rn.f16.f32 %rs67, %f34;}

	// inline asm
	and.b16  	%rs55, %rs67, 32767;
	mov.u16 	%rs56, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs55, %rs56;
  selp.u16 %rs54, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p13, %rs54, 0;
	setp.eq.s16	%p14, %rs55, 0;
	or.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB39_10;

	neg.f32 	%f36, %f31;
	fma.rn.f32 	%f37, %f36, %f34, %f30;
	fma.rn.f32 	%f35, %f32, %f37, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs67, %f35;}

	// inline asm

BB39_10:
	cvta.to.global.u64 	%rd17, %rd6;
	mov.f32 	%f38, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs58, %f38;}

	// inline asm
	// inline asm
	{sub.f16 %rs59,%rs58,%rs67;
}
	// inline asm
	add.s64 	%rd19, %rd17, %rd15;
	st.global.u16 	[%rd19], %rs59;
	ld.global.u16 	%rs63, [%rd5];
	// inline asm
	{mul.f16 %rs62,%rs63,%rs3;
}
	// inline asm
	st.global.u16 	[%rd5], %rs62;

BB39_11:
	ret;
}

	// .globl	AdamFP16
.visible .entry AdamFP16(
	.param .u32 AdamFP16_param_0,
	.param .u64 AdamFP16_param_1,
	.param .u64 AdamFP16_param_2,
	.param .u64 AdamFP16_param_3,
	.param .u64 AdamFP16_param_4,
	.param .align 2 .b8 AdamFP16_param_5[2],
	.param .align 2 .b8 AdamFP16_param_6[2],
	.param .align 2 .b8 AdamFP16_param_7[2],
	.param .align 2 .b8 AdamFP16_param_8[2],
	.param .align 2 .b8 AdamFP16_param_9[2],
	.param .align 2 .b8 AdamFP16_param_10[2],
	.param .align 2 .b8 AdamFP16_param_11[2]
)
{
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<184>;
	.reg .f32 	%f<110>;
	.reg .b32 	%r<93>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd10, [AdamFP16_param_1];
	ld.param.u64 	%rd11, [AdamFP16_param_2];
	ld.param.u64 	%rd12, [AdamFP16_param_3];
	ld.param.u64 	%rd13, [AdamFP16_param_4];
	ld.param.u16 	%rs1, [AdamFP16_param_5];
	ld.param.u16 	%rs4, [AdamFP16_param_6];
	ld.param.u16 	%rs5, [AdamFP16_param_7];
	ld.param.u16 	%rs2, [AdamFP16_param_8];
	ld.param.u16 	%rs37, [AdamFP16_param_9];
	ld.param.u16 	%rs38, [AdamFP16_param_10];
	ld.param.u16 	%rs3, [AdamFP16_param_11];
	ld.param.u32 	%r24, [AdamFP16_param_0];
	// inline asm
	{  mov.b32 %r25, {%rs1,%rs1};}

	// inline asm
	// inline asm
	{  mov.b32 %r26, {%rs2,%rs2};}

	// inline asm
	// inline asm
	{  mov.b32 %r27, {%rs3,%rs3};}

	// inline asm
	// inline asm
	{  mov.b32 %r28, {%rs4,%rs4};}

	// inline asm
	// inline asm
	{  mov.b32 %r29, {%rs5,%rs5};}

	// inline asm
	mov.f32 	%f37, 0f3F800000;
	// inline asm
	{  cvt.rn.f16.f32 %rs49, %f37;}

	// inline asm
	// inline asm
	{  mov.b32 %r30, {%rs49,%rs49};}

	// inline asm
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r31, %ctaid.x;
	mul.lo.s32 	%r8, %r7, %r31;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r92, %r8, %r9;
	shr.u32 	%r32, %r24, 31;
	add.s32 	%r33, %r24, %r32;
	shr.s32 	%r11, %r33, 1;
	setp.ge.s32	%p1, %r92, %r11;
	@%p1 bra 	BB40_15;

	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd3, %rd11;
	cvta.to.global.u64 	%rd4, %rd12;
	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r12, %r34, %r7;
	// inline asm
	{sub.f16x2 %r35,%r30,%r28;
}
	// inline asm
	// inline asm
	{  mov.b32 %r45, {%rs37,%rs37};}

	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r45;
 mov.b16 %rs55, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f39, %rs55;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f40, %f39;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r45;
 mov.b16 %rs64, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f47, %rs64;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f48, %f47;
}
	// inline asm
	// inline asm
	{sub.f16x2 %r51,%r30,%r29;
}
	// inline asm
	// inline asm
	{  mov.b32 %r64, {%rs38,%rs38};}

	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r64;
 mov.b16 %rs77, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f55, %rs77;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f56, %f55;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r64;
 mov.b16 %rs86, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f63, %rs86;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f64, %f63;
}
	// inline asm

BB40_2:
	cvt.s64.s32	%rd5, %r92;
	mul.wide.s32 	%rd14, %r92, 4;
	add.s64 	%rd15, %rd3, %rd14;
	add.s64 	%rd6, %rd2, %rd14;
	ld.global.u32 	%r43, [%rd6];
	ld.global.u32 	%r40, [%rd15];
	// inline asm
	{mul.f16x2 %r38,%r28,%r40;
}
	// inline asm
	// inline asm
	{fma.rn.f16x2 %r41,%r35,%r43,%r38;
}
	// inline asm
	st.global.u32 	[%rd15], %r41;
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r41;
 mov.b16 %rs54, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f38, %rs54;}

	// inline asm
	mul.f32 	%f42, %f38, %f40;
	// inline asm
	{  cvt.rn.f16.f32 %rs175, %f42;}

	// inline asm
	and.b16  	%rs60, %rs175, 32767;
	mov.u16 	%rs61, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs60, %rs61;
  selp.u16 %rs59, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs59, 0;
	setp.eq.s16	%p3, %rs60, 0;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB40_4;

	neg.f32 	%f44, %f39;
	fma.rn.f32 	%f45, %f44, %f42, %f38;
	fma.rn.f32 	%f43, %f40, %f45, %f42;
	// inline asm
	{  cvt.rn.f16.f32 %rs175, %f43;}

	// inline asm

BB40_4:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r41;
 mov.b16 %rs63, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f46, %rs63;}

	// inline asm
	mul.f32 	%f50, %f46, %f48;
	// inline asm
	{  cvt.rn.f16.f32 %rs176, %f50;}

	// inline asm
	and.b16  	%rs69, %rs176, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs69, %rs61;
  selp.u16 %rs68, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p5, %rs68, 0;
	setp.eq.s16	%p6, %rs69, 0;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	BB40_6;

	neg.f32 	%f52, %f47;
	fma.rn.f32 	%f53, %f52, %f50, %f46;
	fma.rn.f32 	%f51, %f48, %f53, %f50;
	// inline asm
	{  cvt.rn.f16.f32 %rs176, %f51;}

	// inline asm

BB40_6:
	// inline asm
	{  mov.b32 %r50, {%rs175,%rs176};}

	// inline asm
	add.s64 	%rd17, %rd4, %rd14;
	ld.global.u32 	%r62, [%rd17];
	ld.global.u32 	%r55, [%rd6];
	// inline asm
	{mul.f16x2 %r54,%r55,%r55;
}
	// inline asm
	// inline asm
	{mul.f16x2 %r57,%r51,%r54;
}
	// inline asm
	// inline asm
	{fma.rn.f16x2 %r60,%r29,%r62,%r57;
}
	// inline asm
	st.global.u32 	[%rd17], %r60;
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r60;
 mov.b16 %rs76, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f54, %rs76;}

	// inline asm
	mul.f32 	%f58, %f54, %f56;
	// inline asm
	{  cvt.rn.f16.f32 %rs177, %f58;}

	// inline asm
	and.b16  	%rs82, %rs177, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs82, %rs61;
  selp.u16 %rs81, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p8, %rs81, 0;
	setp.eq.s16	%p9, %rs82, 0;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	BB40_8;

	neg.f32 	%f60, %f55;
	fma.rn.f32 	%f61, %f60, %f58, %f54;
	fma.rn.f32 	%f59, %f56, %f61, %f58;
	// inline asm
	{  cvt.rn.f16.f32 %rs177, %f59;}

	// inline asm

BB40_8:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r60;
 mov.b16 %rs85, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f62, %rs85;}

	// inline asm
	mul.f32 	%f66, %f62, %f64;
	// inline asm
	{  cvt.rn.f16.f32 %rs178, %f66;}

	// inline asm
	and.b16  	%rs91, %rs178, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs91, %rs61;
  selp.u16 %rs90, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p11, %rs90, 0;
	setp.eq.s16	%p12, %rs91, 0;
	or.pred  	%p13, %p12, %p11;
	@%p13 bra 	BB40_10;

	neg.f32 	%f68, %f63;
	fma.rn.f32 	%f69, %f68, %f66, %f62;
	fma.rn.f32 	%f67, %f64, %f69, %f66;
	// inline asm
	{  cvt.rn.f16.f32 %rs178, %f67;}

	// inline asm

BB40_10:
	// inline asm
	{  mov.b32 %r69, {%rs177,%rs178};}

	// inline asm
	add.s64 	%rd7, %rd1, %rd14;
	ld.global.u32 	%r19, [%rd7];
	// inline asm
	{mul.f16x2 %r70,%r25,%r50;
}
	// inline asm
	// inline asm
	{.reg.b16         hl, hu;         
 .reg.b32         fl, fu;         
  mov.b32         {hl, hu}, %r69;   
  cvt.f32.f16     fl, hl;         
  cvt.f32.f16     fu, hu;         
  sqrt.approx.f32   fl, fl;     
  sqrt.approx.f32   fu, fu;     
  cvt.rn.f16.f32      hl, fl;     
  cvt.rn.f16.f32      hu, fu;     
  mov.b32         %r73, {hl, hu};   
}
	// inline asm
	// inline asm
	{add.f16x2 %r75,%r73,%r26;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r70;
 mov.b16 %rs96, low;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r75;
 mov.b16 %rs97, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f70, %rs96;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f71, %rs97;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f72, %f71;
}
	// inline asm
	mul.f32 	%f74, %f70, %f72;
	// inline asm
	{  cvt.rn.f16.f32 %rs179, %f74;}

	// inline asm
	and.b16  	%rs102, %rs179, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs102, %rs61;
  selp.u16 %rs101, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p14, %rs101, 0;
	setp.eq.s16	%p15, %rs102, 0;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	BB40_12;

	neg.f32 	%f76, %f71;
	fma.rn.f32 	%f77, %f76, %f74, %f70;
	fma.rn.f32 	%f75, %f72, %f77, %f74;
	// inline asm
	{  cvt.rn.f16.f32 %rs179, %f75;}

	// inline asm

BB40_12:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r70;
 mov.b16 %rs105, high;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r75;
 mov.b16 %rs106, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f78, %rs105;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f79, %rs106;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f80, %f79;
}
	// inline asm
	mul.f32 	%f82, %f78, %f80;
	// inline asm
	{  cvt.rn.f16.f32 %rs180, %f82;}

	// inline asm
	and.b16  	%rs111, %rs180, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs111, %rs61;
  selp.u16 %rs110, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p17, %rs110, 0;
	setp.eq.s16	%p18, %rs111, 0;
	or.pred  	%p19, %p18, %p17;
	@%p19 bra 	BB40_14;

	neg.f32 	%f84, %f79;
	fma.rn.f32 	%f85, %f84, %f82, %f78;
	fma.rn.f32 	%f83, %f80, %f85, %f82;
	// inline asm
	{  cvt.rn.f16.f32 %rs180, %f83;}

	// inline asm

BB40_14:
	// inline asm
	{  mov.b32 %r82, {%rs179,%rs180};}

	// inline asm
	// inline asm
	{sub.f16x2 %r83,%r19,%r82;
}
	// inline asm
	st.global.u32 	[%rd7], %r83;
	ld.global.u32 	%r88, [%rd6];
	// inline asm
	{mul.f16x2 %r86,%r27,%r88;
}
	// inline asm
	st.global.u32 	[%rd6], %r86;
	cvt.u32.u64	%r89, %rd5;
	add.s32 	%r92, %r12, %r89;
	setp.lt.s32	%p20, %r92, %r11;
	@%p20 bra 	BB40_2;

BB40_15:
	neg.s32 	%r90, %r9;
	setp.ne.s32	%p21, %r8, %r90;
	and.b32  	%r91, %r24, 1;
	setp.eq.b32	%p22, %r91, 1;
	not.pred 	%p23, %p22;
	or.pred  	%p24, %p21, %p23;
	@%p24 bra 	BB40_23;

	ld.param.u16 	%rs174, [AdamFP16_param_9];
	cvta.to.global.u64 	%rd19, %rd11;
	cvta.to.global.u64 	%rd20, %rd13;
	add.s32 	%r23, %r24, -1;
	mul.wide.s32 	%rd21, %r23, 2;
	add.s64 	%rd22, %rd19, %rd21;
	// inline asm
	{sub.f16 %rs116,%rs49,%rs4;
}
	// inline asm
	add.s64 	%rd8, %rd20, %rd21;
	ld.global.u16 	%rs124, [%rd8];
	ld.global.u16 	%rs121, [%rd22];
	// inline asm
	{mul.f16 %rs119,%rs4,%rs121;
}
	// inline asm
	// inline asm
	{fma.rn.f16 %rs122,%rs116,%rs124,%rs119;
}
	// inline asm
	st.global.u16 	[%rd22], %rs122;
	// inline asm
	{  cvt.f32.f16 %f86, %rs122;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f87, %rs174;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f88, %f87;
}
	// inline asm
	mul.f32 	%f90, %f86, %f88;
	// inline asm
	{  cvt.rn.f16.f32 %rs181, %f90;}

	// inline asm
	and.b16  	%rs130, %rs181, 32767;
	mov.u16 	%rs131, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs130, %rs131;
  selp.u16 %rs129, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p25, %rs129, 0;
	setp.eq.s16	%p26, %rs130, 0;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	BB40_18;

	neg.f32 	%f92, %f87;
	fma.rn.f32 	%f93, %f92, %f90, %f86;
	fma.rn.f32 	%f91, %f88, %f93, %f90;
	// inline asm
	{  cvt.rn.f16.f32 %rs181, %f91;}

	// inline asm

BB40_18:
	cvta.to.global.u64 	%rd23, %rd12;
	add.s64 	%rd25, %rd23, %rd21;
	ld.global.u16 	%rs144, [%rd25];
	// inline asm
	{sub.f16 %rs133,%rs49,%rs5;
}
	// inline asm
	ld.global.u16 	%rs137, [%rd8];
	// inline asm
	{mul.f16 %rs136,%rs137,%rs137;
}
	// inline asm
	// inline asm
	{mul.f16 %rs139,%rs133,%rs136;
}
	// inline asm
	// inline asm
	{fma.rn.f16 %rs142,%rs5,%rs144,%rs139;
}
	// inline asm
	st.global.u16 	[%rd25], %rs142;
	// inline asm
	{  cvt.f32.f16 %f94, %rs142;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f95, %rs38;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f96, %f95;
}
	// inline asm
	mul.f32 	%f98, %f94, %f96;
	// inline asm
	{  cvt.rn.f16.f32 %rs182, %f98;}

	// inline asm
	and.b16  	%rs150, %rs182, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs150, %rs131;
  selp.u16 %rs149, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p28, %rs149, 0;
	setp.eq.s16	%p29, %rs150, 0;
	or.pred  	%p30, %p29, %p28;
	@%p30 bra 	BB40_20;

	neg.f32 	%f100, %f95;
	fma.rn.f32 	%f101, %f100, %f98, %f94;
	fma.rn.f32 	%f99, %f96, %f101, %f98;
	// inline asm
	{  cvt.rn.f16.f32 %rs182, %f99;}

	// inline asm

BB40_20:
	cvta.to.global.u64 	%rd26, %rd10;
	add.s64 	%rd9, %rd26, %rd21;
	ld.global.u16 	%rs33, [%rd9];
	// inline asm
	{mul.f16 %rs153,%rs1,%rs181;
}
	// inline asm
	// inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs182;     
  cvt.f32.f16     f,r;      
  sqrt.approx.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs156,r;     
}
	// inline asm
	// inline asm
	{add.f16 %rs158,%rs156,%rs2;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f102, %rs153;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f103, %rs158;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f104, %f103;
}
	// inline asm
	mul.f32 	%f106, %f102, %f104;
	// inline asm
	{  cvt.rn.f16.f32 %rs183, %f106;}

	// inline asm
	and.b16  	%rs165, %rs183, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs165, %rs131;
  selp.u16 %rs164, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p31, %rs164, 0;
	setp.eq.s16	%p32, %rs165, 0;
	or.pred  	%p33, %p32, %p31;
	@%p33 bra 	BB40_22;

	neg.f32 	%f108, %f103;
	fma.rn.f32 	%f109, %f108, %f106, %f102;
	fma.rn.f32 	%f107, %f104, %f109, %f106;
	// inline asm
	{  cvt.rn.f16.f32 %rs183, %f107;}

	// inline asm

BB40_22:
	// inline asm
	{sub.f16 %rs168,%rs33,%rs183;
}
	// inline asm
	st.global.u16 	[%rd9], %rs168;
	ld.global.u16 	%rs173, [%rd8];
	// inline asm
	{mul.f16 %rs171,%rs3,%rs173;
}
	// inline asm
	st.global.u16 	[%rd8], %rs171;

BB40_23:
	ret;
}

	// .globl	AdaDeltaFP16
.visible .entry AdaDeltaFP16(
	.param .u32 AdaDeltaFP16_param_0,
	.param .u64 AdaDeltaFP16_param_1,
	.param .u64 AdaDeltaFP16_param_2,
	.param .u64 AdaDeltaFP16_param_3,
	.param .u64 AdaDeltaFP16_param_4,
	.param .align 2 .b8 AdaDeltaFP16_param_5[2],
	.param .align 2 .b8 AdaDeltaFP16_param_6[2],
	.param .align 2 .b8 AdaDeltaFP16_param_7[2],
	.param .align 2 .b8 AdaDeltaFP16_param_8[2]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<97>;
	.reg .f32 	%f<38>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd10, [AdaDeltaFP16_param_1];
	ld.param.u64 	%rd11, [AdaDeltaFP16_param_2];
	ld.param.u64 	%rd12, [AdaDeltaFP16_param_3];
	ld.param.u64 	%rd13, [AdaDeltaFP16_param_4];
	ld.param.u16 	%rs1, [AdaDeltaFP16_param_6];
	ld.param.u16 	%rs2, [AdaDeltaFP16_param_7];
	ld.param.u16 	%rs4, [AdaDeltaFP16_param_8];
	ld.param.u32 	%r17, [AdaDeltaFP16_param_0];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mul.lo.s32 	%r2, %r1, %r23;
	// inline asm
	{  mov.b32 %r19, {%rs1,%rs1};}

	// inline asm
	// inline asm
	{  mov.b32 %r20, {%rs2,%rs2};}

	// inline asm
	mov.f32 	%f13, 0f3F800000;
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f13;}

	// inline asm
	// inline asm
	{  mov.b32 %r21, {%rs22,%rs22};}

	// inline asm
	// inline asm
	{  mov.b32 %r22, {%rs4,%rs4};}

	// inline asm
	mov.u32 	%r7, %tid.x;
	add.s32 	%r78, %r2, %r7;
	shr.u32 	%r24, %r17, 31;
	add.s32 	%r25, %r17, %r24;
	shr.s32 	%r9, %r25, 1;
	setp.ge.s32	%p1, %r78, %r9;
	@%p1 bra 	BB41_7;

	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd3, %rd11;
	cvta.to.global.u64 	%rd4, %rd12;
	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r10, %r26, %r1;
	// inline asm
	{sub.f16x2 %r27,%r21,%r20;
}
	// inline asm

BB41_2:
	cvt.s64.s32	%rd5, %r78;
	mul.wide.s32 	%rd14, %r78, 4;
	add.s64 	%rd15, %rd3, %rd14;
	add.s64 	%rd6, %rd2, %rd14;
	ld.global.u32 	%r31, [%rd6];
	// inline asm
	{mul.f16x2 %r30,%r31,%r31;
}
	// inline asm
	ld.global.u32 	%r35, [%rd15];
	// inline asm
	{mul.f16x2 %r33,%r20,%r35;
}
	// inline asm
	// inline asm
	{fma.rn.f16x2 %r36,%r27,%r30,%r33;
}
	// inline asm
	st.global.u32 	[%rd15], %r36;
	add.s64 	%rd7, %rd4, %rd14;
	ld.global.u32 	%r12, [%rd7];
	// inline asm
	{add.f16x2 %r40,%r12,%r19;
}
	// inline asm
	// inline asm
	{add.f16x2 %r43,%r36,%r19;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r40;
 mov.b16 %rs27, low;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r43;
 mov.b16 %rs28, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f14, %rs27;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f15, %rs28;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f16, %f15;
}
	// inline asm
	mul.f32 	%f18, %f14, %f16;
	// inline asm
	{  cvt.rn.f16.f32 %rs94, %f18;}

	// inline asm
	and.b16  	%rs33, %rs94, 32767;
	mov.u16 	%rs34, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs33, %rs34;
  selp.u16 %rs32, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs32, 0;
	setp.eq.s16	%p3, %rs33, 0;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB41_4;

	neg.f32 	%f20, %f15;
	fma.rn.f32 	%f21, %f20, %f18, %f14;
	fma.rn.f32 	%f19, %f16, %f21, %f18;
	// inline asm
	{  cvt.rn.f16.f32 %rs94, %f19;}

	// inline asm

BB41_4:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r40;
 mov.b16 %rs36, high;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r43;
 mov.b16 %rs37, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f22, %rs36;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f23, %rs37;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f24, %f23;
}
	// inline asm
	mul.f32 	%f26, %f22, %f24;
	// inline asm
	{  cvt.rn.f16.f32 %rs95, %f26;}

	// inline asm
	and.b16  	%rs42, %rs95, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs42, %rs34;
  selp.u16 %rs41, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p5, %rs41, 0;
	setp.eq.s16	%p6, %rs42, 0;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	BB41_6;

	neg.f32 	%f28, %f23;
	fma.rn.f32 	%f29, %f28, %f26, %f22;
	fma.rn.f32 	%f27, %f24, %f29, %f26;
	// inline asm
	{  cvt.rn.f16.f32 %rs95, %f27;}

	// inline asm

BB41_6:
	// inline asm
	{  mov.b32 %r50, {%rs94,%rs95};}

	// inline asm
	// inline asm
	{.reg.b16         hl, hu;         
 .reg.b32         fl, fu;         
  mov.b32         {hl, hu}, %r50;   
  cvt.f32.f16     fl, hl;         
  cvt.f32.f16     fu, hu;         
  sqrt.approx.f32   fl, fl;     
  sqrt.approx.f32   fu, fu;     
  cvt.rn.f16.f32      hl, fl;     
  cvt.rn.f16.f32      hu, fu;     
  mov.b32         %r51, {hl, hu};   
}
	// inline asm
	ld.global.u32 	%r55, [%rd6];
	// inline asm
	{mul.f16x2 %r53,%r51,%r55;
}
	// inline asm
	// inline asm
	{mul.f16x2 %r59,%r53,%r53;
}
	// inline asm
	// inline asm
	{mul.f16x2 %r62,%r20,%r12;
}
	// inline asm
	// inline asm
	{fma.rn.f16x2 %r65,%r27,%r59,%r62;
}
	// inline asm
	st.global.u32 	[%rd7], %r65;
	add.s64 	%rd17, %rd1, %rd14;
	ld.global.u32 	%r70, [%rd17];
	// inline asm
	{sub.f16x2 %r69,%r70,%r53;
}
	// inline asm
	st.global.u32 	[%rd17], %r69;
	ld.global.u32 	%r73, [%rd6];
	// inline asm
	{mul.f16x2 %r72,%r73,%r22;
}
	// inline asm
	st.global.u32 	[%rd6], %r72;
	cvt.u32.u64	%r75, %rd5;
	add.s32 	%r78, %r10, %r75;
	setp.lt.s32	%p8, %r78, %r9;
	@%p8 bra 	BB41_2;

BB41_7:
	neg.s32 	%r76, %r7;
	setp.ne.s32	%p9, %r2, %r76;
	and.b32  	%r77, %r17, 1;
	setp.eq.b32	%p10, %r77, 1;
	not.pred 	%p11, %p10;
	or.pred  	%p12, %p9, %p11;
	@%p12 bra 	BB41_11;

	cvta.to.global.u64 	%rd18, %rd12;
	cvta.to.global.u64 	%rd19, %rd11;
	cvta.to.global.u64 	%rd20, %rd13;
	add.s32 	%r16, %r17, -1;
	mul.wide.s32 	%rd21, %r16, 2;
	add.s64 	%rd22, %rd19, %rd21;
	// inline asm
	{sub.f16 %rs47,%rs22,%rs2;
}
	// inline asm
	add.s64 	%rd8, %rd20, %rd21;
	ld.global.u16 	%rs51, [%rd8];
	// inline asm
	{mul.f16 %rs50,%rs51,%rs51;
}
	// inline asm
	ld.global.u16 	%rs55, [%rd22];
	// inline asm
	{mul.f16 %rs53,%rs2,%rs55;
}
	// inline asm
	// inline asm
	{fma.rn.f16 %rs56,%rs47,%rs50,%rs53;
}
	// inline asm
	st.global.u16 	[%rd22], %rs56;
	add.s64 	%rd9, %rd18, %rd21;
	ld.global.u16 	%rs12, [%rd9];
	// inline asm
	{add.f16 %rs60,%rs12,%rs1;
}
	// inline asm
	// inline asm
	{add.f16 %rs63,%rs56,%rs1;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f30, %rs60;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f31, %rs63;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f32, %f31;
}
	// inline asm
	mul.f32 	%f34, %f30, %f32;
	// inline asm
	{  cvt.rn.f16.f32 %rs96, %f34;}

	// inline asm
	and.b16  	%rs70, %rs96, 32767;
	mov.u16 	%rs71, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs70, %rs71;
  selp.u16 %rs69, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p13, %rs69, 0;
	setp.eq.s16	%p14, %rs70, 0;
	or.pred  	%p15, %p14, %p13;
	@%p15 bra 	BB41_10;

	neg.f32 	%f36, %f31;
	fma.rn.f32 	%f37, %f36, %f34, %f30;
	fma.rn.f32 	%f35, %f32, %f37, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs96, %f35;}

	// inline asm

BB41_10:
	cvta.to.global.u64 	%rd23, %rd10;
	// inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs96;     
  cvt.f32.f16     f,r;      
  sqrt.approx.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs73,r;     
}
	// inline asm
	ld.global.u16 	%rs77, [%rd8];
	// inline asm
	{mul.f16 %rs75,%rs73,%rs77;
}
	// inline asm
	// inline asm
	{mul.f16 %rs78,%rs75,%rs75;
}
	// inline asm
	// inline asm
	{mul.f16 %rs81,%rs2,%rs12;
}
	// inline asm
	// inline asm
	{fma.rn.f16 %rs84,%rs47,%rs78,%rs81;
}
	// inline asm
	st.global.u16 	[%rd9], %rs84;
	add.s64 	%rd25, %rd23, %rd21;
	ld.global.u16 	%rs89, [%rd25];
	// inline asm
	{sub.f16 %rs88,%rs89,%rs75;
}
	// inline asm
	st.global.u16 	[%rd25], %rs88;
	ld.global.u16 	%rs92, [%rd8];
	// inline asm
	{mul.f16 %rs91,%rs92,%rs4;
}
	// inline asm
	st.global.u16 	[%rd8], %rs91;

BB41_11:
	ret;
}

	// .globl	L1L2FP16
.visible .entry L1L2FP16(
	.param .u32 L1L2FP16_param_0,
	.param .u64 L1L2FP16_param_1,
	.param .u64 L1L2FP16_param_2,
	.param .u64 L1L2FP16_param_3,
	.param .u64 L1L2FP16_param_4,
	.param .align 2 .b8 L1L2FP16_param_5[2],
	.param .align 2 .b8 L1L2FP16_param_6[2],
	.param .align 2 .b8 L1L2FP16_param_7[2]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<84>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<18>;
	// demoted variable
	.shared .align 8 .u64 _ZZ8L1L2FP16E6l1l2h2;

	ld.param.u32 	%r6, [L1L2FP16_param_0];
	ld.param.u64 	%rd8, [L1L2FP16_param_1];
	ld.param.u64 	%rd9, [L1L2FP16_param_2];
	ld.param.u64 	%rd10, [L1L2FP16_param_3];
	ld.param.u64 	%rd11, [L1L2FP16_param_4];
	ld.param.u16 	%rs19, [L1L2FP16_param_5];
	ld.param.u16 	%rs20, [L1L2FP16_param_6];
	ld.param.u16 	%rs21, [L1L2FP16_param_7];
	mov.f32 	%f9, 0f3F800000;
	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f9;}

	// inline asm
	mov.f32 	%f10, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f10;}

	// inline asm
	ld.shared.u64 	%rd1, [_ZZ8L1L2FP16E6l1l2h2];
	add.s64 	%rd2, %rd1, 4;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r19, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r19, %r6;
	@%p1 bra 	BB42_11;

	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd9;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	// inline asm
	{  cvt.f32.f16 %f23, %rs19;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f24, %f23;
}
	// inline asm

BB42_2:
	cvt.s64.s32	%rd5, %r19;
	mul.wide.s32 	%rd12, %r19, 2;
	add.s64 	%rd6, %rd4, %rd12;
	ld.global.u16 	%rs80, [%rd6];
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs80, %rs23;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs24, 0;
	@%p2 bra 	BB42_4;

	// inline asm
	{  cvt.rn.f16.f32 %rs27, %f10;}

	// inline asm
	// inline asm
	{sub.f16 %rs80,%rs27,%rs80;
}
	// inline asm

BB42_4:
	// inline asm
	{mul.f16 %rs31,%rs80,%rs20;
}
	// inline asm
	// inline asm
	{  mov.b32 %r10, {%rs31,%rs23};}

	// inline asm
	// inline asm
	{ atom.add.noftz.f16x2 %r11,[%rd1],%r10; }

	// inline asm
	ld.global.u16 	%rs37, [%rd6];
	// inline asm
	{mul.f16 %rs36,%rs37,%rs37;
}
	// inline asm
	// inline asm
	{mul.f16 %rs39,%rs36,%rs21;
}
	// inline asm
	mov.f32 	%f12, 0f40000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs42, %f12;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f13, %rs39;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f14, %rs42;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f15, %f14;
}
	// inline asm
	mul.f32 	%f17, %f13, %f15;
	// inline asm
	{  cvt.rn.f16.f32 %rs81, %f17;}

	// inline asm
	and.b16  	%rs47, %rs81, 32767;
	mov.u16 	%rs48, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs47, %rs48;
  selp.u16 %rs46, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs46, 0;
	setp.eq.s16	%p4, %rs47, 0;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB42_6;

	neg.f32 	%f19, %f14;
	fma.rn.f32 	%f20, %f19, %f17, %f13;
	fma.rn.f32 	%f18, %f15, %f20, %f17;
	// inline asm
	{  cvt.rn.f16.f32 %rs81, %f18;}

	// inline asm

BB42_6:
	// inline asm
	{  mov.b32 %r13, {%rs81,%rs23};}

	// inline asm
	// inline asm
	{ atom.add.noftz.f16x2 %r14,[%rd2],%r13; }

	// inline asm
	ld.global.u16 	%rs13, [%rd6];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs13, %rs23;
  selp.u16 %rs52, 1, 0, __$temp3;}
	// inline asm
	setp.ne.s16	%p6, %rs52, 0;
	mov.u16 	%rs82, %rs22;
	@%p6 bra 	BB42_8;

	// inline asm
	{  cvt.rn.f16.f32 %rs55, %f10;}

	// inline asm
	// inline asm
	{sub.f16 %rs82,%rs55,%rs22;
}
	// inline asm

BB42_8:
	// inline asm
	{mul.f16 %rs59,%rs20,%rs82;
}
	// inline asm
	// inline asm
	{mul.f16 %rs62,%rs13,%rs21;
}
	// inline asm
	add.s64 	%rd7, %rd3, %rd12;
	ld.global.u16 	%rs66, [%rd7];
	// inline asm
	{add.f16 %rs65,%rs66,%rs62;
}
	// inline asm
	// inline asm
	{add.f16 %rs68,%rs65,%rs59;
}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f22, %rs68;}

	// inline asm
	mul.f32 	%f26, %f22, %f24;
	// inline asm
	{  cvt.rn.f16.f32 %rs83, %f26;}

	// inline asm
	and.b16  	%rs75, %rs83, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs75, %rs48;
  selp.u16 %rs74, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p7, %rs74, 0;
	setp.eq.s16	%p8, %rs75, 0;
	or.pred  	%p9, %p8, %p7;
	@%p9 bra 	BB42_10;

	neg.f32 	%f28, %f23;
	fma.rn.f32 	%f29, %f28, %f26, %f22;
	fma.rn.f32 	%f27, %f24, %f29, %f26;
	// inline asm
	{  cvt.rn.f16.f32 %rs83, %f27;}

	// inline asm

BB42_10:
	st.global.u16 	[%rd7], %rs83;
	cvt.u32.u64	%r16, %rd5;
	add.s32 	%r19, %r3, %r16;
	setp.lt.s32	%p10, %r19, %r6;
	@%p10 bra 	BB42_2;

BB42_11:
	ld.u32 	%r17, [%rd1];
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r17;
 mov.b16 %rs78, low;}
	// inline asm
	cvta.to.global.u64 	%rd16, %rd10;
	st.global.u16 	[%rd16], %rs78;
	ld.u32 	%r18, [%rd1+4];
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r18;
 mov.b16 %rs79, low;}
	// inline asm
	cvta.to.global.u64 	%rd17, %rd11;
	st.global.u16 	[%rd17], %rs79;
	ret;
}

	// .globl	ThreshForwardFP16
.visible .entry ThreshForwardFP16(
	.param .u32 ThreshForwardFP16_param_0,
	.param .u32 ThreshForwardFP16_param_1,
	.param .u64 ThreshForwardFP16_param_2,
	.param .u64 ThreshForwardFP16_param_3,
	.param .u64 ThreshForwardFP16_param_4,
	.param .u64 ThreshForwardFP16_param_5,
	.param .u64 ThreshForwardFP16_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<21>;


	ld.param.u32 	%r8, [ThreshForwardFP16_param_0];
	ld.param.u32 	%r9, [ThreshForwardFP16_param_1];
	ld.param.u64 	%rd5, [ThreshForwardFP16_param_2];
	ld.param.u64 	%rd6, [ThreshForwardFP16_param_3];
	ld.param.u64 	%rd7, [ThreshForwardFP16_param_4];
	ld.param.u64 	%rd8, [ThreshForwardFP16_param_5];
	ld.param.u64 	%rd9, [ThreshForwardFP16_param_6];
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB43_8;

	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd5;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r18, 0;
	cvta.to.global.u64 	%rd12, %rd8;
	cvta.to.global.u64 	%rd15, %rd6;
	cvta.to.global.u64 	%rd18, %rd7;

BB43_2:
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r19, %r11, %r13, %r15;
	mul.lo.s32 	%r4, %r18, %r8;
	setp.ge.s32	%p2, %r19, %r8;
	@%p2 bra 	BB43_7;

BB43_3:
	add.s32 	%r16, %r19, %r4;
	mul.wide.s32 	%rd10, %r16, 2;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.u16 	%rs1, [%rd11];
	cvt.s64.s32	%rd3, %r19;
	mul.wide.s32 	%rd13, %r19, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u16 	%rs7, [%rd14];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs1, %rs7;
  selp.u16 %rs5, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs5, 0;
	add.s64 	%rd4, %rd15, %rd10;
	@%p3 bra 	BB43_5;
	bra.uni 	BB43_4;

BB43_5:
	add.s64 	%rd20, %rd18, %rd13;
	ld.global.u16 	%rs12, [%rd20];
	// inline asm
	{mul.f16 %rs14,%rs12,%rs1;
}
	// inline asm
	bra.uni 	BB43_6;

BB43_4:
	add.s64 	%rd17, %rd1, %rd13;
	ld.global.u16 	%rs10, [%rd17];
	// inline asm
	{mul.f16 %rs14,%rs1,%rs10;
}
	// inline asm

BB43_6:
	st.global.u16 	[%rd4], %rs14;
	cvt.u32.u64	%r17, %rd3;
	add.s32 	%r19, %r1, %r17;
	setp.lt.s32	%p4, %r19, %r8;
	@%p4 bra 	BB43_3;

BB43_7:
	add.s32 	%r18, %r18, 1;
	setp.lt.s32	%p5, %r18, %r9;
	@%p5 bra 	BB43_2;

BB43_8:
	ret;
}

	// .globl	ThreshBackwardFP16
.visible .entry ThreshBackwardFP16(
	.param .u32 ThreshBackwardFP16_param_0,
	.param .u32 ThreshBackwardFP16_param_1,
	.param .u64 ThreshBackwardFP16_param_2,
	.param .u64 ThreshBackwardFP16_param_3,
	.param .u64 ThreshBackwardFP16_param_4,
	.param .u64 ThreshBackwardFP16_param_5,
	.param .u64 ThreshBackwardFP16_param_6,
	.param .u64 ThreshBackwardFP16_param_7,
	.param .u64 ThreshBackwardFP16_param_8,
	.param .u64 ThreshBackwardFP16_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<37>;


	ld.param.u32 	%r6, [ThreshBackwardFP16_param_0];
	ld.param.u32 	%r7, [ThreshBackwardFP16_param_1];
	ld.param.u64 	%rd2, [ThreshBackwardFP16_param_2];
	ld.param.u64 	%rd3, [ThreshBackwardFP16_param_3];
	ld.param.u64 	%rd4, [ThreshBackwardFP16_param_4];
	ld.param.u64 	%rd5, [ThreshBackwardFP16_param_5];
	ld.param.u64 	%rd6, [ThreshBackwardFP16_param_6];
	ld.param.u64 	%rd7, [ThreshBackwardFP16_param_7];
	ld.param.u64 	%rd8, [ThreshBackwardFP16_param_8];
	ld.param.u64 	%rd9, [ThreshBackwardFP16_param_9];
	setp.lt.s32	%p1, %r7, 1;
	@%p1 bra 	BB44_8;

	mov.u32 	%r17, 0;
	cvta.to.global.u64 	%rd10, %rd2;
	cvta.to.global.u64 	%rd12, %rd7;
	cvta.to.global.u64 	%rd27, %rd5;
	cvta.to.global.u64 	%rd33, %rd6;
	cvta.to.global.u64 	%rd17, %rd8;
	cvta.to.global.u64 	%rd23, %rd9;

BB44_2:
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r18, %r10, %r9, %r11;
	setp.ge.s32	%p2, %r18, %r6;
	@%p2 bra 	BB44_7;

BB44_3:
	mad.lo.s32 	%r12, %r17, %r6, %r18;
	mul.wide.s32 	%rd11, %r12, 2;
	add.s64 	%rd1, %rd10, %rd11;
	ld.global.u16 	%rs3, [%rd1];
	mul.wide.s32 	%rd13, %r18, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u16 	%rs4, [%rd14];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs3, %rs4;
  selp.u16 %rs2, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs2, 0;
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd11;
	ld.global.u16 	%rs1, [%rd16];
	@%p3 bra 	BB44_5;
	bra.uni 	BB44_4;

BB44_5:
	add.s64 	%rd29, %rd27, %rd13;
	ld.global.u16 	%rs14, [%rd29];
	// inline asm
	{mul.f16 %rs12,%rs1,%rs14;
}
	// inline asm
	cvta.to.global.u64 	%rd30, %rd3;
	add.s64 	%rd32, %rd30, %rd11;
	st.global.u16 	[%rd32], %rs12;
	add.s64 	%rd34, %rd33, %rd13;
	add.s64 	%rd36, %rd15, %rd13;
	ld.global.u16 	%rs16, [%rd36];
	ld.global.u16 	%rs17, [%rd1];
	ld.global.u16 	%rs18, [%rd34];
	// inline asm
	{fma.rn.f16 %rs15,%rs16,%rs17,%rs18;
}
	// inline asm
	st.global.u16 	[%rd34], %rs15;
	bra.uni 	BB44_6;

BB44_4:
	add.s64 	%rd19, %rd17, %rd13;
	ld.global.u16 	%rs7, [%rd19];
	// inline asm
	{mul.f16 %rs5,%rs1,%rs7;
}
	// inline asm
	cvta.to.global.u64 	%rd20, %rd3;
	add.s64 	%rd22, %rd20, %rd11;
	st.global.u16 	[%rd22], %rs5;
	add.s64 	%rd24, %rd23, %rd13;
	add.s64 	%rd26, %rd15, %rd13;
	ld.global.u16 	%rs9, [%rd26];
	ld.global.u16 	%rs10, [%rd1];
	ld.global.u16 	%rs11, [%rd24];
	// inline asm
	{fma.rn.f16 %rs8,%rs9,%rs10,%rs11;
}
	// inline asm
	st.global.u16 	[%rd24], %rs8;

BB44_6:
	mov.u32 	%r16, %nctaid.x;
	mad.lo.s32 	%r18, %r16, %r10, %r18;
	setp.lt.s32	%p4, %r18, %r6;
	@%p4 bra 	BB44_3;

BB44_7:
	add.s32 	%r17, %r17, 1;
	setp.lt.s32	%p5, %r17, %r7;
	@%p5 bra 	BB44_2;

BB44_8:
	ret;
}

	// .globl	PreluForwardFP16
.visible .entry PreluForwardFP16(
	.param .u32 PreluForwardFP16_param_0,
	.param .u32 PreluForwardFP16_param_1,
	.param .u64 PreluForwardFP16_param_2,
	.param .u64 PreluForwardFP16_param_3,
	.param .u64 PreluForwardFP16_param_4
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<10>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r8, [PreluForwardFP16_param_0];
	ld.param.u32 	%r9, [PreluForwardFP16_param_1];
	ld.param.u64 	%rd4, [PreluForwardFP16_param_2];
	ld.param.u64 	%rd5, [PreluForwardFP16_param_3];
	ld.param.u64 	%rd6, [PreluForwardFP16_param_4];
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB45_8;

	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r18, 0;
	cvta.to.global.u64 	%rd8, %rd5;
	cvta.to.global.u64 	%rd9, %rd6;

BB45_2:
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r19, %r11, %r13, %r15;
	mul.lo.s32 	%r4, %r18, %r8;
	setp.ge.s32	%p2, %r19, %r8;
	@%p2 bra 	BB45_7;

BB45_3:
	add.s32 	%r17, %r19, %r4;
	mul.wide.s32 	%rd7, %r17, 2;
	add.s64 	%rd2, %rd1, %rd7;
	ld.global.u16 	%rs1, [%rd2];
	mov.u32 	%r16, 0;
	// inline asm
	cvt.rn.f16.s32 %rs2, %r16;
	// inline asm
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs1, %rs2;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs3, 0;
	add.s64 	%rd3, %rd8, %rd7;
	@%p3 bra 	BB45_5;
	bra.uni 	BB45_4;

BB45_5:
	mul.wide.s32 	%rd10, %r19, 2;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u16 	%rs8, [%rd11];
	// inline asm
	{mul.f16 %rs7,%rs8,%rs1;
}
	// inline asm
	st.global.u16 	[%rd3], %rs7;
	bra.uni 	BB45_6;

BB45_4:
	ld.global.u16 	%rs6, [%rd2];
	st.global.u16 	[%rd3], %rs6;

BB45_6:
	add.s32 	%r19, %r1, %r19;
	setp.lt.s32	%p4, %r19, %r8;
	@%p4 bra 	BB45_3;

BB45_7:
	add.s32 	%r18, %r18, 1;
	setp.lt.s32	%p5, %r18, %r9;
	@%p5 bra 	BB45_2;

BB45_8:
	ret;
}

	// .globl	PreluBackwardFP16
.visible .entry PreluBackwardFP16(
	.param .u32 PreluBackwardFP16_param_0,
	.param .u32 PreluBackwardFP16_param_1,
	.param .u64 PreluBackwardFP16_param_2,
	.param .u64 PreluBackwardFP16_param_3,
	.param .u64 PreluBackwardFP16_param_4,
	.param .u64 PreluBackwardFP16_param_5,
	.param .u64 PreluBackwardFP16_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<14>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<24>;


	ld.param.u32 	%r8, [PreluBackwardFP16_param_0];
	ld.param.u32 	%r9, [PreluBackwardFP16_param_1];
	ld.param.u64 	%rd3, [PreluBackwardFP16_param_2];
	ld.param.u64 	%rd4, [PreluBackwardFP16_param_3];
	ld.param.u64 	%rd5, [PreluBackwardFP16_param_4];
	ld.param.u64 	%rd6, [PreluBackwardFP16_param_5];
	ld.param.u64 	%rd7, [PreluBackwardFP16_param_6];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	setp.lt.s32	%p1, %r9, 1;
	@%p1 bra 	BB46_8;

	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r1, %r12, %r11;
	mov.u32 	%r19, 0;
	cvta.to.global.u64 	%rd8, %rd4;
	cvta.to.global.u64 	%rd14, %rd6;
	cvta.to.global.u64 	%rd20, %rd7;

BB46_2:
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r20, %r11, %r13, %r15;
	setp.ge.s32	%p2, %r20, %r8;
	@%p2 bra 	BB46_7;

BB46_3:
	mul.lo.s32 	%r5, %r19, %r8;
	add.s32 	%r16, %r20, %r5;
	mul.wide.s32 	%rd9, %r16, 2;
	add.s64 	%rd1, %rd8, %rd9;
	ld.global.u16 	%rs4, [%rd1];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs4, %rs2;
  selp.u16 %rs3, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs3, 0;
	cvta.to.global.u64 	%rd10, %rd5;
	add.s64 	%rd2, %rd10, %rd9;
	@%p3 bra 	BB46_5;
	bra.uni 	BB46_4;

BB46_5:
	mul.wide.s32 	%rd15, %r20, 2;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u16 	%rs8, [%rd16];
	ld.global.u16 	%rs9, [%rd2];
	// inline asm
	{mul.f16 %rs7,%rs8,%rs9;
}
	// inline asm
	mad.lo.s32 	%r18, %r19, %r8, %r20;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.s32 	%rd18, %r18, 2;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u16 	[%rd19], %rs7;
	add.s64 	%rd21, %rd20, %rd15;
	add.s64 	%rd23, %rd10, %rd15;
	ld.global.u16 	%rs11, [%rd23];
	ld.global.u16 	%rs12, [%rd1];
	ld.global.u16 	%rs13, [%rd21];
	// inline asm
	{fma.rn.f16 %rs10,%rs11,%rs12,%rs13;
}
	// inline asm
	st.global.u16 	[%rd21], %rs10;
	bra.uni 	BB46_6;

BB46_4:
	ld.global.u16 	%rs6, [%rd2];
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd13, %rd11, %rd9;
	st.global.u16 	[%rd13], %rs6;

BB46_6:
	add.s32 	%r20, %r1, %r20;
	setp.lt.s32	%p4, %r20, %r8;
	@%p4 bra 	BB46_3;

BB46_7:
	add.s32 	%r19, %r19, 1;
	setp.lt.s32	%p5, %r19, %r9;
	@%p5 bra 	BB46_2;

BB46_8:
	ret;
}

	// .globl	LeakyForwardAlphaBetaFP16
.visible .entry LeakyForwardAlphaBetaFP16(
	.param .u32 LeakyForwardAlphaBetaFP16_param_0,
	.param .u64 LeakyForwardAlphaBetaFP16_param_1,
	.param .u64 LeakyForwardAlphaBetaFP16_param_2,
	.param .align 2 .b8 LeakyForwardAlphaBetaFP16_param_3[2],
	.param .align 2 .b8 LeakyForwardAlphaBetaFP16_param_4[2],
	.param .align 2 .b8 LeakyForwardAlphaBetaFP16_param_5[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<36>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r4, [LeakyForwardAlphaBetaFP16_param_0];
	ld.param.u64 	%rd2, [LeakyForwardAlphaBetaFP16_param_1];
	ld.param.u64 	%rd3, [LeakyForwardAlphaBetaFP16_param_2];
	ld.param.u16 	%rs10, [LeakyForwardAlphaBetaFP16_param_3];
	ld.param.u16 	%rs11, [LeakyForwardAlphaBetaFP16_param_4];
	ld.param.u16 	%rs12, [LeakyForwardAlphaBetaFP16_param_5];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs13, %f1;}

	// inline asm
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r10, %r5, %r6, %r7;
	setp.ge.s32	%p1, %r10, %r4;
	@%p1 bra 	BB47_6;

	cvta.to.global.u64 	%rd4, %rd2;
	cvta.to.global.u64 	%rd7, %rd3;

BB47_2:
	mul.wide.s32 	%rd5, %r10, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs5, [%rd6];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs5, %rs13;
  selp.u16 %rs14, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs14, 0;
	add.s64 	%rd1, %rd7, %rd5;
	ld.global.u16 	%rs19, [%rd1];
	// inline asm
	{mul.f16 %rs17,%rs12,%rs19;
}
	// inline asm
	@%p2 bra 	BB47_4;
	bra.uni 	BB47_3;

BB47_4:
	// inline asm
	{mul.f16 %rs26,%rs5,%rs10;
}
	// inline asm
	// inline asm
	{mul.f16 %rs29,%rs11,%rs26;
}
	// inline asm
	// inline asm
	{add.f16 %rs35,%rs17,%rs29;
}
	// inline asm
	bra.uni 	BB47_5;

BB47_3:
	// inline asm
	{mul.f16 %rs20,%rs11,%rs5;
}
	// inline asm
	// inline asm
	{add.f16 %rs35,%rs17,%rs20;
}
	// inline asm

BB47_5:
	st.global.u16 	[%rd1], %rs35;
	bar.sync 	0;
	mov.u32 	%r9, %nctaid.x;
	mad.lo.s32 	%r10, %r9, %r5, %r10;
	setp.lt.s32	%p3, %r10, %r4;
	@%p3 bra 	BB47_2;

BB47_6:
	ret;
}

	// .globl	LeakyBackwardAlphaBetaFP16
.visible .entry LeakyBackwardAlphaBetaFP16(
	.param .u32 LeakyBackwardAlphaBetaFP16_param_0,
	.param .u64 LeakyBackwardAlphaBetaFP16_param_1,
	.param .u64 LeakyBackwardAlphaBetaFP16_param_2,
	.param .u64 LeakyBackwardAlphaBetaFP16_param_3,
	.param .align 2 .b8 LeakyBackwardAlphaBetaFP16_param_4[2],
	.param .align 2 .b8 LeakyBackwardAlphaBetaFP16_param_5[2],
	.param .align 2 .b8 LeakyBackwardAlphaBetaFP16_param_6[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<37>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [LeakyBackwardAlphaBetaFP16_param_0];
	ld.param.u64 	%rd3, [LeakyBackwardAlphaBetaFP16_param_1];
	ld.param.u64 	%rd4, [LeakyBackwardAlphaBetaFP16_param_2];
	ld.param.u64 	%rd5, [LeakyBackwardAlphaBetaFP16_param_3];
	ld.param.u16 	%rs8, [LeakyBackwardAlphaBetaFP16_param_4];
	ld.param.u16 	%rs9, [LeakyBackwardAlphaBetaFP16_param_5];
	ld.param.u16 	%rs10, [LeakyBackwardAlphaBetaFP16_param_6];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs11, %f1;}

	// inline asm
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB48_6;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd6, %rd3;
	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd10, %rd5;

BB48_2:
	mul.wide.s32 	%rd7, %r10, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u16 	%rs13, [%rd8];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs13, %rs11;
  selp.u16 %rs12, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs12, 0;
	add.s64 	%rd2, %rd9, %rd7;
	add.s64 	%rd1, %rd10, %rd7;
	@%p2 bra 	BB48_4;
	bra.uni 	BB48_3;

BB48_4:
	ld.global.u16 	%rs26, [%rd2];
	// inline asm
	{mul.f16 %rs24,%rs10,%rs26;
}
	// inline asm
	ld.global.u16 	%rs28, [%rd1];
	// inline asm
	{mul.f16 %rs27,%rs28,%rs8;
}
	// inline asm
	// inline asm
	{mul.f16 %rs30,%rs9,%rs27;
}
	// inline asm
	// inline asm
	{add.f16 %rs36,%rs24,%rs30;
}
	// inline asm
	bra.uni 	BB48_5;

BB48_3:
	ld.global.u16 	%rs17, [%rd1];
	// inline asm
	{mul.f16 %rs15,%rs10,%rs17;
}
	// inline asm
	ld.global.u16 	%rs20, [%rd2];
	// inline asm
	{mul.f16 %rs18,%rs9,%rs20;
}
	// inline asm
	// inline asm
	{add.f16 %rs36,%rs15,%rs18;
}
	// inline asm

BB48_5:
	st.global.u16 	[%rd2], %rs36;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB48_2;

BB48_6:
	ret;
}

	// .globl	LeakyForwardAlphaFP16
.visible .entry LeakyForwardAlphaFP16(
	.param .u32 LeakyForwardAlphaFP16_param_0,
	.param .u64 LeakyForwardAlphaFP16_param_1,
	.param .u64 LeakyForwardAlphaFP16_param_2,
	.param .align 2 .b8 LeakyForwardAlphaFP16_param_3[2],
	.param .align 2 .b8 LeakyForwardAlphaFP16_param_4[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r6, [LeakyForwardAlphaFP16_param_0];
	ld.param.u64 	%rd3, [LeakyForwardAlphaFP16_param_1];
	ld.param.u64 	%rd4, [LeakyForwardAlphaFP16_param_2];
	ld.param.u16 	%rs8, [LeakyForwardAlphaFP16_param_3];
	ld.param.u16 	%rs9, [LeakyForwardAlphaFP16_param_4];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f1;}

	// inline asm
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB49_6;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB49_2:
	mul.wide.s32 	%rd5, %r10, 2;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.u16 	%rs4, [%rd6];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs4, %rs10;
  selp.u16 %rs11, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs11, 0;
	@%p2 bra 	BB49_4;
	bra.uni 	BB49_3;

BB49_4:
	// inline asm
	{mul.f16 %rs17,%rs4,%rs8;
}
	// inline asm
	// inline asm
	{mul.f16 %rs23,%rs17,%rs9;
}
	// inline asm
	bra.uni 	BB49_5;

BB49_3:
	// inline asm
	{mul.f16 %rs23,%rs9,%rs4;
}
	// inline asm

BB49_5:
	add.s64 	%rd8, %rd1, %rd5;
	st.global.u16 	[%rd8], %rs23;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB49_2;

BB49_6:
	ret;
}

	// .globl	LeakyBackwardAlphaFP16
.visible .entry LeakyBackwardAlphaFP16(
	.param .u32 LeakyBackwardAlphaFP16_param_0,
	.param .u64 LeakyBackwardAlphaFP16_param_1,
	.param .u64 LeakyBackwardAlphaFP16_param_2,
	.param .u64 LeakyBackwardAlphaFP16_param_3,
	.param .align 2 .b8 LeakyBackwardAlphaFP16_param_4[2],
	.param .align 2 .b8 LeakyBackwardAlphaFP16_param_5[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<12>;


	ld.param.u32 	%r6, [LeakyBackwardAlphaFP16_param_0];
	ld.param.u64 	%rd3, [LeakyBackwardAlphaFP16_param_1];
	ld.param.u64 	%rd4, [LeakyBackwardAlphaFP16_param_2];
	ld.param.u64 	%rd5, [LeakyBackwardAlphaFP16_param_3];
	ld.param.u16 	%rs8, [LeakyBackwardAlphaFP16_param_4];
	ld.param.u16 	%rs9, [LeakyBackwardAlphaFP16_param_5];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f1;}

	// inline asm
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB50_6;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd8, %rd5;

BB50_2:
	mul.wide.s32 	%rd6, %r10, 2;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.u16 	%rs12, [%rd7];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs12, %rs10;
  selp.u16 %rs11, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs11, 0;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.u16 	%rs4, [%rd9];
	@%p2 bra 	BB50_4;
	bra.uni 	BB50_3;

BB50_4:
	// inline asm
	{mul.f16 %rs17,%rs4,%rs8;
}
	// inline asm
	// inline asm
	{mul.f16 %rs23,%rs17,%rs9;
}
	// inline asm
	bra.uni 	BB50_5;

BB50_3:
	// inline asm
	{mul.f16 %rs23,%rs9,%rs4;
}
	// inline asm

BB50_5:
	add.s64 	%rd11, %rd1, %rd6;
	st.global.u16 	[%rd11], %rs23;
	bar.sync 	0;
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB50_2;

BB50_6:
	ret;
}

	// .globl	LeakyForwardFP16
.visible .entry LeakyForwardFP16(
	.param .u32 LeakyForwardFP16_param_0,
	.param .u64 LeakyForwardFP16_param_1,
	.param .u64 LeakyForwardFP16_param_2,
	.param .align 2 .b8 LeakyForwardFP16_param_3[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r6, [LeakyForwardFP16_param_0];
	ld.param.u64 	%rd4, [LeakyForwardFP16_param_1];
	ld.param.u64 	%rd5, [LeakyForwardFP16_param_2];
	ld.param.u16 	%rs5, [LeakyForwardFP16_param_3];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs6, %f1;}

	// inline asm
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB51_6;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB51_2:
	mul.wide.s32 	%rd6, %r10, 2;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.u16 	%rs3, [%rd7];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs3, %rs6;
  selp.u16 %rs7, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs7, 0;
	add.s64 	%rd3, %rd1, %rd6;
	@%p2 bra 	BB51_4;
	bra.uni 	BB51_3;

BB51_4:
	// inline asm
	{mul.f16 %rs10,%rs3,%rs5;
}
	// inline asm
	st.global.u16 	[%rd3], %rs10;
	bra.uni 	BB51_5;

BB51_3:
	st.global.u16 	[%rd3], %rs3;

BB51_5:
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB51_2;

BB51_6:
	ret;
}

	// .globl	LeakyBackwardFP16
.visible .entry LeakyBackwardFP16(
	.param .u32 LeakyBackwardFP16_param_0,
	.param .u64 LeakyBackwardFP16_param_1,
	.param .u64 LeakyBackwardFP16_param_2,
	.param .u64 LeakyBackwardFP16_param_3,
	.param .align 2 .b8 LeakyBackwardFP16_param_4[2]
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<12>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u32 	%r6, [LeakyBackwardFP16_param_0];
	ld.param.u64 	%rd5, [LeakyBackwardFP16_param_1];
	ld.param.u64 	%rd6, [LeakyBackwardFP16_param_2];
	ld.param.u64 	%rd7, [LeakyBackwardFP16_param_3];
	ld.param.u16 	%rs4, [LeakyBackwardFP16_param_4];
	mov.f32 	%f1, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs5, %f1;}

	// inline asm
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.s32	%p1, %r10, %r6;
	@%p1 bra 	BB52_6;

	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;

BB52_2:
	mul.wide.s32 	%rd8, %r10, 2;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.u16 	%rs7, [%rd9];
	// inline asm
	{ .reg .pred __$temp3;
  setp.gt.f16  __$temp3, %rs7, %rs5;
  selp.u16 %rs6, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs6, 0;
	add.s64 	%rd4, %rd1, %rd8;
	add.s64 	%rd10, %rd2, %rd8;
	ld.global.u16 	%rs3, [%rd10];
	@%p2 bra 	BB52_4;
	bra.uni 	BB52_3;

BB52_4:
	// inline asm
	{mul.f16 %rs9,%rs3,%rs4;
}
	// inline asm
	st.global.u16 	[%rd4], %rs9;
	bra.uni 	BB52_5;

BB52_3:
	st.global.u16 	[%rd4], %rs3;

BB52_5:
	add.s32 	%r10, %r3, %r10;
	setp.lt.s32	%p3, %r10, %r6;
	@%p3 bra 	BB52_2;

BB52_6:
	ret;
}

	// .globl	MSELossbyBatchesFP16
.visible .entry MSELossbyBatchesFP16(
	.param .u32 MSELossbyBatchesFP16_param_0,
	.param .u32 MSELossbyBatchesFP16_param_1,
	.param .u64 MSELossbyBatchesFP16_param_2,
	.param .u64 MSELossbyBatchesFP16_param_3,
	.param .u64 MSELossbyBatchesFP16_param_4,
	.param .u64 MSELossbyBatchesFP16_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<37>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<28>;
	// demoted variable
	.shared .align 8 .u64 _ZZ20MSELossbyBatchesFP16E5loss2;

	ld.param.u32 	%r11, [MSELossbyBatchesFP16_param_1];
	ld.param.u64 	%rd10, [MSELossbyBatchesFP16_param_2];
	ld.param.u64 	%rd11, [MSELossbyBatchesFP16_param_3];
	ld.param.u64 	%rd12, [MSELossbyBatchesFP16_param_4];
	ld.param.u64 	%rd13, [MSELossbyBatchesFP16_param_5];
	mov.f32 	%f9, 0f40000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f9;}

	// inline asm
	// inline asm
	{  mov.b32 %r12, {%rs7,%rs7};}

	// inline asm
	ld.param.u32 	%r13, [MSELossbyBatchesFP16_param_0];
	shr.u32 	%r14, %r13, 31;
	add.s32 	%r15, %r13, %r14;
	shr.s32 	%r2, %r15, 1;
	setp.lt.s32	%p1, %r11, 1;
	@%p1 bra 	BB53_10;

	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd11;
	cvta.to.global.u64 	%rd3, %rd12;
	cvta.to.global.u64 	%rd4, %rd13;
	mov.u32 	%r17, %ctaid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r3, %r18, %r17, %r19;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r4, %r20, %r18;
	mov.u32 	%r39, 0;
	ld.shared.u64 	%rd27, [_ZZ20MSELossbyBatchesFP16E5loss2];
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r12;
 mov.b16 %rs11, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f13, %rs11;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f14, %f13;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r12;
 mov.b16 %rs20, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs20;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f22, %f21;
}
	// inline asm

BB53_2:
	cvt.s64.s32	%rd7, %r39;
	mul.wide.s32 	%rd14, %r39, 4;
	add.s64 	%rd15, %rd27, %rd14;
	mov.f32 	%f11, 0f00000000;
	// inline asm
	{.reg .f16 low,high;
  cvt.rn.f16.f32 low, %f11;
  cvt.rn.f16.f32 high, %f11;
  mov.b32 %r21, {low,high};}

	// inline asm
	st.u32 	[%rd15], %r21;
	setp.ge.s32	%p2, %r3, %r2;
	@%p2 bra 	BB53_9;

	cvt.u32.u64	%r22, %rd7;
	mul.lo.s32 	%r6, %r22, %r2;
	add.s64 	%rd8, %rd1, %rd14;
	mov.u32 	%r40, %r3;

BB53_4:
	add.s32 	%r31, %r40, %r6;
	mul.wide.s32 	%rd17, %r31, 4;
	add.s64 	%rd18, %rd3, %rd17;
	ld.global.u32 	%r24, [%rd18];
	add.s64 	%rd19, %rd2, %rd17;
	ld.global.u32 	%r25, [%rd19];
	// inline asm
	{sub.f16x2 %r23,%r24,%r25;
}
	// inline asm
	st.global.u32 	[%rd8], %r23;
	// inline asm
	{mul.f16x2 %r26,%r23,%r23;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r26;
 mov.b16 %rs10, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f12, %rs10;}

	// inline asm
	mul.f32 	%f16, %f12, %f14;
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f16;}

	// inline asm
	and.b16  	%rs16, %rs35, 32767;
	mov.u16 	%rs17, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs16, %rs17;
  selp.u16 %rs15, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p3, %rs15, 0;
	setp.eq.s16	%p4, %rs16, 0;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB53_6;

	neg.f32 	%f18, %f13;
	fma.rn.f32 	%f19, %f18, %f16, %f12;
	fma.rn.f32 	%f17, %f14, %f19, %f16;
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f17;}

	// inline asm

BB53_6:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r26;
 mov.b16 %rs19, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs19;}

	// inline asm
	mul.f32 	%f24, %f20, %f22;
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f24;}

	// inline asm
	and.b16  	%rs25, %rs36, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs25, %rs17;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p6, %rs24, 0;
	setp.eq.s16	%p7, %rs25, 0;
	or.pred  	%p8, %p7, %p6;
	@%p8 bra 	BB53_8;

	neg.f32 	%f26, %f21;
	fma.rn.f32 	%f27, %f26, %f24, %f20;
	fma.rn.f32 	%f25, %f22, %f27, %f24;
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f25;}

	// inline asm

BB53_8:
	ld.shared.u64 	%rd21, [_ZZ20MSELossbyBatchesFP16E5loss2];
	// inline asm
	{  mov.b32 %r34, {%rs35,%rs36};}

	// inline asm
	shl.b64 	%rd22, %rd7, 2;
	add.s64 	%rd20, %rd21, %rd22;
	// inline asm
	{ atom.add.noftz.f16x2 %r35,[%rd20],%r34; }

	// inline asm
	add.s32 	%r40, %r4, %r40;
	setp.lt.s32	%p9, %r40, %r2;
	@%p9 bra 	BB53_4;

BB53_9:
	ld.shared.u64 	%rd27, [_ZZ20MSELossbyBatchesFP16E5loss2];
	shl.b64 	%rd23, %rd7, 2;
	add.s64 	%rd24, %rd27, %rd23;
	ld.u32 	%r37, [%rd24];
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r37;
 mov.b16 %rs30, low;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r37;
 mov.b16 %rs31, high;}
	// inline asm
	// inline asm
	{add.f16 %rs32,%rs30,%rs31;
}
	// inline asm
	mul.wide.s32 	%rd25, %r39, 2;
	add.s64 	%rd26, %rd4, %rd25;
	st.global.u16 	[%rd26], %rs32;
	add.s32 	%r39, %r39, 1;
	setp.lt.s32	%p10, %r39, %r11;
	@%p10 bra 	BB53_2;

BB53_10:
	ret;
}

	// .globl	MSELossFP16
.visible .entry MSELossFP16(
	.param .u32 MSELossFP16_param_0,
	.param .u64 MSELossFP16_param_1,
	.param .u64 MSELossFP16_param_2,
	.param .u64 MSELossFP16_param_3,
	.param .u64 MSELossFP16_param_4,
	.param .align 2 .b8 MSELossFP16_param_5[2],
	.param .align 2 .b8 MSELossFP16_param_6[2]
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<40>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<16>;
	// demoted variable
	.shared .align 8 .u64 _ZZ11MSELossFP16E5loss2;

	ld.param.u64 	%rd4, [MSELossFP16_param_1];
	ld.param.u64 	%rd5, [MSELossFP16_param_2];
	ld.param.u64 	%rd6, [MSELossFP16_param_3];
	ld.param.u64 	%rd7, [MSELossFP16_param_4];
	mov.f32 	%f9, 0f40000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f9;}

	// inline asm
	// inline asm
	{  mov.b32 %r9, {%rs7,%rs7};}

	// inline asm
	ld.shared.u64 	%rd8, [_ZZ11MSELossFP16E5loss2];
	mov.f32 	%f10, 0f00000000;
	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f10;}

	// inline asm
	// inline asm
	{  mov.b32 %r10, {%rs10,%rs10};}

	// inline asm
	st.u32 	[%rd8], %r10;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r32, %r2, %r11, %r12;
	ld.param.u32 	%r13, [MSELossFP16_param_0];
	shr.u32 	%r14, %r13, 31;
	add.s32 	%r15, %r13, %r14;
	shr.s32 	%r4, %r15, 1;
	setp.ge.s32	%p1, %r32, %r4;
	@%p1 bra 	BB54_7;

	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r5, %r16, %r2;
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r9;
 mov.b16 %rs14, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f12, %rs14;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f13, %f12;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r9;
 mov.b16 %rs23, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs23;}

	// inline asm
	// inline asm
	{rcp.approx.f32 %f21, %f20;
}
	// inline asm

BB54_2:
	mul.wide.s32 	%rd9, %r32, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.u32 	%r18, [%rd10];
	add.s64 	%rd11, %rd2, %rd9;
	ld.global.u32 	%r19, [%rd11];
	// inline asm
	{sub.f16x2 %r17,%r18,%r19;
}
	// inline asm
	add.s64 	%rd12, %rd1, %rd9;
	st.global.u32 	[%rd12], %r17;
	// inline asm
	{mul.f16x2 %r20,%r17,%r17;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r20;
 mov.b16 %rs13, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f11, %rs13;}

	// inline asm
	mul.f32 	%f15, %f11, %f13;
	// inline asm
	{  cvt.rn.f16.f32 %rs38, %f15;}

	// inline asm
	and.b16  	%rs19, %rs38, 32767;
	mov.u16 	%rs20, 143;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs19, %rs20;
  selp.u16 %rs18, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p2, %rs18, 0;
	setp.eq.s16	%p3, %rs19, 0;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	BB54_4;

	neg.f32 	%f17, %f12;
	fma.rn.f32 	%f18, %f17, %f15, %f11;
	fma.rn.f32 	%f16, %f13, %f18, %f15;
	// inline asm
	{  cvt.rn.f16.f32 %rs38, %f16;}

	// inline asm

BB54_4:
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r20;
 mov.b16 %rs22, high;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f19, %rs22;}

	// inline asm
	mul.f32 	%f23, %f19, %f21;
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f23;}

	// inline asm
	and.b16  	%rs28, %rs39, 32767;
	// inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs28, %rs20;
  selp.u16 %rs27, 1, 0, __$temp3;}
	// inline asm
	setp.eq.s16	%p5, %rs27, 0;
	setp.eq.s16	%p6, %rs28, 0;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	BB54_6;

	neg.f32 	%f25, %f20;
	fma.rn.f32 	%f26, %f25, %f23, %f19;
	fma.rn.f32 	%f24, %f21, %f26, %f23;
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f24;}

	// inline asm

BB54_6:
	ld.shared.u64 	%rd13, [_ZZ11MSELossFP16E5loss2];
	// inline asm
	{  mov.b32 %r27, {%rs38,%rs39};}

	// inline asm
	// inline asm
	{ atom.add.noftz.f16x2 %r28,[%rd13],%r27; }

	// inline asm
	add.s32 	%r32, %r5, %r32;
	setp.lt.s32	%p8, %r32, %r4;
	@%p8 bra 	BB54_2;

BB54_7:
	ld.shared.u64 	%rd14, [_ZZ11MSELossFP16E5loss2];
	ld.u32 	%r30, [%rd14];
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r30;
 mov.b16 %rs33, low;}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r30;
 mov.b16 %rs34, high;}
	// inline asm
	// inline asm
	{add.f16 %rs35,%rs33,%rs34;
}
	// inline asm
	cvta.to.global.u64 	%rd15, %rd7;
	st.global.u16 	[%rd15], %rs35;
	ret;
}


`
